###begin article-title 0
###xml 50 70 <span type="species:ncbi:3702">Arabidopsis thaliana</span>
Combining classifiers to predict gene function in Arabidopsis thaliana using large-scale gene expression measurements
###end article-title 0
###begin p 1
This is an Open Access article distributed under the terms of the Creative Commons Attribution License (), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.
###end p 1
###begin title 2
Background
###end title 2
###begin p 3
###xml 0 21 0 21 <italic xmlns:xlink="http://www.w3.org/1999/xlink">Arabidopsis thaliana </italic>
###xml 0 20 <span type="species:ncbi:3702">Arabidopsis thaliana</span>
Arabidopsis thaliana is the model species of current plant genomic research with a genome size of 125 Mb and approximately 28,000 genes. The function of half of these genes is currently unknown. The purpose of this study is to infer gene function in Arabidopsis using machine-learning algorithms applied to large-scale gene expression data sets, with the goal of identifying genes that are potentially involved in plant response to abiotic stress.
###end p 3
###begin title 4
Results
###end title 4
###begin p 5
###xml 105 116 105 116 <italic xmlns:xlink="http://www.w3.org/1999/xlink">A. thaliana</italic>
###xml 320 323 320 323 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 105 116 <span type="species:ncbi:3702">A. thaliana</span>
Using in house and publicly available data, we assembled a large set of gene expression measurements for A. thaliana. Using those genes of known function, we first evaluated and compared the ability of basic machine-learning algorithms to predict which genes respond to stress. Predictive accuracy was measured using ROC50 and precision curves derived through cross validation. To improve accuracy, we developed a method for combining these classifiers using a weighted-voting scheme. The combined classifier was then trained on genes of known function and applied to genes of unknown function, identifying genes that potentially respond to stress. Visual evidence corroborating the predictions was obtained using electronic Northern analysis. Three of the predicted genes were chosen for biological validation. Gene knockout experiments confirmed that all three are involved in a variety of stress responses. The biological analysis of one of these genes (At1g16850) is presented here, where it is shown to be necessary for the normal response to temperature and NaCl.
###end p 5
###begin title 6
Conclusion
###end title 6
###begin p 7
###xml 582 594 582 594 <italic xmlns:xlink="http://www.w3.org/1999/xlink">A. thaliana </italic>
###xml 582 593 <span type="species:ncbi:3702">A. thaliana</span>
Supervised learning methods applied to large-scale gene expression measurements can be used to predict gene function. However, the ability of basic learning methods to predict stress response varies widely and depends heavily on how much dimensionality reduction is used. Our method of combining classifiers can improve the accuracy of such predictions - in this case, predictions of genes involved in stress response in plants - and it effectively chooses the appropriate amount of dimensionality reduction automatically. The method provides a useful means of identifying genes in A. thaliana that potentially respond to stress, and we expect it would be useful in other organisms and for other gene functions.
###end p 7
###begin title 8
Background
###end title 8
###begin p 9
###xml 209 210 209 210 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B1">1</xref>
###xml 211 212 211 212 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B6">6</xref>
###xml 522 523 522 523 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B2">2</xref>
###xml 524 525 524 525 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B7">7</xref>
###xml 690 715 690 715 <italic xmlns:xlink="http://www.w3.org/1999/xlink">Saccharomyces cerevisiae </italic>
###xml 744 745 744 745 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B1">1</xref>
###xml 746 747 746 747 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B8">8</xref>
###xml 748 750 748 750 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B16">16</xref>
###xml 684 689 <span type="species:ncbi:4932">yeast</span>
###xml 690 714 <span type="species:ncbi:4932">Saccharomyces cerevisiae</span>
###xml 738 742 <span type="species:ncbi:10090">mice</span>
Assigning functions to unannotated genes, identified by genome sequencing and other methods, is the goal of functional genomics. Many approaches have been proposed for large-scale prediction of gene function [1-6]. These approaches are largely based on physical association, genetic interaction, sequence relationships and patterns of gene expression. Predicting gene functions based on large-scale gene expression measurements is an attractive strategy since many pathways display coordinated transcriptional regulation [2,7]. Although previous studies show that supervised learning methods can be used to predict gene function based on gene expression in microorganisms such as the yeast Saccharomyces cerevisiae and in mammals such as mice [1,8-16], it remains unknown to what extent this is true in plants.
###end p 9
###begin p 10
###xml 9 21 9 21 <italic xmlns:xlink="http://www.w3.org/1999/xlink">A. thaliana </italic>
###xml 50 52 50 52 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B17">17</xref>
###xml 216 218 216 218 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B18">18</xref>
###xml 313 325 313 325 <italic xmlns:xlink="http://www.w3.org/1999/xlink">A. thaliana </italic>
###xml 423 425 423 425 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B19">19</xref>
###xml 426 428 426 428 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B20">20</xref>
###xml 9 20 <span type="species:ncbi:3702">A. thaliana</span>
###xml 313 324 <span type="species:ncbi:3702">A. thaliana</span>
With the A. thaliana genome completely sequenced [17], functional annotation of the genes remains a key challenge for biologists. Currently, approximately 50% of the 28,000 genes have not been assigned any function [18]. Thus, the extent to which supervised learning methods can be used to infer gene function in A. thaliana is a timely and important question. Little work has been done in this area, two exceptions being [19,20].
###end p 10
###begin p 11
###xml 4 6 4 6 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B19">19</xref>
###xml 173 175 173 175 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B21">21</xref>
###xml 615 626 615 626 <italic xmlns:xlink="http://www.w3.org/1999/xlink">A. thaliana</italic>
###xml 615 626 <span type="species:ncbi:3702">A. thaliana</span>
In [19], a method is developed to infer gene function from microarray data and predicted protein-protein interactions. The method is similar to Nearest Neighbor algorithms [21] in that the predicted function(s) of a gene are based on the function(s) of nearby genes. Here, the "nearness" of one gene to another is based on a normalized Pearson correlation of their expression profiles and on putative interactions of their protein products. In addition, the method is extended to the discovery of biological pathways, and is applied to predicting the signaling pathway of phosphatidic acid as a second messenger in A. thaliana.
###end p 11
###begin p 12
###xml 4 6 4 6 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B20">20</xref>
###xml 112 123 112 123 <italic xmlns:xlink="http://www.w3.org/1999/xlink">A. thaliana</italic>
###xml 605 616 605 616 <italic xmlns:xlink="http://www.w3.org/1999/xlink">A. thaliana</italic>
###xml 112 123 <span type="species:ncbi:3702">A. thaliana</span>
###xml 605 616 <span type="species:ncbi:3702">A. thaliana</span>
In [20], a decision tree algorithm is applied to the problem of predicting the function of protein sequences in A. thaliana. Six sources of data were used: sequence, expression, SCOP, secondary structure, InterPro and sequence similarity. One conclusion of the study is that the decision tree algorithm was unable to extract much information from the expression data. The authors suggest that this is because the expression data came from unrelated and highly-specific experiments with just a few readings per gene each. They also suggest that because many more expression data sets are now available for A. thaliana, results may improve when using this type of data in the future.
###end p 12
###begin p 13
###xml 56 68 56 68 <italic xmlns:xlink="http://www.w3.org/1999/xlink">A. thaliana </italic>
###xml 277 289 277 289 <italic xmlns:xlink="http://www.w3.org/1999/xlink">A. thaliana </italic>
###xml 56 67 <span type="species:ncbi:3702">A. thaliana</span>
###xml 277 288 <span type="species:ncbi:3702">A. thaliana</span>
The present study aims to identify unannotated genes in A. thaliana that are potentially involved in plant response to stress. In the context of plants, a stress (biotic or abiotic) causes a decrease in plant growth or yield. We investigated the prediction of gene function in A. thaliana based solely on gene expression data using a variety of basic supervised learning methods, namely Logistic Regression (LR), Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), Naive Bayes (NB) and K-Nearest Neighbors (KNN). We also investigated the effect on the learning methods of preprocessing the expression data using Principal Component Analysis (PCA). Finally, we improved the performance of the basic learning methods by combining them using a weighted voting (WV) scheme. This work has enabled our collaborators, biologists in the Department of Cell and Systems Biology at the University of Toronto, to carry out directed biological experiments for determining gene function. In addition to these biological results, the paper illustrates how various machine-learning methods have had to be adapted to fit this bioinformatics application.
###end p 13
###begin title 14
Results and discussion
###end title 14
###begin title 15
Microarray data and the Gene Ontology
###end title 15
###begin p 16
###xml 114 116 114 116 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B22">22</xref>
###xml 167 169 167 169 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B23">23</xref>
###xml 196 198 196 198 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B24">24</xref>
###xml 199 201 199 201 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B25">25</xref>
###xml 271 282 271 282 <italic xmlns:xlink="http://www.w3.org/1999/xlink">Arabidopsis</italic>
###xml 921 924 921 924 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i.e</italic>
In this study, we used two microarray data sets: one from the Botany Array Resource at the University of Toronto [22], and the other from the AtGenExpress Consortium [23], archived at NASCArrays [24,25]. These data sets include over 1000 expression-level experiments for Arabidopsis, and using all of them would give a data set with dimensionality over 1000. Since the performance of statistical and machine-learning methods tends to decrease with dimensionality, we chose only those experiments that are specifically stress-related. Even so, the covariance matrix of the resulting data set is singular, which is a problem for many of the machine-learning methods. The singularity is probably due to dependencies between the expression levels under control conditions, since removing the controls from the data sets solved the problem. To compensate, we tried applying the learning algorithms to expression-level ratios (i.e., ratios of experimental to control conditions). However, we found that the results were better when ratios were not used (data not shown). This is probably because the classifiers look for genes that respond similarly to the known stress-associated genes, so it is not so important to include the controls. In addition, since many of the features are time-courses, there is still a "time zero" control included for the values, providing a baseline measurement. The results reported in this article are therefore based on absolute expression levels without controls.
###end p 16
###begin p 17
From the Toronto data set, we selected 54 features corresponding to experiments conducted primarily to study plant environmental and stress physiology, plant physiology, plant-microbe and plant-insect interactions. From the AtGenExpress data set, we selected 236 features, including various abiotic stresses (e.g., osmotic stress, heat stress, cold stress, salt stress, drought stress, UV-B stress, wounding stress, water-deprivation stress and oxidative stress). We combined the selected features into a single data set. The resulting data set consists of gene expression levels for 22,746 genes under 54 + 236 = 290 different experimental conditions.
###end p 17
###begin p 18
###xml 124 156 124 156 <italic xmlns:xlink="http://www.w3.org/1999/xlink">GO:0006950 [response to stress] </italic>
###xml 328 330 328 330 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B26">26</xref>
###xml 641 646 <span type="species:ncbi:9606">child</span>
We used terms from the Gene Ontology for Biological Processes (GOBP) to represent gene function. For example, the GOBP term GO:0006950 [response to stress] refers to genes that respond to stress. In general, the Gene Ontology (GO) provides a dynamic controlled vocabulary for describing genes and gene products in any organism [26]. "Biological Process" is one of three broad GO categories (the other two being "Molecular Function" and "Cellular Component"). GOBP terms are organized into a directed acyclic graph (DAG) to reflect the hierarchical relationships between the terms. Parent GOBP terms are subdivided into increasingly specific child GOBP terms.
###end p 18
###begin p 19
###xml 95 127 95 127 <italic xmlns:xlink="http://www.w3.org/1999/xlink">GO:0006950 [response to stress] </italic>
###xml 193 222 193 222 <italic xmlns:xlink="http://www.w3.org/1999/xlink">GO:0009409 [response to cold]</italic>
###xml 224 253 224 253 <italic xmlns:xlink="http://www.w3.org/1999/xlink">GO:0009408 [response to heat]</italic>
###xml 259 301 259 301 <italic xmlns:xlink="http://www.w3.org/1999/xlink">GO:0009414 [response to water deprivation]</italic>
###xml 575 626 575 626 <italic xmlns:xlink="http://www.w3.org/1999/xlink">GO:0009613 [response to pest, pathogen or parasite]</italic>
###xml 1028 1059 1028 1059 <italic xmlns:xlink="http://www.w3.org/1999/xlink">GO:0006950 [response to stress]</italic>
###xml 172 177 <span type="species:ncbi:9606">child</span>
Since our study focussed on stress, we were concerned with gene functions at or below the term GO:0006950 [response to stress] in the GOBP hierarchy. This GOBP term has 19 child terms, such as GO:0009409 [response to cold], GO:0009408 [response to heat], and GO:0009414 [response to water deprivation]. Since gene function becomes more and more specific as we move down the GOBP hierarchy, fewer and fewer genes have any given annotation. The result is that for specific types of stress, our data set contains many negatives and few positives. In the best case, for the term GO:0009613 [response to pest, pathogen or parasite], over 97% of the training data consists of negatives. The typical case is even worse. In fact, looking at all 19 types of stress, 5 types have no positives at all, and of the remaining 14 types, the median number of negatives is 99.2% of the training data. This highly unbalanced data made accurate prediction of gene function difficult. For this reason, we narrowed our study to the top stress term, GO:0006950 [response to stress]. To get positive training samples for this term, we propagated all genes in its offspring upward to it in the hierarchy. After up-propagation, the top stress term has 1,031 genes, or almost 9% of the total genes in the training data. The training data therefore contains 9% positives and 91% negatives.
###end p 19
###begin p 20
###xml 42 54 42 54 <italic xmlns:xlink="http://www.w3.org/1999/xlink">A. thaliana </italic>
###xml 101 103 101 103 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B27">27</xref>
###xml 104 106 104 106 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B28">28</xref>
###xml 176 178 176 178 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B27">27</xref>
###xml 320 330 320 330 <italic xmlns:xlink="http://www.w3.org/1999/xlink">annotated </italic>
###xml 340 352 340 352 <italic xmlns:xlink="http://www.w3.org/1999/xlink">unannotated </italic>
###xml 590 621 590 621 <italic xmlns:xlink="http://www.w3.org/1999/xlink">GO:0008150 [biological process]</italic>
###xml 42 53 <span type="species:ncbi:3702">A. thaliana</span>
Using GOBP terms to annotate all genes in A. thaliana is an ongoing project started in 2002 by TAIR [27,28]. The gene annotations (updated weekly) can be downloaded from TAIR [27]. The predictions reported in this paper are based on the version for March 10, 2007. Using these annotations, we categorized the genes into annotated genes and unannotated genes. The annotated genes are those which have at least one GOBP annotation; the unannotated genes are those which have no GOBP annotations. In addition, a gene was treated as unannotated if its only annotation is the top GOBP category, GO:0008150 [biological process], since the function of such a gene is unknown. The result was 11,553 annotated genes and 11,193 unannotated genes in our data set.
###end p 20
###begin p 21
The annotated genes formed the training data, in which a gene was called positive if it is annotated as a stress gene, and negative otherwise. The unannotated genes formed the prediction data. It should be noted that this approach probably introduces some false negatives into the training data, because genes not known to have a particular function are considered to be negative, even though future experiments could reveal them to have that function. That is to say, "unknown" is treated as "negative". However, the number of such false negatives should be small, since only a small number of genes participate in any given biological process. That is, most negatives are true negatives.
###end p 21
###begin title 22
Predicting gene function using basic learning methods
###end title 22
###begin p 23
Using a variety of basic learning methods, we trained a number of classifiers to distinguish between genes that do and do not respond to stress, based on their patterns of gene expression in the training data. We then applied each classifier to the prediction data to estimate the function of the unannotated genes. In addition, we used cross validation to evaluate the performance of each classifier and to estimate the precision of each prediction.
###end p 23
###begin p 24
###xml 191 193 191 193 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B21">21</xref>
We used five supervised learning methods: Logistic Regression (LR), Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), Naive Bayes (NB) and K-Nearest Neighbors (KNN) [21] (see Methods). These methods were chosen because they are representative of the most basic supervised learning methods, the goal being to explore simple methods first. These methods are widely understood, take little computation time, and the results provide a benchmark against which more sophisticated methods can be compared. Moreover, as we show below, the results provided by these methods are good enough to enable biologists to conduct targeted laboratory experiments.
###end p 24
###begin p 25
###xml 386 387 386 387 <bold xmlns:xlink="http://www.w3.org/1999/xlink">x</bold>
###xml 492 494 492 494 <bold xmlns:xlink="http://www.w3.org/1999/xlink">x </bold>
###xml 532 535 532 535 <italic xmlns:xlink="http://www.w3.org/1999/xlink">dv </italic>
###xml 538 539 538 539 <bold xmlns:xlink="http://www.w3.org/1999/xlink">x</bold>
###xml 638 641 638 639 <italic xmlns:xlink="http://www.w3.org/1999/xlink">&#964;</italic>
###xml 716 719 714 717 <italic xmlns:xlink="http://www.w3.org/1999/xlink">dv </italic>
###xml 721 724 719 720 <italic xmlns:xlink="http://www.w3.org/1999/xlink">&#964;</italic>
Each of the five methods is discriminative. That is, the classifiers learned by the methods assign a real number (called a discriminant value) to each gene, reflecting the classifier's certainty that the gene responds to stress. Formally, a discriminative classifier is a function, , from genes to discriminant values. In our case, each gene is represented as a 290-dimensional vector, x, whose components are the expression levels of the gene under the 290 experimental conditions. Thus, if x is a vector representing a gene, then dv = (x) is the discriminant value assigned to the gene by the classifier. Finally, a decision threshold, tau, is chosen, and the gene is predicted to respond to stress if and only if dv > tau.
###end p 25
###begin title 26
Unsupervised, semi-supervised and transductive learning
###end title 26
###begin p 27
###xml 453 455 453 455 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B29">29</xref>
###xml 456 458 456 458 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B30">30</xref>
###xml 714 716 714 716 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B29">29</xref>
###xml 717 719 717 719 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B30">30</xref>
###xml 1009 1021 1009 1021 <italic xmlns:xlink="http://www.w3.org/1999/xlink">Arabidopsis </italic>
###xml 1112 1114 1112 1114 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B29">29</xref>
###xml 1115 1117 1115 1117 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B31">31</xref>
In addition to these supervised learning methods, we preprocessed the gene expression data using Principal Components Analysis (PCA), a form of unsupervised learning, to reduce the dimensionality of the data (see Methods). For this purpose, we combined the expression-level measurements for all genes (both annotated and unannotated) into one large data set, and applied PCA to the entire set. We are therefore doing a form of semi-supervised learning [29,30], in which unsupervised learning uses the entire data set (ignoring annotations), and then supervised learning uses the annotated data. This increases the effectiveness of learning by increasing the amount of training data used in the unsupervised phase [29,30]. In our case, the unannotated data is also the prediction data, which means that information about the prediction data is used during (unsupervised) training. This is possible because we know all the prediction data in advance. That is, we know the expression levels for all the genes in Arabidopsis whether they are annotated or not. We are therefore doing a form of transductive learning [29,31], in which the entire prediction set is known during training and is exploited to predict its annotations. This has the added computational advantage of simplifying the way PCA is done during cross validation (see Methods).
###end p 27
###begin title 28
Estimating classifier performance
###end title 28
###begin p 29
###xml 127 129 127 129 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B32">32</xref>
###xml 423 425 423 425 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B33">33</xref>
###xml 715 718 715 718 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 726 728 726 728 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B34">34</xref>
###xml 839 842 839 842 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 858 861 858 861 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 868 870 868 870 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B34">34</xref>
To evaluate the performance of discriminative classifiers, it is common to use receiver operating characteristic (ROC) curves [32]. A ROC curve plots the true positive rate (TP) of a classifier against the false positive rate (FP) for various decision thresholds. It therefore shows the quality of a classifier not at one threshold, but at many, and provides more information than a simple miss-classification rate (as in [33] for example). In practice, however, biologists are not usually interested in having more than a few dozen false positives, especially in unbalanced data such as ours, in which the number of false positives can rapidly overwhelm the number of true positives. We therefore use so-called ROC50 curves [34], a variant of ROC curves in which the horizonal axis only goes up to 50 false positives. The area under a ROC50 curve is the ROC50 score [34], and is a measure of the overall usefulness of a classifier.
###end p 29
###begin p 30
###xml 15 18 15 18 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 250 253 250 253 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 477 480 477 480 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 604 605 604 605 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F1">1</xref>
###xml 713 716 713 716 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 732 733 732 733 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F2">2</xref>
###xml 737 738 737 738 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F7">7</xref>
###xml 778 781 778 781 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
To estimate ROC50 curves for our classifiers, we used 20-fold cross-validation (see Methods). Because cross-validation relies on a random split of the training data into folds (20 folds in our case), there is a certain randomness to the estimated ROC50 curve. To provide more accurate results, we performed cross-validation ten times, each time with a different (randomly selected) 20-fold split of the data (see Methods). Each 20-fold split results in a slightly different ROC50 curve. In some cases, we plot all ten of these curves, to give an idea of the uncertainty in classifier performance (Figure 1). In cases where this would result in overly cluttered graphs, we simply present the average of the ten ROC50 curves (Figures 2 to 7, each of which show several average ROC50 curves).
###end p 30
###begin p 31
###xml 3 6 3 6 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 0 12 0 12 <bold xmlns:xlink="http://www.w3.org/1999/xlink">ROC<sub>50 </sub>curves</bold>
###xml 27 30 27 30 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
ROC50 curves. Estimated ROC50 curves of the combined classifier (WV), showing ten different estimates (dashed curves) and their average (solid curve).
###end p 31
###begin p 32
###xml 0 24 0 24 <bold xmlns:xlink="http://www.w3.org/1999/xlink">Logistic Regression (LR)</bold>
###xml 35 38 35 38 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 192 195 192 195 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
Logistic Regression (LR). Seven ROC50 curves for Logistic Regression with varying amounts of dimensionality reduction using PCA. In the legend, p is the PCA-reduced dimension, and s is the ROC50 score.
###end p 32
###begin p 33
###xml 0 34 0 34 <bold xmlns:xlink="http://www.w3.org/1999/xlink">Linear Discriminant Analysis (LDA)</bold>
###xml 45 48 45 48 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 211 214 211 214 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
Linear Discriminant Analysis (LDA). Seven ROC50 curves for Linear Discriminant Analysis with varying amounts of dimensionality reduction using PCA. In the legend, p is the PCA-reduced dimension, and s is the ROC50 score.
###end p 33
###begin p 34
###xml 0 37 0 37 <bold xmlns:xlink="http://www.w3.org/1999/xlink">Quadratic Discriminant Analysis (QDA)</bold>
###xml 48 51 48 51 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 217 220 217 220 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
Quadratic Discriminant Analysis (QDA). Seven ROC50 curves for Quadratic Discriminant Analysis with varying amounts of dimensionality reduction using PCA. In the legend, p is the PCA-reduced dimension, and s is the ROC50 score.
###end p 34
###begin p 35
###xml 0 16 0 16 <bold xmlns:xlink="http://www.w3.org/1999/xlink">Naive Bayes (NB)</bold>
###xml 27 30 27 30 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 176 179 176 179 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
Naive Bayes (NB). Seven ROC50 curves for Naive Bayes with varying amounts of dimensionality reduction using PCA. In the legend, p is the PCA-reduced dimension, and s is the ROC50 score.
###end p 35
###begin p 36
###xml 0 26 0 26 <bold xmlns:xlink="http://www.w3.org/1999/xlink">K-Nearest Neighbours (KNN)</bold>
###xml 36 39 36 39 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 120 123 120 123 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
K-Nearest Neighbours (KNN). Five ROC50 curves for K-Nearest Neighbours for various values of K. The legend gives the ROC50 score, s, for each value of K.
###end p 36
###begin p 37
###xml 0 21 0 21 <bold xmlns:xlink="http://www.w3.org/1999/xlink">Comparison of methods</bold>
###xml 30 33 30 33 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 120 123 120 123 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 234 237 234 237 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
Comparison of methods. The ROC50 curve (purple) for the combined classifier using weighted voting (WV), and the best ROC50 curves from each of Figures 2 to 6. In the legend, p is the PCA-reduced dimension of the data, and s is the ROC50 score.
###end p 37
###begin p 38
###xml 16 19 16 19 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 740 741 740 741 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F2">2</xref>
###xml 745 746 745 746 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F6">6</xref>
###xml 839 842 839 842 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 1195 1198 1195 1198 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i.e</italic>
###xml 1340 1343 1340 1343 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
We generated ROC50 curves for each supervised learning method combined with various amounts of dimensionality reduction. Using PCA, we reduced the original 290 dimensions to 5, 10, 15, 20, 40 and 100 dimensions, respectively. In this way, the original data set was transformed into six separate data sets of various dimensions. Each basic learning method (except KNN) was applied to the original data set and to each of the six reduced data sets. Thus, for each basic learning method (except KNN), we trained and tested seven different classifiers. In the case of KNN, we used only the original, unreduced data, but with five different values of K. Altogether, we trained and tested a total of 4 x 7 + 5 = 33 different classifiers. Figures 2 to 6 show the estimated performance of these basic classifiers. Each figure shows a number of ROC50 curves, each derived using cross-validation averaged over a number of random splits of the data, as described above. Unlike traditional ROC curves, the axes of these curves give the number of true and false positives, instead of the proportion. The red dash-dot line near the bottom of each figure shows the expected performance of a random classifier (i.e., a classifier that ignores the expression data and guesses whether or not a gene responds to stress by essentially flipping a coin). The ROC50 scores for the curves are shown in the legend of each figure.
###end p 38
###begin p 39
###xml 383 386 383 386 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 564 567 564 567 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 663 666 663 666 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 756 759 756 759 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
As the figures show, in some cases the classifiers perform not much better than random, but in most cases they perform significantly better. The figures also show that the performance of each classification method depends heavily of the amount of dimensionality reduction used. Notice in particular that in some cases, the classifier trained on the reduced data has a much higher ROC50 score than the classifier trained on the original, unreduced data. This is especially true for NB and QDA. For instance, the classifiers trained on the original data have low ROC50 scores of 182.3 for NB and 115.2 for QDA. This is comparable to the random classifier, whose ROC50 score is 122.5. However, reducing the dimensionality of the data to 15 increases their ROC50 scores to 1373.1 and 1651.0, respectively. This shows the importance of dimensionality reduction. In contrast, KNN performs well for all the values of K that we used.
###end p 39
###begin p 40
###xml 7 8 7 8 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F7">7</xref>
###xml 155 158 155 158 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 184 187 184 187 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 217 218 217 218 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F7">7</xref>
Figure 7 compares the basic classification methods by plotting the best performance of each. That is, for each of the basic classification methods, the ROC50 curve with the highest ROC50 score is reproduced in Figure 7. In addition, the figure shows the performance of a classification method that uses a weighted voting scheme (WV) to combine the 33 basic classifiers into a single, composite classifier. Notice that this composite classifier performs best of all. The next section describes how this composite classifier is constructed.
###end p 40
###begin title 41
Improving prediction accuracy by combining classifiers
###end title 41
###begin p 42
###xml 535 536 535 536 <italic xmlns:xlink="http://www.w3.org/1999/xlink">w</italic>
###xml 536 537 536 537 <sub xmlns:xlink="http://www.w3.org/1999/xlink">1</sub>
###xml 550 551 542 543 <italic xmlns:xlink="http://www.w3.org/1999/xlink">w</italic>
###xml 551 552 543 544 <italic xmlns:xlink="http://www.w3.org/1999/xlink">M</italic>
###xml 551 552 543 544 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>M</italic></sub>
###xml 623 625 615 617 <italic xmlns:xlink="http://www.w3.org/1999/xlink">M </italic>
Combining different classifiers in prediction can be thought of as combining different opinions in decision making. The advantage is that a group opinion is better than a single opinion if the single opinions are correctly weighted and combined. In machine-learning systems, classifiers are often combined by weighted voting, in which the discriminant value of the combined classifier is a linear combination of the discriminant values of the individual classifiers. Formally, given a set of basic classifiers, , and a set of weights, w1, em leader, wM, the combined classifier, , is defined by the equation . In our case, M = 33, as described above.
###end p 42
###begin p 43
###xml 37 38 37 38 <italic xmlns:xlink="http://www.w3.org/1999/xlink">w</italic>
###xml 38 39 38 39 <sub xmlns:xlink="http://www.w3.org/1999/xlink">1</sub>
###xml 52 53 44 45 <italic xmlns:xlink="http://www.w3.org/1999/xlink">w</italic>
###xml 53 54 45 46 <italic xmlns:xlink="http://www.w3.org/1999/xlink">M</italic>
###xml 53 54 45 46 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>M</italic></sub>
###xml 201 203 193 195 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B21">21</xref>
###xml 418 421 410 413 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
By judiciously choosing the weights, w1, em leader, wM, the performance of the combined classifier can be maximized. Various methods are available for doing this, such as model averaging and stacking [21]. Using these methods on our data sets, we found that the ROC curve of the combined classifier was usually better than the ROC curves of the basic classifiers, as expected. Unfortunately, we also found that the ROC50 curve of the combined classifier was usually worse (data not shown). We hypothesized that this is because our data sets are highly unbalanced. Intuitively, model averaging and stacking try to choose weights so as to correctly classify as much data as possible. In our case, this means trying to correctly classify the vast number of negative samples in our data sets, even if this means misclassifying the small number of positives. In other words, these methods try to minimize the total number of false positives, even though we only care about the first fifty.
###end p 43
###begin p 44
###xml 228 231 228 231 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 321 324 321 324 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 433 436 433 436 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 547 550 547 550 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 570 571 570 571 <italic xmlns:xlink="http://www.w3.org/1999/xlink">f</italic>
###xml 571 572 571 572 <italic xmlns:xlink="http://www.w3.org/1999/xlink">m</italic>
###xml 571 572 571 572 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>m</italic></sub>
###xml 613 614 613 614 <bold xmlns:xlink="http://www.w3.org/1999/xlink">x</bold>
###xml 683 684 683 684 <bold xmlns:xlink="http://www.w3.org/1999/xlink">x</bold>
###xml 848 851 848 851 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
To choose appropriate weights for our combined classifier, we used the heuristic that classifiers that perform well should be given more weight than classifiers that perform poorly. In our case, since we want to maximize the ROC50 score of the combined classifier, we want to give high weight to classifiers with high ROC50 scores. There are many ways to do this, but we found that it was sufficient to estimate and normalize the ROC50 score of each basic classifier, and use this as its weight. That is, we used , where  is an estimate of the ROC50 score of classifier fm. Note that with these weights, if each (x) is a number between 0 and 1 (as with our classifiers), then so is (x). Also, this method automatically gives low weight to classifiers that use an inappropriate amount of dimensionality reduction, since such classifiers have low ROC50 scores. In this way, the combined classifier incorporates not only the best combination of supervised learning methods, but also the best amounts of dimensionality reduction for each method.
###end p 44
###begin p 45
###xml 55 59 55 59 <italic xmlns:xlink="http://www.w3.org/1999/xlink">two </italic>
###xml 175 178 175 178 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 309 312 309 312 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 553 554 553 554 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F7">7</xref>
###xml 606 609 606 609 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
To train and evaluate the combined classifier, we used two sets of validation data. After the basic classifiers were trained, one validation set was used to estimate their ROC50 scores. The combined classifier was then constructed using these scores, and the second validation set was used to estimate its ROC50 curve. Thus, the validation data for the basic classifiers is part of the training data for the combined classifier. To do this in a cross-validation setting, we used what amounts to nested cross-validation (see Methods). As shown in Figure 7, the resulting combined classifier has a higher ROC50 score than any of the basic classifiers from which it is made.
###end p 45
###begin p 46
###xml 7 8 7 8 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F1">1</xref>
###xml 224 227 224 227 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 291 294 291 294 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 417 418 417 418 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F1">1</xref>
###xml 682 685 682 685 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
Figure 1 gives another view of the performance of the combined classifier. Here, the thin dashed lines are a superposition of ten different curves, where each one is a different estimate of the combined classifier's true ROC50 curve. As described earlier, each estimate of a classifier's ROC50 curve includes some randomness, due to the random choice of folds during cross-validation. The ten dashed curves in Figure 1 are derived from ten different cross-validations, each one using a different set of folds. The thick solid line in the figure is the average of the other ten curves. Because averaging reduces variance, the average curve is a more accurate estimate of the true ROC50 curve (i.e., has lower variance) than any of the other ten curves. The diagonal dash-dot line near the bottom of the plot shows the expected performance of a random classifier.
###end p 46
###begin p 47
###xml 11 14 11 14 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 137 147 137 147 <italic xmlns:xlink="http://www.w3.org/1999/xlink">precision </italic>
###xml 314 316 314 316 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B35">35</xref>
ROC and ROC50 curves plot the number of true positives against the number of false positives. However, in applications such as ours, the precision is also of interest. Precision is the proportion of true positives (TP) among the predicted positives (PP). (It is also the complementary false discovery rate, 1-FDR [35].) Precision is important since each prediction is a potential experiment, and as a matter of economics, a biologist needs an estimate of how many of the experiments will succeed. This is especially important in situations, such as ours, where the number of real negatives is much greater than the number of real positives, and so there is a real possibility of having a huge number of failed experiments.
###end p 47
###begin p 48
###xml 7 8 7 8 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F8">8</xref>
###xml 156 159 156 159 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i.e</italic>
###xml 315 316 315 316 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F1">1</xref>
###xml 656 659 656 659 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i.e</italic>
###xml 708 711 708 711 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i.e</italic>
###xml 963 964 963 964 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F1">1</xref>
###xml 969 970 969 970 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F8">8</xref>
Figure 8 plots estimated precision against the number of predictions for the first hundred predictions. Notice that as the number of predictions increases (i.e., as the classifier's decision threshold is lowered), the precision decreases, meaning that fewer of the predictions are expected to be true. As in Figure 1, the thin dashed lines are a superposition of ten different curves, each one an estimate of the true precision curve, and the thick solid line is their average. Also, the horizontal dash-dot line near the bottom of the plot is the expected precision of a random classifier, and its height is equal to the ratio of the number of positives (i.e., stress genes) to the total number of samples (i.e., genes) in the training data. Since all the estimated precision curves are well above the horizontal dash-dot line, the performance of the combined classifier for the first hundred predictions is significantly better than random. Also, since Figures 1 and 8 show small variance, and since the variance of the average curves will be even less, the combined classifier should have stable prediction performance.
###end p 48
###begin p 49
###xml 0 16 0 16 <bold xmlns:xlink="http://www.w3.org/1999/xlink">Precision curves</bold>
Precision curves. Estimated precision curves of the combined classifier (WV), showing ten different estimates (dashed curves) and their average (solid curve).
###end p 49
###begin title 50
Stress-response predictions
###end title 50
###begin p 51
###xml 318 319 318 319 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="table" rid="T1">1</xref>
###xml 712 715 712 715 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i.e</italic>
###xml 979 982 979 982 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i.e</italic>
We trained the combined classifier on our Arabidopsis data set, using all 22,746 genes for Principal Components Analysis, and the 11,553 annotated genes for supervised learning, as described above. We then applied the classifier to the 11,193 unannotated genes, to get a set of 11,193 predictions (see Methods). Table 1 shows the top fifty predictions. Each row in the table is a prediction: the first (leftmost) entry is the rank of the prediction (1 being the top prediction); the second entry identifies a gene; the third entry is a discriminant value (measuring the likelihood that the gene responds to stress); and the fourth entry is the estimated precision of the prediction and all predictions above it (i.e., the fraction of these predictions expected to be true). As an example, consider the 23rd row of the table, the row for gene At1g09950. Since the estimated precision in this row is given as 0.7044, we expect that about 70% of the top 23 genes respond to stress, i.e., 16 genes.
###end p 51
###begin p 52
The top 50 predictions of the combined classifier ordered by discriminant value
###end p 52
###begin p 53
Pr, estimated precision; Dv, discriminant value.
###end p 53
###begin p 54
###xml 8 9 8 9 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F9">9</xref>
###xml 14 16 14 16 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F10">10</xref>
###xml 299 301 299 301 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B23">23</xref>
###xml 509 511 509 511 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B22">22</xref>
###xml 512 514 512 514 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B24">24</xref>
###xml 515 517 515 517 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B36">36</xref>
###xml 737 739 737 739 <sub xmlns:xlink="http://www.w3.org/1999/xlink">2 </sub>
###xml 1489 1490 1489 1490 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F9">9</xref>
###xml 1495 1497 1495 1497 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F10">10</xref>
###xml 1695 1696 1695 1696 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F9">9</xref>
###xml 1770 1773 1770 1773 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i.e</italic>
###xml 1863 1865 1863 1865 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F10">10</xref>
###xml 1985 1986 1985 1986 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F9">9</xref>
###xml 2002 2004 2002 2004 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F10">10</xref>
Figures 9 and 10 provide visual evidence supporting these predictions. Each figure shows a heat map. These maps, known as "electronic Northerns" (or e-Northerns), were generated using the Expression Browser tool of the Botany Array Resource (BAR) and the AtGenExpress Stress Series (shoot) data set[23]. The program contains expression data for more than 22,000 genes across more than 1000 samples collected from NASCArrays, AtGenExpress Consortium, and the Department of Botany at the University of Toronto [22-24,36]. Each row in an e-Northern is a gene, and each column is an experiment. The colour at a point represents the relative expression level of the gene during the experiment. More specifically, the colour represents the log2 of the ratio of the average of replicate treatments relative to the average of corresponding controls. Yellow means that under the experimental conditions, the gene had the same expression level as the control. (The wide, yellow vertical stripes are the controls.) Red means that the gene had a higher expression level than the control (up-regulation), and blue means it had a lower expression level (down-regulation). A gene that shows significant up-regulation (or down-regulation) under stress conditions is likely to be involved in response to stress. Thus, unlike cross validation, electronic Northerns provide a means of evaluating the quality of predictions based on the prediction data, not just the training data. The e-Northerns of Figures 9 and 10, for instance, are based entirely on prediction data. In these e-Northerns, the experiments exposed the plant to various stress conditions, such as heat, cold, drought, UV-B radiation, etc. Figure 9 is the e-Northern for the top-50 predictions of our combined classifier, i.e., for the 50 genes predicted to most likely to respond to stress. For comparison, Figure 10 is the e-Northern for 50 genes chosen at random from the prediction set. Note that there is much more colour in Figure 9 than in Figure 10, especially red. This suggests that our combined classifier has indeed extracted meaningful gene expression patterns for genes that respond to stress.
###end p 54
###begin p 55
###xml 0 28 0 28 <bold xmlns:xlink="http://www.w3.org/1999/xlink">Electronic Northern analysis</bold>
Electronic Northern analysis. E-Northern of the top 50 predictions.
###end p 55
###begin p 56
###xml 0 28 0 28 <bold xmlns:xlink="http://www.w3.org/1999/xlink">Electronic Northern analysis</bold>
Electronic Northern analysis. E-Northern of 50 randomly selected genes.
###end p 56
###begin title 57
Gene knockout experiments
###end title 57
###begin p 58
From the predictions of the combined classifier, three genes were chosen for biological analysis using gene knockout experiments. Here, we present the results for one of these genes, At1g16850, which show it to be necessary for the normal response to temperature and NaCl. Our results also confirm that the other two genes, At1g11210 and At4g39675, are involved in a variety of stress responses (data not shown).
###end p 58
###begin p 59
###xml 394 396 394 396 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B37">37</xref>
###xml 582 584 582 584 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B38">38</xref>
###xml 637 639 637 639 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B22">22</xref>
###xml 716 718 716 718 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B39">39</xref>
###xml 598 604 <span type="species:ncbi:8074">Angler</span>
The criteria used to choose candidate genes for subsequent biological analysis were: 1) the gene must be expressed in either root or shoot, 2) gene expression should be strongly increased in response to abiotic stress, such as cold, drought, osmotic and salt stresses, 3) T-DNA knockout lines - in which a given gene's expression has been eliminated - should available from the Salk Institute [37], and 4) the gene should not have an annotated function nor be present in any patent database. Further bioinformatics analysis was performed using Athena for promoter motif prediction [38], Expression Angler for co-expressed gene analysis [22] and eFP browser for electronic representation of gene expression patterns [39].
###end p 59
###begin title 60
Stress response
###end title 60
###begin p 61
###xml 197 199 197 199 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F11">11</xref>
###xml 316 318 316 318 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F11">11</xref>
###xml 708 710 708 710 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B40">40</xref>
###xml 711 713 711 713 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B41">41</xref>
###xml 790 792 790 792 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B40">40</xref>
###xml 793 795 793 795 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B41">41</xref>
###xml 946 948 946 948 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B22">22</xref>
###xml 928 934 <span type="species:ncbi:8074">Angler</span>
The increased presence of anthocyanin levels in plants lacking a functional copy of the At1g16850 gene during cold stress of 4C indicates that this gene is involved in cold stress response (Figure 11). The same effect is seen at 30C, indicating that this gene is also associated with response to heat stress (Figure 11). Interestingly, At1g16850 is normally expressed during the later stages of seed maturation, towards seed dessication, and hence may play a role in seed dormancy. This sort of bifunctionality is seen with other stress response genes, which have documented roles in the cold, heat and salt stress pathways, e.g. RD29A (Response to Desiccation) and LEA (Late Embryogensis Abundant) protein [40,41]. These proteins have also been found to accumulate during seed maturation [40,41] and are in fact co-expressed with At1g16850 under stress conditions and during seed maturation, as determined using the Expression Angler algorithm [22].
###end p 61
###begin p 62
###xml 0 25 0 25 <bold xmlns:xlink="http://www.w3.org/1999/xlink">Gene knockout experiments</bold>
###xml 484 486 484 486 <italic xmlns:xlink="http://www.w3.org/1999/xlink">p </italic>
Gene knockout experiments. 10 day old wild-type and mutant plants after exposure for 7 days at 14. (a) The mutant cotyledons appear darker than wild-type due to increased anthocyanin levels. (b) mutant and wild-type seeds 24 h after sowing on agar plates. Mutant seeds have the appearance of lighter colour compared to wild-type. (c) Quantification of anthocyanin levels measuring A535. Bars indicate standard error of 5 replicate measurements. * indicates significantly different at p < 0.05
###end p 62
###begin p 63
###xml 178 180 178 180 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="fig" rid="F12">12</xref>
###xml 242 244 242 244 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B42">42</xref>
In addition to modulating a response to temperature, plants lacking a functional At1g16850 exhibit a defective root growth phenotype under increasing salt concentrations (Figure 12). This phenotype, combined with previous microarray studies [42], which found At1g16850 induction at 250 mM NaCl, gives clear indication that At1g16850 is also part of the salt stress response pathway.
###end p 63
###begin p 64
###xml 0 25 0 25 <bold xmlns:xlink="http://www.w3.org/1999/xlink">Gene knockout experiments</bold>
###xml 219 221 219 221 <italic xmlns:xlink="http://www.w3.org/1999/xlink">n </italic>
###xml 306 308 306 308 <italic xmlns:xlink="http://www.w3.org/1999/xlink">p </italic>
Gene knockout experiments. Root growth on 50 mM NaCl, relative to growth on 0 mM NaCl, on 10 day old wild-type and mutant plants transferred to 50 mM NaCl medium. Error bars indicate the standard error of 5 replicates. n = 25 measurements per treatment and genotype. * indicates significantly different at p < 0.001
###end p 64
###begin title 65
Conclusion
###end title 65
###begin p 66
###xml 139 151 139 151 <italic xmlns:xlink="http://www.w3.org/1999/xlink">A. thaliana </italic>
###xml 536 568 536 568 <italic xmlns:xlink="http://www.w3.org/1999/xlink">GO:0006950 [response to stress] </italic>
###xml 790 793 790 793 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 139 150 <span type="species:ncbi:3702">A. thaliana</span>
In this study, we evaluated and compared five basic supervised learning methods (LR, LDA, QDA, NB and KNN) for gene function prediction in A. thaliana based solely on gene expression data. The major advantage of supervised methods over unsupervised methods is that by including prior knowledge of class information, supervised methods can ignore uninformative features and select informative features that are useful for separating classes. In this study, we focussed on finding genes that respond to stress, as represented by the term GO:0006950 [response to stress] in the GOBP hierarchy. Using a training set of genes of known function, we used the basic learning methods to predict the stress response of genes of unknown function. We estimated the accuracy of the predictions using ROC50 scores derived through cross validation. We found, for instance, that KNN performs well for various values of K. For the other learning methods, the performance depends greatly on whether the data is preprocessed using PCA, and on how much its dimensionality is reduced. Using various values of K and various amounts of dimensionality reduction, we trained and tested a total of 33 basic classifiers.
###end p 66
###begin p 67
###xml 827 829 827 829 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B21">21</xref>
###xml 937 940 937 940 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 1029 1032 1029 1032 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 1121 1132 1121 1132 <italic xmlns:xlink="http://www.w3.org/1999/xlink">A. thaliana</italic>
###xml 1121 1132 <span type="species:ncbi:3702">A. thaliana</span>
We also investigated combining the basic classifiers using weighted voting. Our method of constructing the combined classifier chooses not only the best combination of supervised learning methods, but also the best amount of dimensionality reduction for each method. Our results show that the combined classifier outperforms all the basic classifiers in predicting whether a gene responds to stress. This can be attributed to the relative robustness of methods for combining classifiers. Intuitively, any single learning method represents a single view of the data, while a combination method represents multiple views strategically combined. The proper choice of combining method is important to the success of a combined classifier. For example, model averaging and stacking are well-known methods for combining classifiers [21]; however, we found that while they did improve on the overall ROC curves of the basic classifiers, the ROC50 curve was often worse (data not shown). In contrast, our weighted voting method using ROC50 scores as weights is simple, provides improved accuracy in predicting stress response in A. thaliana, and we would expect it to provide improved accuracy in other organisms and for other gene functions.
###end p 67
###begin p 68
Using electronic Northern analysis, we observed significant up-regulation and down-regulation of many of our predictions. The strong up- and down-regulation are also present among the stress-response genes in the training data (data not shown). In contrast, randomly selected genes show much less up- and down-regulation. This visually confirms that the combined classifier is able to distinguish between stress and non-stress genes. Moreover, unlike cross-validation, this confirmation is based on the prediction data, not the training data.
###end p 68
###begin p 69
Using gene knockout experiments - in which a given gene's expression is eliminated - we tested three of our predictions. We presented the results for one of these genes, At1g16850, which show it to be involved in the stress response pathways to cold (4C), chill (14C) and NaCl. We have also confirmed the biological stress responsive roles of the other two genes, At1g11210 and At4g39675 (data not shown). Further biological studies will determine the pattern of expression in specific cell and tissues types of the plant and the exact physiological role of these genes.
###end p 69
###begin title 70
Methods
###end title 70
###begin title 71
Preprocessing of raw gene expression data
###end title 71
###begin p 72
###xml 93 108 93 108 <italic xmlns:xlink="http://www.w3.org/1999/xlink">detection calls</italic>
###xml 333 343 333 343 <italic xmlns:xlink="http://www.w3.org/1999/xlink">At3g24440 </italic>
The gene expression data from the Botany Array Resource at the University of Toronto contain detection calls: P (present), M (marginal) and A (absent). The detection call determines whether a transcript is reliably detected (present), partially detected (marginal), or not detected (absent). The following is an example for the gene At3g24440 under three selected conditions:
###end p 72
###begin p 73
AT3G24440 : 243.10 P : 120.90 A : 109.40 M
###end p 73
###begin p 74
We simply removed these detection calls (P, A, and M) in this study. In addition, gene expression levels were log transformed. The transformed data have approximately normal distributions while the raw data have approximately exponential distributions (data not shown). Many of the learning methods used in this study were designed with normal data in mind.
###end p 74
###begin title 75
Basic supervised learning methods
###end title 75
###begin p 76
###xml 256 257 256 257 <bold xmlns:xlink="http://www.w3.org/1999/xlink">x</bold>
###xml 338 340 338 340 <italic xmlns:xlink="http://www.w3.org/1999/xlink">dv</italic>
###xml 341 342 341 342 <bold xmlns:xlink="http://www.w3.org/1999/xlink">x</bold>
###xml 465 467 465 467 <italic xmlns:xlink="http://www.w3.org/1999/xlink">dv</italic>
###xml 468 469 468 469 <bold xmlns:xlink="http://www.w3.org/1999/xlink">x</bold>
###xml 473 476 473 474 <italic xmlns:xlink="http://www.w3.org/1999/xlink">&#964;</italic>
###xml 484 488 482 484 <italic xmlns:xlink="http://www.w3.org/1999/xlink">&#964; </italic>
###xml 595 596 591 592 <italic xmlns:xlink="http://www.w3.org/1999/xlink">p</italic>
###xml 597 599 593 595 <italic xmlns:xlink="http://www.w3.org/1999/xlink">k </italic>
###xml 603 604 599 600 <bold xmlns:xlink="http://www.w3.org/1999/xlink">x</bold>
Each of the learning methods described below trains a discriminative classifier. We used the methods to train binary classifiers in which the two classes correspond to genes that respond to stress (Class 1) and genes that do not (Class 0). Given a vector, x, of gene expression measurements, each classifier returns a discriminant value, dv(x), reflecting the classifier's confidence that the gene belongs to Class 1. The gene is assigned to Class 1 if and only if dv(x) > tau, where tau is a decision threshold. For the classifiers LR, LDA, QDA and NB, the discriminate value is an estimate of p(k = 1|x), the posterior probability that the gene is in Class 1. For KNN, the discriminant value is simply a number between 0 and 1.
###end p 76
###begin title 77
LR (Logistic Regression)
###end title 77
###begin p 78
###xml 124 125 124 125 <bold xmlns:xlink="http://www.w3.org/1999/xlink">x</bold>
Given a set of classes, LR models the log likelihood ratio for any pair of classes as a linear function of the test vector, x, and thus defines linear decision boundaries between the classes. In the case of just two classes, the model has the simple form
###end p 78
###begin p 79

###end p 79
###begin p 80
and hence,
###end p 80
###begin p 81

###end p 81
###begin p 82

###end p 82
###begin p 83
###xml 4 5 4 5 <italic xmlns:xlink="http://www.w3.org/1999/xlink">p</italic>
###xml 6 8 6 8 <italic xmlns:xlink="http://www.w3.org/1999/xlink">k </italic>
###xml 12 13 12 13 <bold xmlns:xlink="http://www.w3.org/1999/xlink">x</bold>
###xml 17 18 17 18 <italic xmlns:xlink="http://www.w3.org/1999/xlink">p</italic>
###xml 19 21 19 21 <italic xmlns:xlink="http://www.w3.org/1999/xlink">k </italic>
###xml 25 26 25 26 <bold xmlns:xlink="http://www.w3.org/1999/xlink">x</bold>
###xml 48 52 48 49 <italic xmlns:xlink="http://www.w3.org/1999/xlink">&#946;</italic>
###xml 52 54 49 51 <sub xmlns:xlink="http://www.w3.org/1999/xlink">0 </sub>
###xml 58 62 55 56 <italic xmlns:xlink="http://www.w3.org/1999/xlink">&#946;</italic>
###xml 62 64 56 58 <sub xmlns:xlink="http://www.w3.org/1999/xlink">1 </sub>
###xml 122 124 116 118 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B21">21</xref>
and p(k = 1|x) + p(k = 0|x) = 1. The parameters beta0 and beta1 are fitted to the training data using maximum likelihood [21].
###end p 83
###begin title 84
LDA (Linear Discriminant Analysis)
###end title 84
###begin p 85
###xml 145 147 145 147 <italic xmlns:xlink="http://www.w3.org/1999/xlink">k </italic>
LDA models the classes as multivariate Gaussians, where each class is assumed to have the same covariance matrix. The density function for class k is therefore given by
###end p 85
###begin p 86

###end p 86
###begin p 87
###xml 6 8 6 7 <italic xmlns:xlink="http://www.w3.org/1999/xlink">&#956;</italic>
###xml 8 10 7 9 <italic xmlns:xlink="http://www.w3.org/1999/xlink">k </italic>
###xml 8 10 7 9 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>k </italic></sub>
###xml 39 40 38 39 <italic xmlns:xlink="http://www.w3.org/1999/xlink">k</italic>
###xml 85 87 80 82 <italic xmlns:xlink="http://www.w3.org/1999/xlink">p </italic>
###xml 112 113 107 108 <bold xmlns:xlink="http://www.w3.org/1999/xlink">x</bold>
###xml 132 134 127 129 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B21">21</xref>
###xml 177 179 172 174 <italic xmlns:xlink="http://www.w3.org/1999/xlink">k </italic>
where muk is the mean vector for class k, Sigma is the common covariance matrix, and p is the dimensionality of x. It can be shown [21] that the discriminant function for class k is equivalent to the following function:
###end p 87
###begin p 88

###end p 88
###begin p 89
###xml 6 8 6 7 <italic xmlns:xlink="http://www.w3.org/1999/xlink">&#960;</italic>
###xml 8 10 7 9 <italic xmlns:xlink="http://www.w3.org/1999/xlink">k </italic>
###xml 8 10 7 9 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>k </italic></sub>
###xml 44 45 43 44 <italic xmlns:xlink="http://www.w3.org/1999/xlink">k</italic>
###xml 108 110 107 108 <italic xmlns:xlink="http://www.w3.org/1999/xlink">&#960;</italic>
###xml 110 111 108 109 <italic xmlns:xlink="http://www.w3.org/1999/xlink">k</italic>
###xml 110 111 108 109 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>k</italic></sub>
###xml 113 115 111 112 <italic xmlns:xlink="http://www.w3.org/1999/xlink">&#956;</italic>
###xml 115 117 112 114 <italic xmlns:xlink="http://www.w3.org/1999/xlink">k </italic>
###xml 115 117 112 114 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>k </italic></sub>
###xml 194 196 187 189 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B21">21</xref>
where pik is the prior probability of class k. The decision boundaries and therefore linear. The parameters pik, muk and Sigma are estimated by applying maximum likelihood to the training data [21], giving
###end p 89
###begin p 90

###end p 90
###begin p 91

###end p 91
###begin p 92

###end p 92
###begin p 93
###xml 6 8 6 8 <italic xmlns:xlink="http://www.w3.org/1999/xlink">n </italic>
###xml 49 50 49 50 <italic xmlns:xlink="http://www.w3.org/1999/xlink">n</italic>
###xml 50 52 50 52 <italic xmlns:xlink="http://www.w3.org/1999/xlink">k </italic>
###xml 50 52 50 52 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>k </italic></sub>
###xml 95 96 95 96 <italic xmlns:xlink="http://www.w3.org/1999/xlink">k</italic>
###xml 102 104 102 104 <italic xmlns:xlink="http://www.w3.org/1999/xlink">K </italic>
###xml 145 147 145 147 <italic xmlns:xlink="http://www.w3.org/1999/xlink">K </italic>
where n is the total number of training samples, nk is the number of training samples in class k, and K is the number of classes. In this study, K = 2.
###end p 93
###begin title 94
QDA (Quadratic Discriminant Analysis)
###end title 94
###begin p 95
###xml 83 84 83 84 <italic xmlns:xlink="http://www.w3.org/1999/xlink">k</italic>
###xml 83 84 83 84 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>k</italic></sub>
###xml 117 119 117 119 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B21">21</xref>
###xml 162 164 162 164 <italic xmlns:xlink="http://www.w3.org/1999/xlink">k </italic>
QDA is a generalization of LDA in which each class has its own covariance matrix, Sk. In this case, it can be shown [21] that the discriminant function for class k is equivalent to the following function:
###end p 95
###begin p 96

###end p 96
###begin p 97
###xml 138 140 138 140 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B21">21</xref>
The decision boundaries are therefore quadratic. Again, the parameters are estimated by applying maximum likelihood to the training data [21].
###end p 97
###begin title 98
NB (Naive Bayes)
###end title 98
###begin p 99
###xml 104 106 104 106 <bold xmlns:xlink="http://www.w3.org/1999/xlink">x </bold>
###xml 190 191 190 191 <italic xmlns:xlink="http://www.w3.org/1999/xlink">p</italic>
###xml 192 193 192 193 <italic xmlns:xlink="http://www.w3.org/1999/xlink">x</italic>
###xml 193 194 193 194 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i</italic>
###xml 193 194 193 194 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>i</italic></sub>
###xml 195 196 195 196 <italic xmlns:xlink="http://www.w3.org/1999/xlink">k</italic>
###xml 244 245 244 245 <italic xmlns:xlink="http://www.w3.org/1999/xlink">x</italic>
###xml 245 246 245 246 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i</italic>
###xml 245 246 245 246 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>i</italic></sub>
###xml 385 386 385 386 <italic xmlns:xlink="http://www.w3.org/1999/xlink">k</italic>
###xml 424 426 424 426 <bold xmlns:xlink="http://www.w3.org/1999/xlink">x </bold>
###xml 429 430 429 430 <italic xmlns:xlink="http://www.w3.org/1999/xlink">x</italic>
###xml 430 431 430 431 <sub xmlns:xlink="http://www.w3.org/1999/xlink">1</sub>
###xml 433 434 433 434 <italic xmlns:xlink="http://www.w3.org/1999/xlink">x</italic>
###xml 434 435 434 435 <sub xmlns:xlink="http://www.w3.org/1999/xlink">2</sub>
###xml 442 443 442 443 <italic xmlns:xlink="http://www.w3.org/1999/xlink">x</italic>
###xml 443 444 443 444 <italic xmlns:xlink="http://www.w3.org/1999/xlink">p</italic>
###xml 443 444 443 444 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>p</italic></sub>
###xml 445 447 445 447 <italic xmlns:xlink="http://www.w3.org/1999/xlink">T </italic>
###xml 445 447 445 447 <sup xmlns:xlink="http://www.w3.org/1999/xlink"><italic>T </italic></sup>
NB is based on the independent variable assumption: for each class, the variables in the feature vector x are assumed to be independent. This assumption allows the class conditional density p(xi|k) to be estimated separately for each variable, xi. In essence, NB reduces the problem of multi-dimensional density estimation to that of one-dimensional density estimation. Given a class, k, each variable in the feature vector x = (x1, x2, ..., xp)T is independent; so
###end p 99
###begin p 100

###end p 100
###begin p 101
Using Bayes Rule, we obtain
###end p 101
###begin p 102

###end p 102
###begin p 103
###xml 6 7 6 7 <italic xmlns:xlink="http://www.w3.org/1999/xlink">p</italic>
###xml 8 9 8 9 <italic xmlns:xlink="http://www.w3.org/1999/xlink">k</italic>
###xml 45 46 45 46 <italic xmlns:xlink="http://www.w3.org/1999/xlink">k</italic>
###xml 118 120 118 120 <italic xmlns:xlink="http://www.w3.org/1999/xlink">k </italic>
###xml 228 229 228 229 <italic xmlns:xlink="http://www.w3.org/1999/xlink">p</italic>
###xml 230 231 230 231 <italic xmlns:xlink="http://www.w3.org/1999/xlink">x</italic>
###xml 231 232 231 232 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i</italic>
###xml 231 232 231 232 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>i</italic></sub>
###xml 233 234 233 234 <italic xmlns:xlink="http://www.w3.org/1999/xlink">k</italic>
###xml 238 239 238 239 <italic xmlns:xlink="http://www.w3.org/1999/xlink">N</italic>
###xml 339 341 339 341 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B21">21</xref>
###xml 549 551 549 551 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B21">21</xref>
where p(k) is the prior probability of class k, estimated as the ratio of the number of the training samples in class k to the total number of training samples. In this paper, we model each variable as a univariate Gaussian, so p(xi|k) = N(, ), where the parameters  and  are estimated by applying maximum likelihood to the training data [21]. Note that NB has far fewer parameters to estimate than either LDA or QDA, and for this reason, it often performs surprisingly well in practise, despite the unrealistic assumption of independent variables [21].
###end p 103
###begin title 104
KNN (K-Nearest Neighbors)
###end title 104
###begin p 105
###xml 150 152 150 152 <italic xmlns:xlink="http://www.w3.org/1999/xlink">K </italic>
###xml 195 196 195 196 <italic xmlns:xlink="http://www.w3.org/1999/xlink">K</italic>
###xml 196 198 196 198 <sub xmlns:xlink="http://www.w3.org/1999/xlink">1 </sub>
###xml 248 249 248 249 <italic xmlns:xlink="http://www.w3.org/1999/xlink">K</italic>
###xml 249 250 249 250 <sub xmlns:xlink="http://www.w3.org/1999/xlink">1</sub>
###xml 250 253 250 253 <italic xmlns:xlink="http://www.w3.org/1999/xlink">/K </italic>
###xml 356 357 356 357 <italic xmlns:xlink="http://www.w3.org/1999/xlink">K</italic>
###xml 357 358 357 358 <sub xmlns:xlink="http://www.w3.org/1999/xlink">1</sub>
###xml 358 361 358 361 <italic xmlns:xlink="http://www.w3.org/1999/xlink">/K </italic>
###xml 363 366 363 364 <italic xmlns:xlink="http://www.w3.org/1999/xlink">&#964;</italic>
###xml 374 378 372 374 <italic xmlns:xlink="http://www.w3.org/1999/xlink">&#964; </italic>
KNN is a nonparametric method, since it does not require the estimation of any parameters. Instead, to classify a test vector, KNN finds the vector's K nearest neighbors in the training data. If K1 is the number of these neighbors in Class 1, then K1/K is returned as the discriminant value. The test vector is therefore assigned to Class 1 if and only if K1/K > tau, where tau is the decision threshold.
###end p 105
###begin p 106
###xml 138 141 138 139 <italic xmlns:xlink="http://www.w3.org/1999/xlink">&#961;</italic>
###xml 149 153 147 149 <italic xmlns:xlink="http://www.w3.org/1999/xlink">&#961; </italic>
###xml 244 246 240 242 <bold xmlns:xlink="http://www.w3.org/1999/xlink">x </bold>
###xml 250 251 246 247 <bold xmlns:xlink="http://www.w3.org/1999/xlink">y</bold>
A variety of different distance measures can be used with KNN to measure the nearness of one vector to another. In this paper, we use 1 - rho, where rho is the Pearson correlation coefficient of the two vectors. That is, if the two vectors are x and y, then
###end p 106
###begin p 107

###end p 107
###begin p 108
###xml 290 291 290 291 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B2">2</xref>
In terms of gene expression measurements, two genes are highly correlated if their expression levels tend to rise and fall together (even though their absolute expression levels may be quite different). For this reason, Pearson correlation is often used to detect coregulation among genes [2].
###end p 108
###begin title 109
Principal components analysis
###end title 109
###begin p 110
###xml 511 513 511 513 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B21">21</xref>
Hidden dependencies and noise among experiments may confound the classification problem. In particular, experiments that are biologically different may actually be similar in terms of gene expression. Principal components analysis (PCA) helps to identify independent information in the data by transforming it to a data set of reduced dimension. The attributes of the reduced data set, called principal components, explain most of the variance in the original data and are mutually uncorrelated and orthogonal [21]. In addition, by reducing the dimension of the data, PCA reduces the number of parameters that must be estimated during supervised learning, thus permitting more efficient use of the data.
###end p 110
###begin p 111
One can think of PCA as having a learning phase and a prediction phase. During learning, PCA is given a data set, from which it generates (learns) a linear transformation. This transformation maps high-dimensional vectors to low-dimensional vectors, and is applied to the given data set to reduce its dimensionality. During prediction, the transformation is applied to other data.
###end p 111
###begin p 112
###xml 105 107 105 107 <italic xmlns:xlink="http://www.w3.org/1999/xlink">p </italic>
###xml 123 125 123 125 <italic xmlns:xlink="http://www.w3.org/1999/xlink">p </italic>
###xml 217 220 217 220 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i.e</italic>
We used PCA to reduce the dimensionality of the gene expression data from its original 290 dimensions to p dimensions, for p = 5, 10, 15, 20, 40, 100. During learning, we gave PCA our entire data set of 22,746 genes, i.e., the 11,533 annotated genes and the 11,193 unannotated genes. This is possible because PCA is a form of unsupervised learning, so it uses only the gene expression measurements (which are known), and not the gene annotations (which are to be learned). This increases the effectiveness of PCA by doubling the amount of data that it uses during learning. That is, using a larger data set decreases the variance of the principal components learned by PCA, thus increasing their statistical significance and reducing the number of anomalous components.
###end p 112
###begin p 113
###xml 217 220 217 220 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i.e</italic>
###xml 818 820 818 820 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B29">29</xref>
###xml 821 823 821 823 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B31">31</xref>
It is worth noting that this use of PCA is different from that of many traditional applications of machine learning. This is because we apply PCA to the entire data set during learning, including the prediction data (i.e., the unannotated data). This is not possible in traditional applications simply because the prediction data is not known during learning. In such applications, a learning procedure is first trained and tested on one data set, and then applied to prediction data as it becomes available. This is not the situation for genome-wide expression experiments, since all the genes (and their expression levels) are known in advance, including the genes in the prediction set. PCA can therefore use both the prediction data and the training data during learning. This is a form of transductive inference [29,31], in which the prediction data is known and exploited during learning.
###end p 113
###begin title 114
PCA and classifier evaluation
###end title 114
###begin p 115
###xml 207 209 207 209 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B29">29</xref>
###xml 210 212 210 212 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B30">30</xref>
After PCA is performed on the entire data set, supervised learning is performed on the annotated portion of the dimensionally-reduced data. (As described earlier, this is a form of semi-supervised learning [29,30]). The result is a set of classifiers, one for each supervised learning method. The classifiers are then applied to the unannotated portion of the dimensionally-reduced data to predict the missing annotations. Cross validation was used to estimate the accuracy of these predictions.
###end p 115
###begin p 116
###xml 169 171 169 171 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B21">21</xref>
###xml 719 724 719 724 <italic xmlns:xlink="http://www.w3.org/1999/xlink">both </italic>
###xml 763 771 763 771 <italic xmlns:xlink="http://www.w3.org/1999/xlink">neither </italic>
Before discussing our use of cross validation, we consider the simpler setting in which the annotated data is divided into two parts, training data and validation data [21]. This will clarify our handling of PCA during validation. In this setting, classifier evaluation proceeds as follows. First, PCA is applied to the entire data set (training, validation and prediction data) to produce a dimensionally-reduced data set. Then, a supervised learner uses the (dimensionally-reduced) training data to produce a classifier. Finally, the accuracy of the classifier is estimated using the (dimensionally-reduced) validation data. Note that this process treats the validation and prediction data equally. That is, they are both used during unsupervised learning, and neither is used during supervised learning. In this way, the validation data is representative of the prediction data, as it should be. Also note that PCA is now effectively a preprocessing phase prior to supervised learning.
###end p 116
###begin p 117
###xml 204 208 204 208 <italic xmlns:xlink="http://www.w3.org/1999/xlink">not </italic>
###xml 270 273 270 273 <italic xmlns:xlink="http://www.w3.org/1999/xlink">e.g</italic>
###xml 476 488 476 488 <italic xmlns:xlink="http://www.w3.org/1999/xlink">Arabidopsis </italic>
Because PCA is applied to the entire data set, this validation process estimates the accuracy of the classifier on prediction data that is known and used during learning. In particular, the estimate does not apply to new prediction data that might arrive in the future (e.g., if new genes were discovered). In fact, it would likely be an overestimate of classifier accuracy on such data. However, this is not an issue in our application, since most if not all of the genes in Arabidopsis are already known. Moreover, even if some new genes were to be discovered in the future, we could simply add them to our prediction data and retrain the classifier on the enlarged data set.
###end p 117
###begin p 118
The above ideas are easily extended to cross validation. First, PCA is applied to the entire data set. Then, a supervised learner uses the annotated portion of the dimensionally-reduced data to produce a classifier. Finally, this classifier is evaluated by cross validation in the normal way, as described below. Note that this approach has the added computational advantage that PCA is applied only once, to the entire data set, and not over-and-over again during the many training phases of cross validation. The discussions below assume that the entire data set has been preprocessed using PCA, so that all references to data refer to the dimensionally-reduced data. Also, all references to generalization performance refer to the accuracy of the classifier on the given set of prediction data.
###end p 118
###begin title 119
Cross validation
###end title 119
###begin p 120
###xml 330 333 330 333 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i.e</italic>
###xml 554 556 554 556 <italic xmlns:xlink="http://www.w3.org/1999/xlink">dv</italic>
###xml 673 676 673 676 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i.e</italic>
###xml 716 719 716 719 <italic xmlns:xlink="http://www.w3.org/1999/xlink">dv </italic>
###xml 721 724 721 722 <italic xmlns:xlink="http://www.w3.org/1999/xlink">&#964;</italic>
###xml 732 736 730 732 <italic xmlns:xlink="http://www.w3.org/1999/xlink">&#964; </italic>
###xml 854 857 850 853 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 977 980 973 976 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 1064 1067 1060 1063 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
We used 20-fold cross-validation to assess the generalization performance of each classifier as well as to estimate the precision of its predictions. We randomly divided the annotated data into 20 non-overlapping, equal-sized parts, called folds. The classifier was trained on 19 of these folds, and tested on the remaining fold; i.e., the trained classifier was used to generate a discriminant value for each gene in the remaining fold. This was done in all 20 possible ways, using a different testing fold each time. In this way, a discriminant value, dv, was generated for every gene in the training set. Each gene in the training set was then predicted to be positive (i.e., to respond to stress) if and only if dv > tau, where tau is a decision threshold. From these predictions, true and false positives were computed, from which a point on the ROC50 curve was plotted. Using a large number of different decision thresholds, we plotted a large number of points on the ROC50 curve, effectively generating the entire curve. The area under this curve is the ROC50 score. To get an idea of how stable the estimated performance of the classifier is, we repeated the entire cross-validation and curve-generation procedure 10 times, each time using a different, random, 20-fold split of the training data.
###end p 120
###begin p 121
###xml 300 303 300 303 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 427 430 427 430 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 590 593 590 593 <italic xmlns:xlink="http://www.w3.org/1999/xlink">th </italic>
###xml 590 593 590 593 <sup xmlns:xlink="http://www.w3.org/1999/xlink"><italic>th </italic></sup>
###xml 624 627 624 627 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 654 667 654 667 <italic xmlns:xlink="http://www.w3.org/1999/xlink">all 20 folds </italic>
###xml 755 758 755 758 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i.e</italic>
The above procedure was applied to all the basic classifiers, but assessing the combined classifier involved an additional subtlety. Recall that the combined classifier is a linear combination of the basic classifiers, where the weight given to a basic classifier is proportional to its estimated ROC50 score. The subtlety is in computing that score. A naive approach would be to simply use the above procedure to compute a ROC50 score for each basic classifier. However, this would mean that during cross validation, 19 of the 20 folds are used to train the basic classifiers, while the 20th fold is used to compute the ROC50 scores. The result is that all 20 folds are involved in computing the weights. Thus, all 20 folds are involved in constructing (i.e., training) the combined classifier, so no folds are left for testing it. If cross validation were used anyway to assess the combined classifier, it would amount to using training data as testing data, and the results would tend to overestimate the classifier's performance.
###end p 121
###begin p 122
###xml 167 170 167 170 <italic xmlns:xlink="http://www.w3.org/1999/xlink">th </italic>
###xml 167 170 167 170 <sup xmlns:xlink="http://www.w3.org/1999/xlink"><italic>th </italic></sup>
###xml 203 206 203 206 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 224 227 224 227 <italic xmlns:xlink="http://www.w3.org/1999/xlink">th </italic>
###xml 224 227 224 227 <sup xmlns:xlink="http://www.w3.org/1999/xlink"><italic>th </italic></sup>
###xml 310 317 310 317 <italic xmlns:xlink="http://www.w3.org/1999/xlink">nested </italic>
###xml 604 607 604 607 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
###xml 900 902 900 902 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B21">21</xref>
As described earlier, we surmount this problem by using two sets of validation data. Loosely speaking, 18 of the 20 folds are used to train the basic classifiers, a 19th fold is used to compute their ROC50 scores, and the 20th fold is used to test the combined classifier. This results in what might be called nested cross validation. To start, the training data are divided randomly into 20 folds. Picking one of these as a testing fold, the other 19 are used to train the combined classifier. This in turn involves 19-fold cross validation to train and test the basic classifiers (and compute their ROC50 scores). Thus, each time the combined classifier is trained once, the basic classifiers are trained 19 times. Since the combined classifier is trained 20 times, each basic classifier is trained a total of 20 x 19 = 380 times. A similar form of nested cross validation is involved in Stacking [21].
###end p 122
###begin title 123
Predicting gene function and estimating precision
###end title 123
###begin p 124
###xml 284 286 284 286 <italic xmlns:xlink="http://www.w3.org/1999/xlink">dv</italic>
###xml 393 394 393 394 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="table" rid="T1">1</xref>
###xml 599 602 599 602 <italic xmlns:xlink="http://www.w3.org/1999/xlink">dv </italic>
###xml 757 759 757 759 <italic xmlns:xlink="http://www.w3.org/1999/xlink">dv</italic>
###xml 816 826 816 826 <italic xmlns:xlink="http://www.w3.org/1999/xlink">precision </italic>
###xml 1184 1186 1184 1186 <italic xmlns:xlink="http://www.w3.org/1999/xlink">dv</italic>
To predict which genes respond to stress, we first train a combined classifier using the 11,553 annotated genes in the training data. The classifier is then applied to the 11,193 unannotated genes in the prediction data. After this step, each annotated gene has a discriminant value, dv. The unannotated genes are then sorted in descending order by discriminant value, as illustrated in Table 1. To make actual predictions, a gene in the sorted list is chosen as a decision point. This gene and every gene above it in the sorted list are then predicted to respond to stress. In other words, suppose dv is the discriminant value of the chosen gene. An unannotated gene is then predicted to respond to stress if and only if its discriminant value is at least dv. The fraction of these predictions that are true is the precision of the predictions. We estimate this precision using the training data. Recall that each gene in the training set has a discriminant value assigned to it during cross validation. We also know which of these genes respond to stress. To estimate the precision of our predictions, we look at those genes in the training set whose discriminant value is at least dv. The fraction of them that respond to stress is an estimate of precision.
###end p 124
###begin p 125
###xml 503 505 503 505 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i </italic>
###xml 559 561 551 553 <italic xmlns:xlink="http://www.w3.org/1999/xlink">dv</italic>
###xml 567 569 559 561 <italic xmlns:xlink="http://www.w3.org/1999/xlink">PP</italic>
###xml 569 571 561 563 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i </italic>
###xml 569 571 561 563 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>i </italic></sub>
###xml 652 655 644 647 <italic xmlns:xlink="http://www.w3.org/1999/xlink">dv </italic>
###xml 662 663 654 655 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i</italic>
###xml 663 666 655 658 <italic xmlns:xlink="http://www.w3.org/1999/xlink">th </italic>
###xml 663 666 655 658 <sup xmlns:xlink="http://www.w3.org/1999/xlink"><italic>th </italic></sup>
###xml 732 734 724 726 <italic xmlns:xlink="http://www.w3.org/1999/xlink">TP</italic>
###xml 734 736 726 728 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i </italic>
###xml 734 736 726 728 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>i </italic></sub>
###xml 881 883 873 875 <italic xmlns:xlink="http://www.w3.org/1999/xlink">TP</italic>
###xml 883 884 875 876 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i</italic>
###xml 883 884 875 876 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>i</italic></sub>
###xml 884 887 876 879 <italic xmlns:xlink="http://www.w3.org/1999/xlink">/PP</italic>
###xml 887 888 879 880 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i</italic>
###xml 887 888 879 880 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>i</italic></sub>
###xml 932 935 924 927 <italic xmlns:xlink="http://www.w3.org/1999/xlink">dv </italic>
###xml 949 951 941 943 <italic xmlns:xlink="http://www.w3.org/1999/xlink">PP</italic>
###xml 951 953 943 945 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i </italic>
###xml 951 953 943 945 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>i </italic></sub>
###xml 964 966 956 958 <italic xmlns:xlink="http://www.w3.org/1999/xlink">TP</italic>
###xml 966 967 958 959 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i</italic>
###xml 966 967 958 959 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>i</italic></sub>
###xml 1092 1094 1084 1086 <italic xmlns:xlink="http://www.w3.org/1999/xlink">PP</italic>
###xml 1094 1096 1086 1088 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i </italic>
###xml 1094 1096 1086 1088 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>i </italic></sub>
###xml 1107 1109 1099 1101 <italic xmlns:xlink="http://www.w3.org/1999/xlink">TP</italic>
###xml 1109 1110 1101 1102 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i</italic>
###xml 1109 1110 1101 1102 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>i</italic></sub>
Using this idea we actually get ten precision estimates, not one. This is because we do cross validation ten times, using ten different random splits of the data. The result is that each gene in the training set receives ten discriminant values, and for each one we get a different precision estimate. We could simply use the average of these ten precision estimates; however, to reduce the variance of the estimate, we use a weighted average. Specifically, let us number the cross validation runs from i = 1, em leader, 10. Then, given a discriminant value, dv, let PPi be the number of genes in the training set whose discriminant values is at least dv in the ith run of cross validation. (These are the predicted positives.) Let TPi be the number of these genes that respond to stress (the true positives). Using only this cross validation run, the estimated precision would be TPi/PPi. One problem with this estimate is that if dv is high, then PPi (and hence TPi) could be 0, so the precision estimate would be undefined, something we observed frequently in practice. More generally, if PPi (and hence TPi) is low, then the precision estimate will have high variance, since it is supported by very little data. To circumvent these problems, we estimate the precision using the formula
###end p 125
###begin p 126

###end p 126
###begin p 127
###xml 88 90 88 90 <italic xmlns:xlink="http://www.w3.org/1999/xlink">TP</italic>
###xml 90 91 90 91 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i</italic>
###xml 90 91 90 91 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>i</italic></sub>
###xml 91 94 91 94 <italic xmlns:xlink="http://www.w3.org/1999/xlink">/PP</italic>
###xml 94 95 94 95 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i</italic>
###xml 94 95 94 95 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>i</italic></sub>
###xml 170 173 170 173 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i.e</italic>
###xml 186 188 186 188 <italic xmlns:xlink="http://www.w3.org/1999/xlink">PP</italic>
###xml 188 190 188 190 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i </italic>
###xml 188 190 188 190 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>i </italic></sub>
###xml 346 349 346 349 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i.e</italic>
###xml 357 358 353 354 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i</italic>
###xml 357 358 353 354 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>i</italic></sub>
###xml 358 360 354 356 <italic xmlns:xlink="http://www.w3.org/1999/xlink">PP</italic>
###xml 360 362 356 358 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i </italic>
###xml 360 362 356 358 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>i </italic></sub>
###xml 416 418 412 414 <italic xmlns:xlink="http://www.w3.org/1999/xlink">PP</italic>
###xml 418 419 414 415 <italic xmlns:xlink="http://www.w3.org/1999/xlink">i</italic>
###xml 418 419 414 415 <sub xmlns:xlink="http://www.w3.org/1999/xlink"><italic>i</italic></sub>
where . The right-hand formula is a weighted average of individual precision estimates, TPi/PPi. It gives more weight to precision estimates that are based on more data, i.e., for which PPi is higher. In addition, by using the left-hand formula, we rarely end up dividing by zero, since the denominator is a sum of (random) non-negative numbers; i.e., SigmaiPPi is much less likely to be zero than is any individual PPi.
###end p 127
###begin title 128
Biological experiments
###end title 128
###begin p 129
###xml 347 349 347 349 <xref xmlns:xlink="http://www.w3.org/1999/xlink" ref-type="bibr" rid="B43">43</xref>
Wild type and homozygous mutant seeds were plated on 0.5X MS media. They were stratified for 3 days and then germinated at 25C for 7 days. The abiotic temperature stresses consisted of 7 days exposure to either 30C, 14C or 4C. Anthocyanin levels were quantified as a measure of plant stress response. Anthocyanin was extracted using methanol-HCl [43]. In order to measure response to salt stress, plants were germinated for 3 days on 0.5X MS media and then transferred to medium containing 50 mM NaCl or to control plates. New root growth was measured 7 days after the transfer.
###end p 129
###begin title 130
Competing interests
###end title 130
###begin p 131
The author(s) declares that there are no competing interests.
###end p 131
###begin title 132
Authors' contributions
###end title 132
###begin p 133
###xml 109 112 109 112 <sub xmlns:xlink="http://www.w3.org/1999/xlink">50 </sub>
HL and AJB did the machine learning, with HL doing the actual programming. HL developed the idea of using ROC50 scores to combine classifiers. RC performed the gene knockout experiments under the supervision of NJP. HL and AJB wrote the bioinformatics sections of the manuscript, with HL providing the first draft. RC wrote the biological sections. All authors read and approved the final manuscript.
###end p 133
###begin title 134
Acknowledgements
###end title 134
###begin p 135
AJB is supported by a grant from NSERC. HL is funded by the Department of Computer Science at the University of Toronto. NJP is supported by grants from NSERC. RC is funded in part by a University of Toronto fellowship. The Botany Beowulf Cluster was funded by a Genome Canada grant administered through the Ontario Genomics Institute.
###end p 135
###begin article-title 136
Knowledge-based analysis of microarray gene expression data by using support vector machines
###end article-title 136
###begin article-title 137
Cluster analysis and display of genome-wide expression patterns
###end article-title 137
###begin article-title 138
###xml 24 29 <span type="species:ncbi:9606">human</span>
A literature network of human genes for high-throughput analysis of gene expression
###end article-title 138
###begin article-title 139
Assessment of prediction accuracy of protein function from protein-protein interaction data
###end article-title 139
###begin article-title 140
Synexpression groups in eukaryotes
###end article-title 140
###begin article-title 141
Genes, themes and microarrays: Using information retrieval for large-scale gene analysis
###end article-title 141
###begin article-title 142
Functional discovery via a compendium of expression profiles
###end article-title 142
###begin article-title 143
Predicting gene function from gene expressions and ontologies
###end article-title 143
###begin article-title 144
Predicting gene ontology biological process from temporal gene expression patterns
###end article-title 144
###begin article-title 145
Gene classification using expression profiles: A feasibility study
###end article-title 145
###begin article-title 146
Gene functional classification by semi-supervised learning from heterogeneous data
###end article-title 146
###begin article-title 147
Exploration of essential gene functions via titratable promoter alleles
###end article-title 147
###begin article-title 148
Gene functional classification from heterogeneous data
###end article-title 148
###begin article-title 149
Systematic learning of gene functional classes from DNA array expression data by using multilayer perceptrons
###end article-title 149
###begin article-title 150
###xml 86 91 <span type="species:ncbi:4932">Yeast</span>
Clustering Labeled Data and Cross-Validation for Classification with Few Positives in Yeast
###end article-title 150
###begin article-title 151
###xml 28 33 <span type="species:ncbi:10090">mouse</span>
The functional landscape of mouse gene expression
###end article-title 151
###begin article-title 152
A green chapter in the book of life
###end article-title 152
###begin article-title 153
Bioinformatic resources, challenges, and opportunities using Arabidopsis as a model organism in a post-genomic era
###end article-title 153
###begin article-title 154
###xml 65 85 <span type="species:ncbi:3702">Arabidopsis thaliana</span>
Cellular function prediction and biological pathway discovery in Arabidopsis thaliana using microarray data
###end article-title 154
###begin article-title 155
###xml 30 50 <span type="species:ncbi:3702">Arabidopsis thaliana</span>
Functional Bioinformatics for Arabidopsis thaliana
###end article-title 155
###begin article-title 156
The Botany Array Resource: e-Northerns, Expression Angling, and promoter analyses
###end article-title 156
###begin article-title 157
The AtGenExpress global stress expression data set: protocols, evaluation and model data analysis of UV-B light, drought and cold stress responses
###end article-title 157
###begin article-title 158
NASCArrays: a repository for microarray data generated by NASC's transcriptomics service
###end article-title 158
###begin article-title 159
Nottingham Arabidopsis Stock Centre (NASC)
###end article-title 159
###begin article-title 160
Gene Ontology: Tool for the unification of biology
###end article-title 160
###begin article-title 161
The Arabidopsis Information Resource (TAIR)
###end article-title 161
###begin article-title 162
Functional annotation of the Arabidopsis genome using controlled vocabularies
###end article-title 162
###begin article-title 163
Semi-supervised Learning on Riemannian Manifolds
###end article-title 163
###begin article-title 164
Splitting the Unsupervised and Supervised Components of Semi-Supervised Learning
###end article-title 164
###begin article-title 165
ROC Graphs: Notes and practical considerations for researchers
###end article-title 165
###begin article-title 166
On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes
###end article-title 166
###begin article-title 167
Use of Receiver Operating Characteristic (ROC) analysis to evaluate sequence matching
###end article-title 167
###begin article-title 168
Controlling the false discovery rate: a practical and powerful approach to multiple testing
###end article-title 168
###begin article-title 169
###xml 25 45 <span type="species:ncbi:3702">Arabidopsis thaliana</span>
A gene expression map of Arabidopsis thaliana development
###end article-title 169
###begin article-title 170
###xml 39 59 <span type="species:ncbi:3702">Arabidopsis thaliana</span>
Genome-wide insertional mutagenesis of Arabidopsis thaliana
###end article-title 170
###begin article-title 171
Athena: a resource for rapid visualization and systematic analysis of Arabidopsis promoter sequences
###end article-title 171
###begin article-title 172
An 'electronic fluorescent protein' browser for exploring Arabidopsis Microarray Data
###end article-title 172
###begin article-title 173
Arabidopsis transcriptome profiling indicates that multiple regulatory pathways are activated during cold acclimation in addition to the CBF cold response pathway
###end article-title 173
###begin article-title 174
DREB takes the stress out of growing up
###end article-title 174
###begin article-title 175
Comparative genomics in salt tolerance between Arabidopsis and Arabidopsis-related halophyte salt cress using Arabidopsis microarray
###end article-title 175
###begin article-title 176
Sucrose-specific induction of the anthocyanin biosynthetic pathway in Arabidopsis
###end article-title 176

