<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.1 20050630//EN" "archivearticle.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Nucleic Acids Res</journal-id><journal-title>Nucleic Acids Research</journal-title><issn pub-type="ppub">0305-1048</issn><issn pub-type="epub">1362-4962</issn><publisher><publisher-name>Oxford University Press</publisher-name><publisher-loc>Oxford, UK</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">gkh485</article-id><article-id pub-id-type="doi">10.1093/nar/gkh485</article-id><article-id pub-id-type="pmid">15215412</article-id><article-categories><subj-group subj-group-type="heading"><subject>Articles</subject></subj-group></article-categories><title-group><article-title>Proteome Analyst: custom predictions with explanations in a web-based tool for high-throughput proteome annotations</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Szafron</surname><given-names>Duane</given-names></name><xref ref-type="author-notes" rid="gkh485fn2">a</xref></contrib><contrib contrib-type="author"><name><surname>Lu</surname><given-names>Paul</given-names></name><xref ref-type="author-notes" rid="gkh485fn1">&#x0002a;</xref></contrib><contrib contrib-type="author"><name><surname>Greiner</surname><given-names>Russell</given-names></name></contrib><contrib contrib-type="author"><name><surname>Wishart</surname><given-names>David S.</given-names></name></contrib><contrib contrib-type="author"><name><surname>Poulin</surname><given-names>Brett</given-names></name></contrib><contrib contrib-type="author"><name><surname>Eisner</surname><given-names>Roman</given-names></name></contrib><contrib contrib-type="author"><name><surname>Lu</surname><given-names>Zhiyong</given-names></name></contrib><contrib contrib-type="author"><name><surname>Anvik</surname><given-names>John</given-names></name></contrib><contrib contrib-type="author"><name><surname>Macdonell</surname><given-names>Cam</given-names></name></contrib><contrib contrib-type="author"><name><surname>Fyshe</surname><given-names>Alona</given-names></name></contrib><contrib contrib-type="author"><name><surname>Meeuwis</surname><given-names>David</given-names></name></contrib></contrib-group><aff id="N0x8d42538.0x8bba198">Department of Computing Science, University of Alberta, Edmonton, AB, T6G 2E8, Canada </aff><author-notes><fn id="gkh485fn1"><label>&#x0002a;</label><p>To whom correspondence should be addressed. Tel: &#x0002b;1 780 492 7760; Fax: &#x0002b;1 780 492 1071; Email <email>paullu@cs.ualberta.ca</email></p></fn><fn id="gkh485fn2"><label>a</label><p>Correspondence may also be addressed to Duane Szafron. Email: <email>duane@cs.uslberta.ca</email></p></fn><fn id="N0x8d42538.0x8bba218"><label>a</label><p>The authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First Authors</p></fn><fn id="N0x8d42538.0x8bba238"><label>a</label><p>The online version of this article has been published under an open access model. Users are entitled to use, reproduce, disseminate, or display the open access version of this article provided that: the original authorship is properly and fully attributed; the Journal and Oxford University Press are attributed as the original place of publication with the correct citation details given; if an article is subsequently reproduced or disseminated not in its entirety but only in part or as a derivative work this must be clearly indicated.</p></fn><fn id="N0x8d42538.0x8bba258"><label>a</label><p>&#x000a9; 2004, the authors</p></fn></author-notes><pub-date pub-type="ppub"><day>1</day><month>7</month><year>2004</year></pub-date><volume>32</volume><issue>Web Server issue</issue><fpage>W365</fpage><lpage>W371</lpage><ext-link ext-link-type="uri" xlink:href="http://www.nar.oupjournals.org/content/vol32/issueWeb Server issue//"/><history><date date-type="received"><day>15</day><month>2</month><year>2004</year></date><date date-type="rev-recd"><day>19</day><month>4</month><year>2004</year></date><date date-type="accepted"><day>5</day><month>5</month><year>2004</year></date></history><copyright-statement>Copyright &#x000a9; 2004 Oxford University Press</copyright-statement><copyright-year>2004</copyright-year><abstract><p>Proteome Analyst (PA) (<ext-link ext-link-type="uri" xlink:href="http://www.cs.ualberta.ca/~bioinfo/PA/">http://www.cs.ualberta.ca/~bioinfo/PA/</ext-link>) is a publicly available, high-throughput, web-based system for predicting various properties of each protein in an entire proteome. Using machine-learned classifiers, PA can predict, for example, the GeneQuiz general function and Gene Ontology (GO) molecular function of a protein. In addition, PA is currently the most accurate and most comprehensive system for predicting subcellular localization, the location within a cell where a protein performs its main function. Two other capabilities of PA are notable. First, PA can create a custom classifier to predict a new property, without requiring any programming, based on labeled training data (i.e. a set of examples, each with the correct classification label) provided by a user. PA has been used to create custom classifiers for potassium-ion channel proteins and other general function ontologies. Second, PA provides a sophisticated explanation feature that shows why one prediction is chosen over another. The PA system produces a Na&#x000ef;ve Bayes classifier, which is amenable to a graphical and interactive approach to explanations for its predictions; transparent predictions increase the user's confidence in, and understanding of, PA.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>There are now more than 1200 complete or partially sequenced genomes deposited in public databases (<ext-link ext-link-type="uri" xlink:href="http://www.ebi.ac.uk/genomes/">http://www.ebi.ac.uk/genomes/</ext-link>) and this number is growing rapidly. Given the size and complexity of these data sets, most researchers are compelled to use automated annotation systems to identify or classify individual genes&#x0002f;proteins in their genomic data. A number of systems have been developed over the past few years that permit automated genome-wide or proteome-wide annotation. These include GeneQuiz (<xref ref-type="bibr" rid="gkh485c1">1</xref>), GeneAtlas (<xref ref-type="bibr" rid="gkh485c2">2</xref>), Ensembl (<xref ref-type="bibr" rid="gkh485c3">3</xref>), PEDANT (<xref ref-type="bibr" rid="gkh485c4">4</xref>), Genotator (<xref ref-type="bibr" rid="gkh485c5">5</xref>), MAGPIE (<xref ref-type="bibr" rid="gkh485c6">6</xref>) and GAIA (<xref ref-type="bibr" rid="gkh485c7">7</xref>).</p><p>The Proteome Analyst (PA) system (<xref ref-type="bibr" rid="gkh485c8">8</xref>&#x02013;<xref ref-type="bibr" rid="gkh485c10">10</xref>) (<ext-link ext-link-type="uri" xlink:href="http://www.cs.ualberta.ca/~bioinfo/PA/">http://www.cs.ualberta.ca/~bioinfo/PA/</ext-link>) focuses on the task of predicting (classifying) various aspects of a protein. Our results show that classification can be used for many annotations, including general function, subcellular localization, specific function and many specialized predictors, such as the potassium-ion channel predictor described later in this paper.</p><p>Although there are a variety of tools for protein annotation, PA has unique capabilities. In addition to being the most accurate and most comprehensive (i.e. broadest range of organisms and organelles, highest number of proteins annotated) predictor of subcellular localization (<xref ref-type="bibr" rid="gkh485c9">9</xref>),</p><list list-type="roman-lower"><list-item><p>PA provides a single, integrated, high-throughput and web-based interface to a number of different tools for proteome annotation;</p></list-item><list-item><p>PA allows the user to create custom predictors in a simple train-by-example way, for any user-specified ontology of labels; and</p></list-item><list-item><p>PA provides clear and transparent explanations for each of its predictions.</p></list-item></list><p>In the context of PA, transparency is the ability to provide formally sound and intuitively simple explanations for predictions. PA bases its predictions on well-understood concepts from probability theory. Its explanations use stacked-bar graphs (Figure <xref ref-type="fig" rid="gkh485f8">8</xref>) and hyperlinks to clearly display the evidence for each prediction.</p></sec><sec><title>USING PROTEOME ANALYST</title><p>Proteome Analyst is web based. The user may choose to either analyze (annotate) a proteome using built-in (previously trained) classifiers or train a new custom classifier, which can afterwards be used to analyze specific properties of proteins. We explain both options.</p><sec><title>Analysis of a proteome</title><p>To analyze a proteome, the user first uploads a FASTA-format file containing the sequences to be analyzed. Two important tools are a classifier-based predictor and the PACardCreator. Currently, a user may select from several built-in general function classifiers that use the GeneQuiz (GQ) ontology and were trained on sequences from individual organisms: <italic>Escherichia coli</italic>, Yeast, <italic>Drosophila</italic> or a multiorganism-trained classifier trained with sequences from all three organisms. A Gene Ontology (GO)-based classifier for molecular function is also available in the latest version of PA. Alternatively, a user may select any custom classification-based predictor that has been trained as described below.</p><p>The PACardCreator generates a PACard for each sequence&#x02014;a summary of all the predicted properties of each protein specified in the input. The top of a typical PACard is shown in Figure <xref ref-type="fig" rid="gkh485f1">1</xref>. A PACard is based on the <italic>E.coli</italic> cards from the CyberCell Database (CCDB) (<ext-link ext-link-type="uri" xlink:href="http://redpoll.pharmacy.ualberta.ca/CCDB/">http://redpoll.pharmacy.ualberta.ca/CCDB/</ext-link>).</p><p>Currently, PA can fill in over 30 different fields: Name, GeneQuiz general function, subcellular location, GeneOntology molecular function, Specific Function, Pfam Domain&#x0002f;Function, EC Number, Specific Reaction, General Reaction, PROSITE, BLAST, Important Sites, Inhibitor, Interacting Partners, Sequence, Secondary Structure, Metabolic Importance, Copy Number, RNA Copy No., Similarity, Number of Amino Acids, Molecular Weight, Transmembrane, Cys&#x0002f;Met Content, Structure Class, Quaternary Structure, Cofactors, Metals Ions, Kcat Value (1&#x0002f;min), Specific Activity (&#x003bc;mol) and Km Value (mM).</p><p>Figure <xref ref-type="fig" rid="gkh485f2">2</xref> shows an example analysis, sorted by general function (GeneQuiz ontological class). The probability of the predicted general function class is shown for each sequence. The output also shows the predicted subcellular localization (and the probability of this prediction), the top homologs found during the BLAST search, a link to the full BLAST output in standard format, links to the full general function classifier output (Figure <xref ref-type="fig" rid="gkh485f3">3</xref>), the subcellular classifier output, the PACard (Figure <xref ref-type="fig" rid="gkh485f1">1</xref>) and explanations for each classifier prediction. The &#x02018;explain&#x02019; facility is discussed later in the paper and is one of the most novel characteristics of PA. We believe explanations are essential for widespread acceptance of computational prediction techniques in bioinformatics.</p><p>Figure <xref ref-type="fig" rid="gkh485f3">3</xref> shows that the predicted ontological class (Energy metabolism) of the ACEA&#x0005f;ECOLI protein (Protein &#x00023;1 from Figure <xref ref-type="fig" rid="gkh485f2">2</xref>) has a probability of 72.1%. It also shows that the next most probable class is Other categories, with a probability of 27.8%.</p></sec><sec><title>Prediction techniques in PA</title><p>PA makes extensive use of machine-learning (ML) classifiers to predict annotations. PA can help the user build novel classifiers, for new annotations, by applying a standard ML algorithm to a set of labeled training items&#x02014;a list of known proteins with their respective class labels (i.e. annotations). The classifier is later used to provide labels (predictions or annotations) to previously unlabeled proteins. In PA, each training item consists of a primary protein sequence and the ontological class it has been assigned by an expert.</p><p>In general, an ML classifier algorithm requires features to be associated with each training item. Note that PA is given only the primary sequence of the protein; the features are automatically computed by the system. Once built, a classifier takes a protein sequence with unknown class and uses the values of these features (i.e. the presence or absence of the associated word or phrase) to predict its class.</p><p>Specifically, PA uses a preprocessing step that maps each sequence to a set of features, as shown in Figure <xref ref-type="fig" rid="gkh485f4">4</xref>. First, the sequence is compared to the SWISS-PROT database using BLAST. Second, the SWISS-PROT entries of (up to) three top homologs (whose <italic>E</italic>-values are &#x0003c;0.001) are parsed to extract a feature set from the SWISS-PROT KEYWORDS field, any Interpro numbers (<xref ref-type="bibr" rid="gkh485c11">11</xref>) contained in the DBSOURCE field, and the SUBCELLULAR LOCATION field. The union of the features for the selected homologs forms the feature set. If no homologs match the <italic>E</italic>-value cutoff or if all features are removed by feature selection (described later) then the sequence has no features, so no prediction is made.</p><p>The feature set is then used as input for both the training and classification phases of PA (discussed below). In essence, PA learns a mapping from feature sets to classes (also known as &#x02018;annotations&#x02019;). The same extraction algorithm is used to determine the prediction or annotation for each protein sequence, whether the classifier is a built-in one or a custom user-trained one.</p></sec><sec><title>Training a custom classifier</title><p>Since PA provides several built-in classifiers, many users will not need to build their own custom classifier. However, for user-specified ontologies or other specialized purposes, the ability to build and explain a custom classifier, without requiring any programming, is a key advantage of PA.</p><p>As shown in Figure <xref ref-type="fig" rid="gkh485f5">5</xref>, classification-based prediction is a two-step process: training&#x0002f;learning and prediction. In the training&#x0002f;learning step, a classifier is built using an ML classification algorithm by analyzing a set of training sequences, each tagged by a known class label. In the prediction step, the generated classifier is used to predict the class label of an unknown query sequence.</p><p>Of course, when building any classifier, it is necessary for the training data to satisfy two criteria. First, they must be broad enough to contain representative examples of each labeled class. Second, the training data must be relatively free from errors. Training data with narrow coverage or labeling errors cannot produce an accurate classifier using any ML technology. However, PA's explanation system can actually be used to find errors in the training data, if necessary (<xref ref-type="bibr" rid="gkh485c10">10</xref>). If the training data contain too many errors, PA will indicate the poor quality of the trained classifier by reporting low accuracy in the automatic validation that is done after training a classifier.</p><p>The production version of PA includes a general function (GeneQuiz ontology) classifier and a series of subcellular localization classifiers (based on organism type). However, a user can also train a Na&#x000ef;ve Bayes (NB) custom classifier. The first step in training a custom classifier is to provide a name for the classifier and a corresponding training file in FASTA format. Each sequence in the file must have a FASTA tag that starts with a known class label. For example, Figure <xref ref-type="fig" rid="gkh485f6">6</xref> shows part of a training file for a custom K-ion channel classifier, where the two training sequences have known class labels KV1 and KV2, respectively.</p><p>After uploading the training file, the user has a choice of two configuration parameters: feature wrapping (<xref ref-type="bibr" rid="gkh485c12">12</xref>) and the value of <italic>k</italic> for the <italic>k</italic>-fold cross-validation. The wrapping (feature selection) process is a standard ML technique (<xref ref-type="bibr" rid="gkh485c13">13</xref>). It removes the less discriminating features from the trained classifier and has the overall effect of improving accuracy by reducing overfitting. The default configuration uses wrapping.</p><p>In <italic>k</italic>-fold cross-validation (<xref ref-type="bibr" rid="gkh485c12">12</xref>), the labeled training instances are &#x02018;randomly&#x02019; divided into <italic>k</italic> groups (G<sub>1</sub>,&#x02026;,G<sub><italic>k</italic></sub>), while keeping the number of training instances with each label approximately the same in each training group. Then, <italic>k</italic> different classifiers are constructed (C<sub>1</sub>,&#x02026;,C<sub><italic>k</italic></sub>), where C<sub><italic>i</italic></sub> uses all of the training instances from all of the groups except G<sub><italic>i</italic></sub>. Next, a confusion matrix is computed for each of the <italic>k</italic> classifiers, C<sub><italic>i</italic></sub>, using the sequences in group G<sub><italic>i</italic></sub> (which were not used in its training) as test data. The confusion matrix records the number and type of classification mistakes made by the newly trained classifier (false positives, false negatives, and so on). The final confusion matrix is then computed by summing the entries in all of the confusion matrices. The PA default value of <italic>k</italic> is 5 (common in ML).</p><p>Once the classifier has been trained, the user may view a classifier information page (Figure <xref ref-type="fig" rid="gkh485f7">7</xref>) that contains three lists that summarize the training. The first list shows the training sequences that PA excluded (none in this example) because the BLAST search did not produce any usable features. The second list contains training sequences that are most probably labeled incorrectly, sorted from the highest to the lowest probability. The third list contains the rest of the training sequences, sorted from the highest to the lowest probability of being labeled correctly. The user can discover why PA inferred that a training sequence was labeled correctly or incorrectly by selecting an Explain hyperlink or by looking at the raw BLAST results.</p></sec></sec><sec><title>CUSTOM CLASSIFIER EXAMPLE</title><p>Voltage-gated potassium channels (VKCs) are intrinsic membrane proteins that respond to changes in the transmembrane electric field by changing shape and selectively allowing potassium ions to pass through the lipid bi-layer (<xref ref-type="bibr" rid="gkh485c14">14</xref>). We obtained 78 protein sequences that were divided into 4 classes (KV1, 23 sequences; KV2, 19 sequences; KV3, 17 sequences; and KV4, 19 sequences) from W. Gallin's laboratory. Many of the VKC sequences have close homologs that lie in classes other than their own class.</p><p>In a process that mirrors how users of PA might create a custom classifier, we iteratively trained a classifier, used PA's Explain (and other capabilities) to find the reasons for any inaccurate predictions on the training set itself [i.e. re-substitution or training set errors (<xref ref-type="bibr" rid="gkh485c12">12</xref>)], fixed the source of the inaccuracies and then trained a new classifier. After only two rounds, we were able to create a final custom classifier that is 100% accurate, over 5-fold cross-validation, with respect to the training data.</p><p>Initially, PA produced an NB classifier that made only three errors during 5-fold cross-validation. However, one error was a labeling error in the training set. After consulting with an expert and fixing the labeling error, we re-trained another classifier. The output in Figure <xref ref-type="fig" rid="gkh485f7">7</xref> shows the two remaining errors. We eliminated these two errors by modifying the feature extraction algorithm's parameters. Originally, we performed three PSI-BLAST iterations before picking the top three homologs to use for feature extraction. We found that when there are many homologs in different ontological classes, better accuracy can be obtained by using only a single PSI-BLAST iteration. Since one iteration of PSI-BLAST is equivalent to BlastP, PA uses BlastP.</p><p>From this case study, we now understand that PSI-BLAST with multiple iterations is likely to perform poorly because multiple iterations tend to promote sequences from the most prevalent organisms in the SWISS-PROT database, at the expense of sequences from minority organisms, even though the minority sequences may have better similarity. The lesson is that when you train a classifier for predicting properties that differentiate based on small differences, a single iteration is better. After making this change, the accuracy increased to 100% on the K-ion training set. PA's explanation mechanism was key in improving the custom classifier.</p></sec><sec><title>ACCURACY AND COVERAGE OF PA</title><p>Identifying the destination or localization of proteins is key to understanding their function and facilitating their purification. A number of existing computational prediction methods are based on sequence analysis. However, these methods are limited in scope, accuracy and most particularly breadth of coverage. Rather than using sequence information alone, we have explored the use of database text annotations from homologs and machine learning to substantially improve the prediction of subcellular location.</p><p>We constructed five custom classifiers for predicting subcellular localization of proteins from animals, plants, fungi, Gram-negative bacteria and Gram-positive bacteria which are 81% accurate for fungi and 92&#x02013;94% accurate for the other four categories (Table <xref ref-type="table" rid="gkh485tb1">1</xref>). These are the most accurate subcellular predictors across the widest set of organisms published to date (<xref ref-type="bibr" rid="gkh485c9">9</xref>). In a series of experiments, we showed that PA makes highly accurate subcellular localization predictions, for many different organisms (e.g. the five custom classifiers listed above), for a variety of different data sets (e.g. SWISS-PROT, LOCkey, PSORT-B) and using a variety of ML techniques. We tested Na&#x000ef;ve Bayes, artificial neural networks (ANNs), support vector machines (SVMs), and nearest-neighbor classifiers.</p><p>PA uses Na&#x000ef;ve Bayes classifiers, since the accuracy is always within 3&#x02013;5% of the best technique for all of the classifiers we have trained (the best technique varies for different training sets, so no particular ML technique is always best) (<xref ref-type="bibr" rid="gkh485c9">9</xref>). Since there is very little quantitative difference in accuracy between NB and the best technique for any training set, we select NB for qualitative reasons. Specifically, as described in the next section, it is possible to transparently explain NB predictions to non-computational scientists. Since the production version of PA uses only NB, it is the only ML technique discussed in this paper.</p><p>The subcellular predictors began their lives as custom predictors in PA. However, after their success, we constructed a simple standalone web tool for predicting just subcellular localization (PA-SUB), available at (<ext-link ext-link-type="uri" xlink:href="http://www.cs.ualberta.ca/~bioinfo/PA/Sub">http://www.cs.ualberta.ca/~bioinfo/PA/Sub</ext-link>). In addition, these predictors have also become built-in classifiers in the production version of PA (<ext-link ext-link-type="uri" xlink:href="http://www.cs.ualberta.ca/~bioinfo/PA">http://www.cs.ualberta.ca/~bioinfo/PA</ext-link>).</p></sec><sec><title>TRANSPARENCY AND EXPLAINABILITY</title><p>While it is necessary for a protein prediction tool to be accurate, it is also important that it can clearly explain its predictions to the user. This is important for two main reasons. First, it helps biologists to develop confidence in the tool. Second, it can help locate and correct errors that occur in the training set, the underlying database (which might give rise to incorrect predictions) or the system parameters, as with the K-ion custom classifier in PA.</p><sec><title>Explaining a prediction&#x0002f;classification</title><p>PA provides an explanation mechanism to help users understand why a classifier makes a particular classification (<xref ref-type="bibr" rid="gkh485c10">10</xref>). It allows a user to examine the query protein itself, as well as the proteins on which the classifier was trained. The user can then examine which particular features added the most evidence to a classification. We will use the protein ACEA&#x0005f;ECOLI as an example. If the user clicks the Explain hyperlink of the ACEA&#x0005f;ECOLI protein in Figure <xref ref-type="fig" rid="gkh485f2">2</xref>, then an Explain page (Figure <xref ref-type="fig" rid="gkh485f8">8</xref>) is displayed.</p><p>Each stacked bar in the graph represents a class in the ontology, and each of its five colored sub-bars corresponds to the presence of one of five selected features in the training sequences. In fact, a sub-bar may represent the absence of a feature. However, for simplicity, in this subsection, we will assume that sub-bars mark the presence of a feature (and this is the case in Figure <xref ref-type="fig" rid="gkh485f8">8</xref>, where the features are tricarboxylic acid cycle, glyoxylate bypass, ipr000918, lyase, and phophorylation).</p><p>Each composed bar on a single line represents the logarithm (base 2) of the combined probability that the protein is in the class represented by the line. For example the lengths of the Energy metabolism and Other categories bars are &#x0223c;43 and 41.6 units respectively. The difference is &#x0223c;1.4 units, which means that the ratio of the probabilities is &#x0223c;2<sup>1.4</sup> &#x02248; 2.6. From Figure <xref ref-type="fig" rid="gkh485f3">3</xref>, the ratio is actually 72.1&#x0002f;27.8 &#x02248; 2.6. The logarithm is used so that the contributions to the probabilities represented by each feature can be added. Additive quantities can be visualized using stacked bar graphs. No simple visual mechanism is available for multiplicative values.</p><p>The (red) tricarboxylic acid cycle subbars occur in the class lines of Other categories, Purines, Energy metabolism and Amino acid biosynthesis and in no other class lines. This indicates that this feature occurred only in the training data of these four classes. The relative lengths of the sub-bars indicate the (logs of the) relative number of times the feature occurred in the different training sets. Similarly, the violet sub-bar represents the occurrence of the feature glyoxylate bypass, which appeared only in training data for the classes Other categories, Purines, Energy metabolism and Regulatory functions.</p><p>The (orange) reduced residual bar represents the combined contributions of all features that are not explicitly shown in the graph. The length of each (orange) reduced residual bar has been reduced by subtracting the length of the shortest one from all the (orange) reduced residual bars. Since bar lengths represent logarithms of probabilities, any fixed amount can always be subtracted from all bars in the graph without affecting the difference between the lengths of any two bars. For example, if the original lengths of two bars were 63 and 61.6 respectively, this would mean that the ratio of probabilities for the two predicted classes was 2<sup>(63&#x02212;61.6)</sup> &#x0003d; 2<sup>1.4</sup> &#x0003d; 2.6. If we subtract 20 from both bars the new lengths are 63 &#x02212; 20 &#x0003d; 43 and 61.6 &#x02212; 20 &#x0003d; 41.6. The ratio of probabilities of the two predicted labels becomes 2<sup>(43&#x02212;41.6)</sup> &#x0003d; 2<sup>1.4</sup> &#x02248; 2.6 (unchanged). Since Transport and binding proteins had the shortest (orange) bar to begin with, its (orange) bar is eliminated (has zero length) after subtracting its length from all orange bars including itself. This subtraction is equivalent to applying a zoom to the graph that focuses on the five most important features.</p><p>The (gray) reduced prior bars account for the different sizes of the training sets. A similar subtraction of the shortest (gray) reduced prior bar has been performed (i.e. the bar for the class Other categories). The explanation facility also allows the user to change which features are explicitly displayed in the graph.</p></sec><sec><title>The importance of transparency</title><p>The PA Explain mechanism is the most important example of prediction transparency in PA. First, the Explain mechanism can be used to understand how a particular protein prediction was made. Second, it can be used to understand the internal structure of a predictor&#x02014;how its training data affect its predictions. Prediction transparency is very important for two reasons. First, it is hard to accept predictions unless you understand how they were made. After using the Explain mechanism, you gain confidence that the predictor is working properly. Second, even the best predictors will make wrong predictions. They should not be trusted blindly.</p><p>There are three important situations in which classification-based predictors fail. First, classifiers are only as good as their training data, and the current databases that are used to obtain training data are not perfect. This is why PA clearly labels &#x02018;suspicious&#x02019; training data as probably mislabeled, after it constructs any new classifier. This is another example of transparency in PA. Of course, the user decides on the training data, and whether to include the suspicious sequences in a classifier; PA's role is only to clearly identify the suspicious data. Given PA's feedback on the training data, a more conservative user can retrain a new classifier, without these suspicious sequences. We have found many suspicious sequences while training classifiers.</p><p>Second, predictors can fail if there is not enough training data to uniquely identify a single prediction class. In PA, this is characterized by a full-classifier graph (e.g. Figure <xref ref-type="fig" rid="gkh485f3">3</xref>) where there are multiple bars with significant probabilities.</p><p>Third, predictors can fail due to an inferior classifier algorithm which cannot adequately use the training data to differentiate between query sequences. A trend in ML in general, and recently in bioinformatics, has been always to select the algorithm with the best accuracy. If we had followed that advice we would be using an ANN or SVM classifier in PA (since they have accuracies that are a few percentage points higher) (<xref ref-type="bibr" rid="gkh485c9">9</xref>). But we are not. The ANN and SVM classifiers are not transparent, which, as argued above, is important. In the production version of PA, we have opted for a classifier with slightly lower accuracy so that we can provide transparent predictions. We believe that this is essential in this domain, where even the best predictors make errors due to bad training data or not enough training data, and these errors must be found.</p></sec></sec><sec><title>SUMMARY</title><p>We have presented Proteome Analyst (PA), a web-based tool for the high-throughput prediction of protein features. In addition to several built-in classifiers and tools for protein annotation, PA supports the construction of custom classification-based predictors. PA's custom classifiers can be for any user-specified ontology and, since classifiers are trained by example, no programming knowledge is required. Notably, using PA's custom classifier features, we have trained and then made available (as a built-in predictor) in PA, the (currently) most accurate and most comprehensive subcellular localization predictor. Furthermore, to increase the user's confidence in the system and to help improve the accuracy of the classifiers, every prediction made by PA can be explained in a transparent way. PA is publicly available at <ext-link ext-link-type="uri" xlink:href="http://www.cs.ualberta.ca/~bioinfo/PA">http://www.cs.ualberta.ca/~bioinfo/PA</ext-link>.</p></sec></body><back><sec sec-type="display-objects"><title>Figures and Tables</title><fig id="gkh485f1" position="float"><label>Figure 1</label><caption><p> The top part of a sample PACard.</p></caption><graphic xlink:href="gkh485f1"/></fig><fig id="gkh485f2" position="float"><label>Figure 2</label><caption><p> Proteins by ontological class (partial screenshot).</p></caption><graphic xlink:href="gkh485f2"/></fig><fig id="gkh485f3" position="float"><label>Figure 3</label><caption><p> Full classifier output for ACEA&#x0005f;ECOLI (partial screenshot).</p></caption><graphic xlink:href="gkh485f3"/></fig><fig id="gkh485f4" position="float"><label>Figure 4</label><caption><p> The feature extraction algorithm for a protein sequence in PA.</p></caption><graphic xlink:href="gkh485f4"/></fig><fig id="gkh485f5" position="float"><label>Figure 5</label><caption><p> The training and prediction phases of classification.</p></caption><graphic xlink:href="gkh485f5"/></fig><fig id="gkh485f6" position="float"><label>Figure 6</label><caption><p> The FASTA-based format of a classifier training file.</p></caption><graphic xlink:href="gkh485f6"/></fig><fig id="gkh485f7" position="float"><label>Figure 7</label><caption><p> Information for a trained classifier (partial screenshot).</p></caption><graphic xlink:href="gkh485f7"/></fig><fig id="gkh485f8" position="float"><label>Figure 8</label><caption><p> Part of the general function prediction (GeneQuiz ontology) Explain page for ACEA&#x0005f;ECOLI.</p></caption><graphic xlink:href="gkh485f8"/></fig><table-wrap id="gkh485tb1"><label>Table 1.</label><caption><title>Accuracies and informal sequence&#x0002f;taxonomic coverage of current subcellular localization predictors</title></caption><table frame="hsides" rules="groups" border="1" width="85%"><colgroup><col/><col/><col/><col/></colgroup><thead valign="bottom"><tr valign="top"><th valign="top">Name</th><th valign="top">Accuracies</th><th valign="top">Coverage</th><th valign="top">Technique</th></tr></thead><tbody valign="top"><tr valign="top"><td valign="top">PSORT-B</td><td valign="top">0.75</td><td valign="top">1443 GN bacterial</td><td valign="top">Combination</td></tr><tr valign="top"><td valign="top">LOCkey</td><td valign="top">0.87</td><td valign="top">1161 assorted</td><td valign="top">Homology</td></tr><tr valign="top"><td valign="top">SubLoc</td><td valign="top">0.91</td><td valign="top">291 prokaryotic</td><td valign="top">AA composition</td></tr><tr valign="top"><td valign="top">&#x000a0;</td><td valign="top">0.79</td><td valign="top">2427 eukaryotic</td><td valign="top">&#x000a0;</td></tr><tr valign="top"><td valign="top">TargetP</td><td valign="top">0.85</td><td valign="top">940 plant</td><td valign="top">Signal prediction</td></tr><tr valign="top"><td valign="top">&#x000a0;</td><td valign="top">0.90</td><td valign="top">2738 non-plant</td><td valign="top">&#x000a0;</td></tr><tr valign="top"><td valign="top">Proteome analyst</td><td valign="top">0.93</td><td valign="top">16&#x02009;284 animal</td><td valign="top">Homology and machine learning</td></tr><tr valign="top"><td valign="top">&#x000a0;</td><td valign="top">0.93</td><td valign="top">3420 plant</td><td valign="top">&#x000a0;</td></tr><tr valign="top"><td valign="top">&#x000a0;</td><td valign="top">0.81</td><td valign="top">2104 fungal</td><td valign="top">&#x000a0;</td></tr><tr valign="top"><td valign="top">&#x000a0;</td><td valign="top">0.92</td><td valign="top">3218 GN bacterial</td><td valign="top">&#x000a0;</td></tr><tr valign="top"><td valign="top">&#x000a0;</td><td valign="top">0.94</td><td valign="top">1571 GP bacterial</td><td valign="top">&#x000a0;</td></tr></tbody></table><table-wrap-foot><fn><p>Gram-negative bacteria and Gram-positive bacteria are denoted GN and GP respectively. This table is reproduced from (<xref ref-type="bibr" rid="gkh485c9">9</xref>).</p></fn></table-wrap-foot></table-wrap></sec><ack><sec><title>ACKNOWLEDGEMENTS</title><p>This research was partially funded by research or equipment grants from the Protein Engineering Network of Centres of Excellence, the Natural Sciences and Engineering Research Council of Canada, Alberta Ingenuity Centre for Machine Learning, Sun Microsystems, SGI, and the Alberta Science and Research Authority.</p></sec></ack><ref-list><title>REFERENCES</title><ref id="gkh485c1"><label>1.</label><citation citation-type="journal"><name><surname>Andrade</surname><given-names>M.A.</given-names></name>, Brown,N.P., Leroy,C., Hoersch,S., de Daruvar,A., Reich,C., Franchini,A., Tamames,J., Valencia,A., Ouzounis,C. and Sander,C. (1999) Automated genome sequence analysis and annotation. <source>Bioinformatics</source>, <volume>15</volume>, <fpage>391</fpage>&#x02013;<lpage>412</lpage>.<pub-id pub-id-type="pmid">10366660</pub-id></citation></ref><ref id="gkh485c2"><label>2.</label><citation citation-type="journal"><name><surname>Kitson</surname><given-names>D.H.</given-names></name>, Badretdinov,A., Zhu,Z.Y., Velikanov,M., Edwards,D.J., Olszewski,K., Szalma,S. and Yan,L. (2002) Functional annotation of proteomic sequences based on consensus of sequence and structural analysis. <source>Brief. Bioinformatics</source>, <volume>3</volume>, <fpage>32</fpage>&#x02013;<lpage>44</lpage>.<pub-id pub-id-type="pmid">12002222</pub-id></citation></ref><ref id="gkh485c3"><label>3.</label><citation citation-type="journal"><name><surname>Hubbard</surname><given-names>T.</given-names></name>, Barker,D., Birney,E., Cameron,G., Chen,Y., Clark,L., Cox,T., Cuff,J., Curwen,V., Down,T. <italic>et al</italic>. (2002) The Ensembl genome database project. <source>Nucleic Acids Res.</source>, <volume>30</volume>, <fpage>38</fpage>&#x02013;<lpage>41</lpage>.<pub-id pub-id-type="pmid">11752248</pub-id></citation></ref><ref id="gkh485c4"><label>4.</label><citation citation-type="journal"><name><surname>Frishman</surname><given-names>D.</given-names></name>, Albermann,K., Hani,J., Heumann,K., Metanomski,A., Zollner,A. and Mewes,H.W. (2001) Functional and structural genomics using PEDANT. <source>Bioinformatics</source>, <volume>17</volume>, <fpage>44</fpage>&#x02013;<lpage>57</lpage>.<pub-id pub-id-type="pmid">11222261</pub-id></citation></ref><ref id="gkh485c5"><label>5.</label><citation citation-type="journal"><name><surname>Harris</surname><given-names>N.L.</given-names></name> (1997) Genotator: a workbench for sequence annotation. <source>Genome Res.</source>, <volume>7</volume>, <fpage>754</fpage>&#x02013;<lpage>762</lpage>.<pub-id pub-id-type="pmid">9253604</pub-id></citation></ref><ref id="gkh485c6"><label>6.</label><citation citation-type="journal"><name><surname>Gaasterland</surname><given-names>T.</given-names></name> and Sensen,C.W. (1996) MAGPIE: automated genome interpretation. <source>Trends Genet.</source>, <volume>12</volume>, <fpage>76</fpage>&#x02013;<lpage>78</lpage>.<pub-id pub-id-type="pmid">8851977</pub-id></citation></ref><ref id="gkh485c7"><label>7.</label><citation citation-type="journal"><name><surname>Overton</surname><given-names>G.C.</given-names></name>, Bailey,C., Crabtree,J., Gibson,M., Fischer,S. and Schug,J. (1998) The GAIA software framework for genome annotation. <source>Pac. Symp. Biocomput.</source>, <fpage>291</fpage>&#x02013;<lpage>302</lpage>.<pub-id pub-id-type="pmid">9697190</pub-id></citation></ref><ref id="gkh485c8"><label>8.</label><citation citation-type="other"><name><surname>Szafron</surname><given-names>D.</given-names></name>, Lu,P., Greiner,R., Wishart,D., Lu,Z., Poulin,B., Eisner,R., Anvik,J. and Macdonell,C. (2003) Proteome Analyst&#x02014;transparent high-throughput protein annotation: function, localization and custom predictors. 12th International Conference on Machine Learning, Workshop on Machine Learning in Bioinformatics, Washington, DC, August 21, 2003, pp. <fpage>2</fpage>&#x02013;<lpage>10</lpage>. <ext-link ext-link-type="uri" xlink:href="http://bioml-workshop.rutgers.edu/">http://bioml-workshop.rutgers.edu/</ext-link></citation></ref><ref id="gkh485c9"><label>9.</label><citation citation-type="journal"><name><surname>Lu</surname><given-names>Z.</given-names></name>, Szafron,D., Greiner,R., Lu,P., Wishart,D.S., Poulin,B., Anvik,J., Macdonell,C. and Eisner,R. (2004) Predicting subcellular localization of proteins using machine-learned classifiers. <source>Bioinformatics</source>, <volume>20</volume>, <fpage>547</fpage>&#x02013;<lpage>556</lpage>.<pub-id pub-id-type="pmid">14990451</pub-id></citation></ref><ref id="gkh485c10"><label>10.</label><citation citation-type="book"><name><surname>Szafron</surname><given-names>D.</given-names></name>, Greiner,R., Lu,P., Wishart,D., Macdonell,C., Anvik,J., Poulin,B., Lu,Z. and Eisner,R. (2003) <source>Explaining Naive Bayes Classifications</source>, Technical Report 03-09. <publisher-name>Department of Computer Science</publisher-name>, University of Alberta.</citation></ref><ref id="gkh485c11"><label>11.</label><citation citation-type="journal"><name><surname>Apweiler</surname><given-names>R.</given-names></name>, Attwood,T.K., Bairoch,A., Bateman,A., Birney,E., Biswas,M., Bucher,P., Cerutti,L., Corpet,F., Croning,M.D.R. <italic>et al</italic>. (2001) The InterPro database, an integrated documentation resource for protein families, domains and functional sites. <source>Nucleic Acids Res.</source>, <volume>29</volume>, <fpage>37</fpage>&#x02013;<lpage>40</lpage>.<pub-id pub-id-type="pmid">11125043</pub-id></citation></ref><ref id="gkh485c12"><label>12.</label><citation citation-type="book"><name><surname>Mitchell</surname><given-names>T.M.</given-names></name> (1997) <source>Machine Learning</source>. <publisher-name>McGraw-Hill</publisher-name>, NY.</citation></ref><ref id="gkh485c13"><label>13.</label><citation citation-type="journal"><name><surname>Kohavi</surname><given-names>R.</given-names></name> and John,G.H. (1997) Wrappers for feature subset selection. <source>Artif. Intell.</source>, <volume>97</volume>, <fpage>273</fpage>&#x02013;<lpage>324</lpage>.</citation></ref><ref id="gkh485c14"><label>14.</label><citation citation-type="book"><name><surname>Gallin</surname><given-names>W.J.</given-names></name> and Spencer,A.N. (2001) Evolution of potassium channels. In Archer,S.L. and Spencer,A.N. (eds), <source>Potassium Channels in Cardiovascular Biology</source>. <publisher-name>Kluwer</publisher-name>, Dordrecht, pp. <fpage>3</fpage>&#x02013;<lpage>16</lpage>.</citation></ref></ref-list></back></article> 