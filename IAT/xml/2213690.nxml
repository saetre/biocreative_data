<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd"><article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id><journal-title>BMC Bioinformatics</journal-title><issn pub-type="epub">1471-2105</issn><publisher><publisher-name>BioMed Central</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">1471-2105-8-358</article-id><article-id pub-id-type="pmid">17888165</article-id><article-id pub-id-type="doi">10.1186/1471-2105-8-358</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Combining classifiers to predict gene function in Arabidopsis thaliana using large-scale gene expression measurements</article-title></title-group><contrib-group><contrib id="A1" equal-contrib="yes" contrib-type="author"><name><surname>Lan</surname><given-names>Hui</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>lanhui@cs.toronto.edu</email></contrib><contrib id="A2" equal-contrib="yes" contrib-type="author"><name><surname>Carson</surname><given-names>Rachel</given-names></name><xref ref-type="aff" rid="I2">2</xref><email>r.carson@utoronto.ca</email></contrib><contrib id="A3" contrib-type="author"><name><surname>Provart</surname><given-names>Nicholas J</given-names></name><xref ref-type="aff" rid="I2">2</xref><email>nicholas.provart@utoronto.ca</email></contrib><contrib id="A4" corresp="yes" contrib-type="author"><name><surname>Bonner</surname><given-names>Anthony J</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>bonner@cs.toronto.edu</email></contrib></contrib-group><aff id="I1"><label>1</label>Department of Computer Science, University of Toronto, 40 St George St, Toronto, ON M5S 2E4, Canada</aff><aff id="I2"><label>2</label>Department of Cell and Systems Biology/Centre for the Analysis of Genome Evolution and Function, University of Toronto, 25 Wilcocks St, Toronto, ON M5S 3B2, Canada</aff><pub-date pub-type="collection"><year>2007</year></pub-date><pub-date pub-type="epub"><day>21</day><month>9</month><year>2007</year></pub-date><volume>8</volume><fpage>358</fpage><lpage>358</lpage><ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2105/8/358"/><history><date date-type="received"><day>6</day><month>10</month><year>2006</year></date><date date-type="accepted"><day>21</day><month>9</month><year>2007</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2007 Lan et al; licensee BioMed Central Ltd.</copyright-statement><copyright-year>2007</copyright-year><copyright-holder>Lan et al; licensee BioMed Central Ltd.</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0"/>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</p><!--<rdf xmlns="http://web.resource.org/cc/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:dcterms="http://purl.org/dc/terms"><Work xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:dcterms="http://purl.org/dc/terms/" rdf:about=""><license rdf:resource="http://creativecommons.org/licenses/by/2.0"/><dc:type rdf:resource="http://purl.org/dc/dcmitype/Text"/><dc:author> Lan Hui lanhui@cs.toronto.edu </dc:author><dc:title> Combining classifiers to predict gene function in Arabidopsis thaliana using large-scale gene expression measurements </dc:title><dc:date>2007</dc:date><dcterms:bibliographicCitation>BMC Bioinformatics 8(1): 358-. (2007)</dcterms:bibliographicCitation><dc:identifier type="sici">1471-2105(2007)8:1&#x0003c;358&#x0003e;</dc:identifier><dcterms:isPartOf>urn:ISSN:1471-2105</dcterms:isPartOf><License rdf:about="http://creativecommons.org/licenses/by/2.0"><permits rdf:resource="http://web.resource.org/cc/Reproduction" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/Distribution" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Notice" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Attribution" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/DerivativeWorks" xmlns=""/></License></Work></rdf>--></license></permissions><abstract><sec><title>Background</title><p><italic>Arabidopsis thaliana </italic>is the model species of current plant genomic research with a genome size of 125 Mb and approximately 28,000 genes. The function of half of these genes is currently unknown. The purpose of this study is to infer gene function in Arabidopsis using machine-learning algorithms applied to large-scale gene expression data sets, with the goal of identifying genes that are potentially involved in plant response to abiotic stress.</p></sec><sec><title>Results</title><p>Using in house and publicly available data, we assembled a large set of gene expression measurements for <italic>A. thaliana</italic>. Using those genes of known function, we first evaluated and compared the ability of basic machine-learning algorithms to predict which genes respond to stress. Predictive accuracy was measured using ROC<sub>50 </sub>and precision curves derived through cross validation. To improve accuracy, we developed a method for combining these classifiers using a weighted-voting scheme. The combined classifier was then trained on genes of known function and applied to genes of unknown function, identifying genes that potentially respond to stress. Visual evidence corroborating the predictions was obtained using electronic Northern analysis. Three of the predicted genes were chosen for biological validation. Gene knockout experiments confirmed that all three are involved in a variety of stress responses. The biological analysis of one of these genes (At1g16850) is presented here, where it is shown to be necessary for the normal response to temperature and NaCl.</p></sec><sec><title>Conclusion</title><p>Supervised learning methods applied to large-scale gene expression measurements can be used to predict gene function. However, the ability of basic learning methods to predict stress response varies widely and depends heavily on how much dimensionality reduction is used. Our method of combining classifiers can improve the accuracy of such predictions &#x02013; in this case, predictions of genes involved in stress response in plants &#x02013; and it effectively chooses the appropriate amount of dimensionality reduction automatically. The method provides a useful means of identifying genes in <italic>A. thaliana </italic>that potentially respond to stress, and we expect it would be useful in other organisms and for other gene functions.</p></sec></abstract></article-meta></front><body><sec><title>Background</title><p>Assigning functions to unannotated genes, identified by genome sequencing and other methods, is the goal of functional genomics. Many approaches have been proposed for large-scale prediction of gene function [<xref ref-type="bibr" rid="B1">1</xref>-<xref ref-type="bibr" rid="B6">6</xref>]. These approaches are largely based on physical association, genetic interaction, sequence relationships and patterns of gene expression. Predicting gene functions based on large-scale gene expression measurements is an attractive strategy since many pathways display coordinated transcriptional regulation [<xref ref-type="bibr" rid="B2">2</xref>,<xref ref-type="bibr" rid="B7">7</xref>]. Although previous studies show that supervised learning methods can be used to predict gene function based on gene expression in microorganisms such as the yeast <italic>Saccharomyces cerevisiae </italic>and in mammals such as mice [<xref ref-type="bibr" rid="B1">1</xref>,<xref ref-type="bibr" rid="B8">8</xref>-<xref ref-type="bibr" rid="B16">16</xref>], it remains unknown to what extent this is true in plants.</p><p>With the <italic>A. thaliana </italic>genome completely sequenced [<xref ref-type="bibr" rid="B17">17</xref>], functional annotation of the genes remains a key challenge for biologists. Currently, approximately 50% of the 28,000 genes have not been assigned any function [<xref ref-type="bibr" rid="B18">18</xref>]. Thus, the extent to which supervised learning methods can be used to infer gene function in <italic>A. thaliana </italic>is a timely and important question. Little work has been done in this area, two exceptions being [<xref ref-type="bibr" rid="B19">19</xref>,<xref ref-type="bibr" rid="B20">20</xref>].</p><p>In [<xref ref-type="bibr" rid="B19">19</xref>], a method is developed to infer gene function from microarray data and predicted protein-protein interactions. The method is similar to Nearest Neighbor algorithms [<xref ref-type="bibr" rid="B21">21</xref>] in that the predicted function(s) of a gene are based on the function(s) of nearby genes. Here, the "nearness" of one gene to another is based on a normalized Pearson correlation of their expression profiles and on putative interactions of their protein products. In addition, the method is extended to the discovery of biological pathways, and is applied to predicting the signaling pathway of phosphatidic acid as a second messenger in <italic>A. thaliana</italic>.</p><p>In [<xref ref-type="bibr" rid="B20">20</xref>], a decision tree algorithm is applied to the problem of predicting the function of protein sequences in <italic>A. thaliana</italic>. Six sources of data were used: sequence, expression, SCOP, secondary structure, InterPro and sequence similarity. One conclusion of the study is that the decision tree algorithm was unable to extract much information from the expression data. The authors suggest that this is because the expression data came from unrelated and highly-specific experiments with just a few readings per gene each. They also suggest that because many more expression data sets are now available for <italic>A. thaliana</italic>, results may improve when using this type of data in the future.</p><p>The present study aims to identify unannotated genes in <italic>A. thaliana </italic>that are potentially involved in plant response to stress. In the context of plants, a stress (biotic or abiotic) causes a decrease in plant growth or yield. We investigated the prediction of gene function in <italic>A. thaliana </italic>based solely on gene expression data using a variety of basic supervised learning methods, namely Logistic Regression (LR), Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), Naive Bayes (NB) and K-Nearest Neighbors (KNN). We also investigated the effect on the learning methods of preprocessing the expression data using Principal Component Analysis (PCA). Finally, we improved the performance of the basic learning methods by combining them using a weighted voting (WV) scheme. This work has enabled our collaborators, biologists in the Department of Cell and Systems Biology at the University of Toronto, to carry out directed biological experiments for determining gene function. In addition to these biological results, the paper illustrates how various machine-learning methods have had to be adapted to fit this bioinformatics application.</p></sec><sec><title>Results and discussion</title><sec><title>Microarray data and the Gene Ontology</title><p>In this study, we used two microarray data sets: one from the Botany Array Resource at the University of Toronto [<xref ref-type="bibr" rid="B22">22</xref>], and the other from the AtGenExpress Consortium [<xref ref-type="bibr" rid="B23">23</xref>], archived at NASCArrays [<xref ref-type="bibr" rid="B24">24</xref>,<xref ref-type="bibr" rid="B25">25</xref>]. These data sets include over 1000 expression-level experiments for <italic>Arabidopsis</italic>, and using all of them would give a data set with dimensionality over 1000. Since the performance of statistical and machine-learning methods tends to decrease with dimensionality, we chose only those experiments that are specifically stress-related. Even so, the covariance matrix of the resulting data set is singular, which is a problem for many of the machine-learning methods. The singularity is probably due to dependencies between the expression levels under control conditions, since removing the controls from the data sets solved the problem. To compensate, we tried applying the learning algorithms to expression-level ratios (<italic>i.e</italic>., ratios of experimental to control conditions). However, we found that the results were better when ratios were not used (data not shown). This is probably because the classifiers look for genes that respond similarly to the known stress-associated genes, so it is not so important to include the controls. In addition, since many of the features are time-courses, there is still a "time zero" control included for the values, providing a baseline measurement. The results reported in this article are therefore based on absolute expression levels without controls.</p><p>From the Toronto data set, we selected 54 features corresponding to experiments conducted primarily to study plant environmental and stress physiology, plant physiology, plant-microbe and plant-insect interactions. From the AtGenExpress data set, we selected 236 features, including various abiotic stresses (e.g., osmotic stress, heat stress, cold stress, salt stress, drought stress, UV-B stress, wounding stress, water-deprivation stress and oxidative stress). We combined the selected features into a single data set. The resulting data set consists of gene expression levels for 22,746 genes under 54 + 236 = 290 different experimental conditions.</p><p>We used terms from the Gene Ontology for Biological Processes (GOBP) to represent gene function. For example, the GOBP term <italic>GO:0006950 [response to stress] </italic>refers to genes that respond to stress. In general, the Gene Ontology (GO) provides a dynamic controlled vocabulary for describing genes and gene products in any organism [<xref ref-type="bibr" rid="B26">26</xref>]. "Biological Process" is one of three broad GO categories (the other two being "Molecular Function" and "Cellular Component"). GOBP terms are organized into a directed acyclic graph (DAG) to reflect the hierarchical relationships between the terms. Parent GOBP terms are subdivided into increasingly specific child GOBP terms.</p><p>Since our study focussed on stress, we were concerned with gene functions at or below the term <italic>GO:0006950 [response to stress] </italic>in the GOBP hierarchy. This GOBP term has 19 child terms, such as <italic>GO:0009409 [response to cold]</italic>, <italic>GO:0009408 [response to heat]</italic>, and <italic>GO:0009414 [response to water deprivation]</italic>. Since gene function becomes more and more specific as we move down the GOBP hierarchy, fewer and fewer genes have any given annotation. The result is that for specific types of stress, our data set contains many negatives and few positives. In the best case, for the term <italic>GO:0009613 [response to pest, pathogen or parasite]</italic>, over 97% of the training data consists of negatives. The typical case is even worse. In fact, looking at all 19 types of stress, 5 types have no positives at all, and of the remaining 14 types, the median number of negatives is 99.2% of the training data. This highly unbalanced data made accurate prediction of gene function difficult. For this reason, we narrowed our study to the top stress term, <italic>GO:0006950 [response to stress]</italic>. To get positive training samples for this term, we propagated all genes in its offspring upward to it in the hierarchy. After up-propagation, the top stress term has 1,031 genes, or almost 9% of the total genes in the training data. The training data therefore contains 9% positives and 91% negatives.</p><p>Using GOBP terms to annotate all genes in <italic>A. thaliana </italic>is an ongoing project started in 2002 by TAIR [<xref ref-type="bibr" rid="B27">27</xref>,<xref ref-type="bibr" rid="B28">28</xref>]. The gene annotations (updated weekly) can be downloaded from TAIR [<xref ref-type="bibr" rid="B27">27</xref>]. The predictions reported in this paper are based on the version for March 10, 2007. Using these annotations, we categorized the genes into <italic>annotated </italic>genes and <italic>unannotated </italic>genes. The annotated genes are those which have at least one GOBP annotation; the unannotated genes are those which have no GOBP annotations. In addition, a gene was treated as unannotated if its only annotation is the top GOBP category, <italic>GO:0008150 [biological process]</italic>, since the function of such a gene is unknown. The result was 11,553 annotated genes and 11,193 unannotated genes in our data set.</p><p>The annotated genes formed the training data, in which a gene was called positive if it is annotated as a stress gene, and negative otherwise. The unannotated genes formed the prediction data. It should be noted that this approach probably introduces some false negatives into the training data, because genes not known to have a particular function are considered to be negative, even though future experiments could reveal them to have that function. That is to say, "unknown" is treated as "negative". However, the number of such false negatives should be small, since only a small number of genes participate in any given biological process. That is, most negatives are true negatives.</p></sec><sec><title>Predicting gene function using basic learning methods</title><p>Using a variety of basic learning methods, we trained a number of classifiers to distinguish between genes that do and do not respond to stress, based on their patterns of gene expression in the training data. We then applied each classifier to the prediction data to estimate the function of the unannotated genes. In addition, we used cross validation to evaluate the performance of each classifier and to estimate the precision of each prediction.</p><p>We used five supervised learning methods: Logistic Regression (LR), Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), Naive Bayes (NB) and K-Nearest Neighbors (KNN) [<xref ref-type="bibr" rid="B21">21</xref>] (see Methods). These methods were chosen because they are representative of the most basic supervised learning methods, the goal being to explore simple methods first. These methods are widely understood, take little computation time, and the results provide a benchmark against which more sophisticated methods can be compared. Moreover, as we show below, the results provided by these methods are good enough to enable biologists to conduct targeted laboratory experiments.</p><p>Each of the five methods is discriminative. That is, the classifiers learned by the methods assign a real number (called a discriminant value) to each gene, reflecting the classifier's certainty that the gene responds to stress. Formally, a discriminative classifier is a function, <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M1" name="1471-2105-8-358-i1" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacuWGMbGzgaqcaaaa@2E11@</mml:annotation></mml:semantics></mml:math></inline-formula>, from genes to discriminant values. In our case, each gene is represented as a 290-dimensional vector, <bold>x</bold>, whose components are the expression levels of the gene under the 290 experimental conditions. Thus, if <bold>x </bold>is a vector representing a gene, then <italic>dv </italic>= <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M2" name="1471-2105-8-358-i1" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacuWGMbGzgaqcaaaa@2E11@</mml:annotation></mml:semantics></mml:math></inline-formula>(<bold>x</bold>) is the discriminant value assigned to the gene by the classifier. Finally, a decision threshold, <italic>&#x003c4;</italic>, is chosen, and the gene is predicted to respond to stress if and only if <italic>dv </italic>&#x0003e; <italic>&#x003c4;</italic>.</p><sec><title>Unsupervised, semi-supervised and transductive learning</title><p>In addition to these supervised learning methods, we preprocessed the gene expression data using Principal Components Analysis (PCA), a form of unsupervised learning, to reduce the dimensionality of the data (see Methods). For this purpose, we combined the expression-level measurements for all genes (both annotated and unannotated) into one large data set, and applied PCA to the entire set. We are therefore doing a form of semi-supervised learning [<xref ref-type="bibr" rid="B29">29</xref>,<xref ref-type="bibr" rid="B30">30</xref>], in which unsupervised learning uses the entire data set (ignoring annotations), and then supervised learning uses the annotated data. This increases the effectiveness of learning by increasing the amount of training data used in the unsupervised phase [<xref ref-type="bibr" rid="B29">29</xref>,<xref ref-type="bibr" rid="B30">30</xref>]. In our case, the unannotated data is also the prediction data, which means that information about the prediction data is used during (unsupervised) training. This is possible because we know all the prediction data in advance. That is, we know the expression levels for all the genes in <italic>Arabidopsis </italic>whether they are annotated or not. We are therefore doing a form of transductive learning [<xref ref-type="bibr" rid="B29">29</xref>,<xref ref-type="bibr" rid="B31">31</xref>], in which the entire prediction set is known during training and is exploited to predict its annotations. This has the added computational advantage of simplifying the way PCA is done during cross validation (see Methods).</p></sec></sec><sec><title>Estimating classifier performance</title><p>To evaluate the performance of discriminative classifiers, it is common to use receiver operating characteristic (ROC) curves [<xref ref-type="bibr" rid="B32">32</xref>]. A ROC curve plots the true positive rate (TP) of a classifier against the false positive rate (FP) for various decision thresholds. It therefore shows the quality of a classifier not at one threshold, but at many, and provides more information than a simple miss-classification rate (as in [<xref ref-type="bibr" rid="B33">33</xref>] for example). In practice, however, biologists are not usually interested in having more than a few dozen false positives, especially in unbalanced data such as ours, in which the number of false positives can rapidly overwhelm the number of true positives. We therefore use so-called ROC<sub>50 </sub>curves [<xref ref-type="bibr" rid="B34">34</xref>], a variant of ROC curves in which the horizonal axis only goes up to 50 false positives. The area under a ROC<sub>50 </sub>curve is the ROC<sub>50 </sub>score [<xref ref-type="bibr" rid="B34">34</xref>], and is a measure of the overall usefulness of a classifier.</p><p>To estimate ROC<sub>50 </sub>curves for our classifiers, we used 20-fold cross-validation (see Methods). Because cross-validation relies on a random split of the training data into folds (20 folds in our case), there is a certain randomness to the estimated ROC<sub>50 </sub>curve. To provide more accurate results, we performed cross-validation ten times, each time with a different (randomly selected) 20-fold split of the data (see Methods). Each 20-fold split results in a slightly different ROC<sub>50 </sub>curve. In some cases, we plot all ten of these curves, to give an idea of the uncertainty in classifier performance (Figure <xref ref-type="fig" rid="F1">1</xref>). In cases where this would result in overly cluttered graphs, we simply present the average of the ten ROC<sub>50 </sub>curves (Figures <xref ref-type="fig" rid="F2">2</xref> to <xref ref-type="fig" rid="F7">7</xref>, each of which show several average ROC<sub>50 </sub>curves).</p><fig position="float" id="F1"><label>Figure 1</label><caption><p><bold>ROC<sub>50 </sub>curves</bold>. Estimated ROC<sub>50 </sub>curves of the combined classifier (WV), showing ten different estimates (dashed curves) and their average (solid curve).</p></caption><graphic xlink:href="1471-2105-8-358-1"/></fig><fig position="float" id="F2"><label>Figure 2</label><caption><p><bold>Logistic Regression (LR)</bold>. Seven ROC<sub>50 </sub>curves for Logistic Regression with varying amounts of dimensionality reduction using PCA. In the legend, p is the PCA-reduced dimension, and s is the ROC<sub>50 </sub>score.</p></caption><graphic xlink:href="1471-2105-8-358-2"/></fig><fig position="float" id="F3"><label>Figure 3</label><caption><p><bold>Linear Discriminant Analysis (LDA)</bold>. Seven ROC<sub>50 </sub>curves for Linear Discriminant Analysis with varying amounts of dimensionality reduction using PCA. In the legend, p is the PCA-reduced dimension, and s is the ROC<sub>50 </sub>score.</p></caption><graphic xlink:href="1471-2105-8-358-3"/></fig><fig position="float" id="F4"><label>Figure 4</label><caption><p><bold>Quadratic Discriminant Analysis (QDA)</bold>. Seven ROC<sub>50 </sub>curves for Quadratic Discriminant Analysis with varying amounts of dimensionality reduction using PCA. In the legend, p is the PCA-reduced dimension, and s is the ROC<sub>50 </sub>score.</p></caption><graphic xlink:href="1471-2105-8-358-4"/></fig><fig position="float" id="F5"><label>Figure 5</label><caption><p><bold>Naive Bayes (NB)</bold>. Seven ROC<sub>50 </sub>curves for Naive Bayes with varying amounts of dimensionality reduction using PCA. In the legend, p is the PCA-reduced dimension, and s is the ROC<sub>50 </sub>score.</p></caption><graphic xlink:href="1471-2105-8-358-5"/></fig><fig position="float" id="F6"><label>Figure 6</label><caption><p><bold>K-Nearest Neighbours (KNN)</bold>. Five ROC<sub>50 </sub>curves for K-Nearest Neighbours for various values of K. The legend gives the ROC<sub>50 </sub>score, s, for each value of K.</p></caption><graphic xlink:href="1471-2105-8-358-6"/></fig><fig position="float" id="F7"><label>Figure 7</label><caption><p><bold>Comparison of methods</bold>. The ROC<sub>50 </sub>curve (purple) for the combined classifier using weighted voting (WV), and the best ROC<sub>50 </sub>curves from each of Figures 2 to 6. In the legend, p is the PCA-reduced dimension of the data, and s is the ROC<sub>50 </sub>score.</p></caption><graphic xlink:href="1471-2105-8-358-7"/></fig><p>We generated ROC<sub>50 </sub>curves for each supervised learning method combined with various amounts of dimensionality reduction. Using PCA, we reduced the original 290 dimensions to 5, 10, 15, 20, 40 and 100 dimensions, respectively. In this way, the original data set was transformed into six separate data sets of various dimensions. Each basic learning method (except KNN) was applied to the original data set and to each of the six reduced data sets. Thus, for each basic learning method (except KNN), we trained and tested seven different classifiers. In the case of KNN, we used only the original, unreduced data, but with five different values of K. Altogether, we trained and tested a total of 4 &#x000d7; 7 + 5 = 33 different classifiers. Figures <xref ref-type="fig" rid="F2">2</xref> to <xref ref-type="fig" rid="F6">6</xref> show the estimated performance of these basic classifiers. Each figure shows a number of ROC<sub>50 </sub>curves, each derived using cross-validation averaged over a number of random splits of the data, as described above. Unlike traditional ROC curves, the axes of these curves give the number of true and false positives, instead of the proportion. The red dash-dot line near the bottom of each figure shows the expected performance of a random classifier (<italic>i.e</italic>., a classifier that ignores the expression data and guesses whether or not a gene responds to stress by essentially flipping a coin). The ROC<sub>50 </sub>scores for the curves are shown in the legend of each figure.</p><p>As the figures show, in some cases the classifiers perform not much better than random, but in most cases they perform significantly better. The figures also show that the performance of each classification method depends heavily of the amount of dimensionality reduction used. Notice in particular that in some cases, the classifier trained on the reduced data has a much higher ROC<sub>50 </sub>score than the classifier trained on the original, unreduced data. This is especially true for NB and QDA. For instance, the classifiers trained on the original data have low ROC<sub>50 </sub>scores of 182.3 for NB and 115.2 for QDA. This is comparable to the random classifier, whose ROC<sub>50 </sub>score is 122.5. However, reducing the dimensionality of the data to 15 increases their ROC<sub>50 </sub>scores to 1373.1 and 1651.0, respectively. This shows the importance of dimensionality reduction. In contrast, KNN performs well for all the values of K that we used.</p><p>Figure <xref ref-type="fig" rid="F7">7</xref> compares the basic classification methods by plotting the best performance of each. That is, for each of the basic classification methods, the ROC<sub>50 </sub>curve with the highest ROC<sub>50 </sub>score is reproduced in Figure <xref ref-type="fig" rid="F7">7</xref>. In addition, the figure shows the performance of a classification method that uses a weighted voting scheme (WV) to combine the 33 basic classifiers into a single, composite classifier. Notice that this composite classifier performs best of all. The next section describes how this composite classifier is constructed.</p></sec><sec><title>Improving prediction accuracy by combining classifiers</title><p>Combining different classifiers in prediction can be thought of as combining different opinions in decision making. The advantage is that a group opinion is better than a single opinion if the single opinions are correctly weighted and combined. In machine-learning systems, classifiers are often combined by weighted voting, in which the discriminant value of the combined classifier is a linear combination of the discriminant values of the individual classifiers. Formally, given a set of basic classifiers, <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M3" name="1471-2105-8-358-i2" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>M</mml:mi></mml:msub></mml:mrow><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacuWGMbGzgaqcamaaBaaaleaacqaIXaqmaeqaaOGaeiilaWIaeS47IWKaeiilaWIafmOzayMbaKaadaWgaaWcbaGaemyta0eabeaaaaa@3599@</mml:annotation></mml:semantics></mml:math></inline-formula>, and a set of weights, <italic>w</italic><sub>1</sub>, &#x02026;, <italic>w</italic><sub><italic>M</italic></sub>, the combined classifier, <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M4" name="1471-2105-8-358-i1" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacuWGMbGzgaqcaaaa@2E11@</mml:annotation></mml:semantics></mml:math></inline-formula>, is defined by the equation <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M5" name="1471-2105-8-358-i3" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mi>m</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacuWGMbGzgaqcaiabcIcaOGqabiab=Hha4jabcMcaPiabg2da9maaqababaGaem4DaC3aaSbaaSqaaiabd2gaTbqabaGccuWGMbGzgaqcamaaBaaaleaacqWGTbqBaeqaaOGaeiikaGIae8hEaGNaeiykaKcaleaacqWGTbqBaeqaniabggHiLdaaaa@3EC3@</mml:annotation></mml:semantics></mml:math></inline-formula>. In our case, <italic>M </italic>= 33, as described above.</p><p>By judiciously choosing the weights, <italic>w</italic><sub>1</sub>, &#x02026;, <italic>w</italic><sub><italic>M</italic></sub>, the performance of the combined classifier can be maximized. Various methods are available for doing this, such as model averaging and stacking [<xref ref-type="bibr" rid="B21">21</xref>]. Using these methods on our data sets, we found that the ROC curve of the combined classifier was usually better than the ROC curves of the basic classifiers, as expected. Unfortunately, we also found that the ROC<sub>50 </sub>curve of the combined classifier was usually worse (data not shown). We hypothesized that this is because our data sets are highly unbalanced. Intuitively, model averaging and stacking try to choose weights so as to correctly classify as much data as possible. In our case, this means trying to correctly classify the vast number of negative samples in our data sets, even if this means misclassifying the small number of positives. In other words, these methods try to minimize the total number of false positives, even though we only care about the first fifty.</p><p>To choose appropriate weights for our combined classifier, we used the heuristic that classifiers that perform well should be given more weight than classifiers that perform poorly. In our case, since we want to maximize the ROC<sub>50 </sub>score of the combined classifier, we want to give high weight to classifiers with high ROC<sub>50 </sub>scores. There are many ways to do this, but we found that it was sufficient to estimate and normalize the ROC<sub>50 </sub>score of each basic classifier, and use this as its weight. That is, we used <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M6" name="1471-2105-8-358-i4" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mi>m</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWG3bWDdaWgaaWcbaGaemyBa0gabeaakiabg2da9iqbdohaZzaajaWaaSbaaSqaaiabd2gaTbqabaGccqGGVaWldaaeqaqaaiqbdohaZzaajaWaaSbaaSqaaiabd2gaTbqabaaabaGaemyBa0gabeqdcqGHris5aaaa@3B09@</mml:annotation></mml:semantics></mml:math></inline-formula>, where <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M7" name="1471-2105-8-358-i5" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacuWGZbWCgaqcamaaBaaaleaacqWGTbqBaeqaaaaa@2FBA@</mml:annotation></mml:semantics></mml:math></inline-formula> is an estimate of the ROC<sub>50 </sub>score of classifier <italic>f</italic><sub><italic>m</italic></sub>. Note that with these weights, if each <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M8" name="1471-2105-8-358-i6" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacuWGMbGzgaqcamaaBaaaleaacqWGTbqBaeqaaaaa@2FA0@</mml:annotation></mml:semantics></mml:math></inline-formula>(<bold>x</bold>) is a number between 0 and 1 (as with our classifiers), then so is <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M9" name="1471-2105-8-358-i1" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacuWGMbGzgaqcaaaa@2E11@</mml:annotation></mml:semantics></mml:math></inline-formula>(<bold>x</bold>). Also, this method automatically gives low weight to classifiers that use an inappropriate amount of dimensionality reduction, since such classifiers have low ROC<sub>50 </sub>scores. In this way, the combined classifier incorporates not only the best combination of supervised learning methods, but also the best amounts of dimensionality reduction for each method.</p><p>To train and evaluate the combined classifier, we used <italic>two </italic>sets of validation data. After the basic classifiers were trained, one validation set was used to estimate their ROC<sub>50 </sub>scores. The combined classifier was then constructed using these scores, and the second validation set was used to estimate its ROC<sub>50 </sub>curve. Thus, the validation data for the basic classifiers is part of the training data for the combined classifier. To do this in a cross-validation setting, we used what amounts to nested cross-validation (see Methods). As shown in Figure <xref ref-type="fig" rid="F7">7</xref>, the resulting combined classifier has a higher ROC<sub>50 </sub>score than any of the basic classifiers from which it is made.</p><p>Figure <xref ref-type="fig" rid="F1">1</xref> gives another view of the performance of the combined classifier. Here, the thin dashed lines are a superposition of ten different curves, where each one is a different estimate of the combined classifier's true ROC<sub>50 </sub>curve. As described earlier, each estimate of a classifier's ROC<sub>50 </sub>curve includes some randomness, due to the random choice of folds during cross-validation. The ten dashed curves in Figure <xref ref-type="fig" rid="F1">1</xref> are derived from ten different cross-validations, each one using a different set of folds. The thick solid line in the figure is the average of the other ten curves. Because averaging reduces variance, the average curve is a more accurate estimate of the true ROC<sub>50 </sub>curve (i.e., has lower variance) than any of the other ten curves. The diagonal dash-dot line near the bottom of the plot shows the expected performance of a random classifier.</p><p>ROC and ROC<sub>50 </sub>curves plot the number of true positives against the number of false positives. However, in applications such as ours, the <italic>precision </italic>is also of interest. Precision is the proportion of true positives (TP) among the predicted positives (PP). (It is also the complementary false discovery rate, 1-FDR [<xref ref-type="bibr" rid="B35">35</xref>].) Precision is important since each prediction is a potential experiment, and as a matter of economics, a biologist needs an estimate of how many of the experiments will succeed. This is especially important in situations, such as ours, where the number of real negatives is much greater than the number of real positives, and so there is a real possibility of having a huge number of failed experiments.</p><p>Figure <xref ref-type="fig" rid="F8">8</xref> plots estimated precision against the number of predictions for the first hundred predictions. Notice that as the number of predictions increases (<italic>i.e</italic>., as the classifier's decision threshold is lowered), the precision decreases, meaning that fewer of the predictions are expected to be true. As in Figure <xref ref-type="fig" rid="F1">1</xref>, the thin dashed lines are a superposition of ten different curves, each one an estimate of the true precision curve, and the thick solid line is their average. Also, the horizontal dash-dot line near the bottom of the plot is the expected precision of a random classifier, and its height is equal to the ratio of the number of positives (<italic>i.e</italic>., stress genes) to the total number of samples (<italic>i.e</italic>., genes) in the training data. Since all the estimated precision curves are well above the horizontal dash-dot line, the performance of the combined classifier for the first hundred predictions is significantly better than random. Also, since Figures <xref ref-type="fig" rid="F1">1</xref> and <xref ref-type="fig" rid="F8">8</xref> show small variance, and since the variance of the average curves will be even less, the combined classifier should have stable prediction performance.</p><fig position="float" id="F8"><label>Figure 8</label><caption><p><bold>Precision curves</bold>. Estimated precision curves of the combined classifier (WV), showing ten different estimates (dashed curves) and their average (solid curve).</p></caption><graphic xlink:href="1471-2105-8-358-8"/></fig></sec><sec><title>Stress-response predictions</title><p>We trained the combined classifier on our Arabidopsis data set, using all 22,746 genes for Principal Components Analysis, and the 11,553 annotated genes for supervised learning, as described above. We then applied the classifier to the 11,193 unannotated genes, to get a set of 11,193 predictions (see Methods). Table <xref ref-type="table" rid="T1">1</xref> shows the top fifty predictions. Each row in the table is a prediction: the first (leftmost) entry is the rank of the prediction (1 being the top prediction); the second entry identifies a gene; the third entry is a discriminant value (measuring the likelihood that the gene responds to stress); and the fourth entry is the estimated precision of the prediction and all predictions above it (<italic>i.e</italic>., the fraction of these predictions expected to be true). As an example, consider the 23rd row of the table, the row for gene At1g09950. Since the estimated precision in this row is given as 0.7044, we expect that about 70% of the top 23 genes respond to stress, <italic>i.e</italic>., 16 genes.</p><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>The top 50 predictions of the combined classifier ordered by discriminant value</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="center">No.</td><td align="center">Gene name</td><td align="center">Dv</td><td align="center">Pr</td></tr></thead><tbody><tr><td align="center">1</td><td align="center">At1g61340</td><td align="center">0.7879</td><td align="center">0.8491</td></tr><tr><td align="center">2</td><td align="center">At1g72660</td><td align="center">0.7315</td><td align="center">0.8423</td></tr><tr><td align="center">3</td><td align="center">At5g04340</td><td align="center">0.7269</td><td align="center">0.8405</td></tr><tr><td align="center">4</td><td align="center">At1g19180</td><td align="center">0.7219</td><td align="center">0.8448</td></tr><tr><td align="center">5</td><td align="center">At2g01520</td><td align="center">0.7017</td><td align="center">0.8311</td></tr><tr><td align="center">6</td><td align="center">At2g36220</td><td align="center">0.6987</td><td align="center">0.8293</td></tr><tr><td align="center">7</td><td align="center">At5g10695</td><td align="center">0.6912</td><td align="center">0.8138</td></tr><tr><td align="center">8</td><td align="center">At3g10020</td><td align="center">0.6850</td><td align="center">0.8030</td></tr><tr><td align="center">9</td><td align="center">At3g16050</td><td align="center">0.6778</td><td align="center">0.8000</td></tr><tr><td align="center">10</td><td align="center">At4g18280</td><td align="center">0.6673</td><td align="center">0.7945</td></tr><tr><td align="center">11</td><td align="center">At1g11210</td><td align="center">0.6636</td><td align="center">0.7955</td></tr><tr><td align="center">12</td><td align="center">At5g64510</td><td align="center">0.6514</td><td align="center">0.7900</td></tr><tr><td align="center">13</td><td align="center">At3g09350</td><td align="center">0.6412</td><td align="center">0.7807</td></tr><tr><td align="center">14</td><td align="center">At5g42380</td><td align="center">0.6357</td><td align="center">0.7718</td></tr><tr><td align="center">15</td><td align="center">At3g44860</td><td align="center">0.6278</td><td align="center">0.7623</td></tr><tr><td align="center">16</td><td align="center">At1g73260</td><td align="center">0.6252</td><td align="center">0.7583</td></tr><tr><td align="center">17</td><td align="center">At1g16850</td><td align="center">0.6186</td><td align="center">0.7452</td></tr><tr><td align="center">18</td><td align="center">At1g78070</td><td align="center">0.6185</td><td align="center">0.7439</td></tr><tr><td align="center">19</td><td align="center">At3g01830</td><td align="center">0.6098</td><td align="center">0.7398</td></tr><tr><td align="center">20</td><td align="center">At5g19875</td><td align="center">0.6094</td><td align="center">0.7402</td></tr><tr><td align="center">21</td><td align="center">At3g62260</td><td align="center">0.6040</td><td align="center">0.7213</td></tr><tr><td align="center">22</td><td align="center">At1g03070</td><td align="center">0.5961</td><td align="center">0.7106</td></tr><tr><td align="center">23</td><td align="center">At1g09950</td><td align="center">0.5942</td><td align="center">0.7044</td></tr><tr><td align="center">24</td><td align="center">At1g19020</td><td align="center">0.5867</td><td align="center">0.6928</td></tr><tr><td align="center">25</td><td align="center">At1g07430</td><td align="center">0.5866</td><td align="center">0.6919</td></tr><tr><td align="center">26</td><td align="center">At1g76960</td><td align="center">0.5860</td><td align="center">0.6901</td></tr><tr><td align="center">27</td><td align="center">At1g30070</td><td align="center">0.5838</td><td align="center">0.6819</td></tr><tr><td align="center">28</td><td align="center">At2g05510</td><td align="center">0.5799</td><td align="center">0.6726</td></tr><tr><td align="center">29</td><td align="center">At3g50930</td><td align="center">0.5796</td><td align="center">0.6726</td></tr><tr><td align="center">30</td><td align="center">At1g67360</td><td align="center">0.5767</td><td align="center">0.6691</td></tr><tr><td align="center">31</td><td align="center">At5g09530</td><td align="center">0.5758</td><td align="center">0.6703</td></tr><tr><td align="center">32</td><td align="center">At3g53230</td><td align="center">0.5737</td><td align="center">0.6663</td></tr><tr><td align="center">33</td><td align="center">At3g55970</td><td align="center">0.5694</td><td align="center">0.6586</td></tr><tr><td align="center">34</td><td align="center">At4g27657</td><td align="center">0.5676</td><td align="center">0.6549</td></tr><tr><td align="center">35</td><td align="center">At4g38080</td><td align="center">0.5658</td><td align="center">0.6458</td></tr><tr><td align="center">36</td><td align="center">At1g17380</td><td align="center">0.5651</td><td align="center">0.6448</td></tr><tr><td align="center">37</td><td align="center">At4g27652</td><td align="center">0.5647</td><td align="center">0.6445</td></tr><tr><td align="center">38</td><td align="center">At1g68500</td><td align="center">0.5588</td><td align="center">0.6204</td></tr><tr><td align="center">39</td><td align="center">At1g76650</td><td align="center">0.5573</td><td align="center">0.6146</td></tr><tr><td align="center">40</td><td align="center">At2g15960</td><td align="center">0.5549</td><td align="center">0.6074</td></tr><tr><td align="center">41</td><td align="center">At1g14870</td><td align="center">0.5520</td><td align="center">0.6017</td></tr><tr><td align="center">42</td><td align="center">At1g49450</td><td align="center">0.5497</td><td align="center">0.5991</td></tr><tr><td align="center">43</td><td align="center">At1g13930</td><td align="center">0.5467</td><td align="center">0.5942</td></tr><tr><td align="center">44</td><td align="center">At2g32190</td><td align="center">0.5453</td><td align="center">0.5914</td></tr><tr><td align="center">45</td><td align="center">At4g23493</td><td align="center">0.5429</td><td align="center">0.5879</td></tr><tr><td align="center">46</td><td align="center">At2g28400</td><td align="center">0.5418</td><td align="center">0.5842</td></tr><tr><td align="center">47</td><td align="center">At1g48720</td><td align="center">0.5399</td><td align="center">0.5780</td></tr><tr><td align="center">48</td><td align="center">At3g02480</td><td align="center">0.5384</td><td align="center">0.5721</td></tr><tr><td align="center">49</td><td align="center">At2g43620</td><td align="center">0.5376</td><td align="center">0.5677</td></tr><tr><td align="center">50</td><td align="center">At4g14270</td><td align="center">0.5373</td><td align="center">0.5676</td></tr></tbody></table><table-wrap-foot><p>Pr, estimated precision; Dv, discriminant value.</p></table-wrap-foot></table-wrap><p>Figures <xref ref-type="fig" rid="F9">9</xref> and <xref ref-type="fig" rid="F10">10</xref> provide visual evidence supporting these predictions. Each figure shows a heat map. These maps, known as "electronic Northerns" (or e-Northerns), were generated using the Expression Browser tool of the Botany Array Resource (BAR) and the AtGenExpress Stress Series (shoot) data set[<xref ref-type="bibr" rid="B23">23</xref>]. The program contains expression data for more than 22,000 genes across more than 1000 samples collected from NASCArrays, AtGenExpress Consortium, and the Department of Botany at the University of Toronto [<xref ref-type="bibr" rid="B22">22</xref>-<xref ref-type="bibr" rid="B24">24</xref>,<xref ref-type="bibr" rid="B36">36</xref>]. Each row in an e-Northern is a gene, and each column is an experiment. The colour at a point represents the relative expression level of the gene during the experiment. More specifically, the colour represents the log<sub>2 </sub>of the ratio of the average of replicate treatments relative to the average of corresponding controls. Yellow means that under the experimental conditions, the gene had the same expression level as the control. (The wide, yellow vertical stripes are the controls.) Red means that the gene had a higher expression level than the control (up-regulation), and blue means it had a lower expression level (down-regulation). A gene that shows significant up-regulation (or down-regulation) under stress conditions is likely to be involved in response to stress. Thus, unlike cross validation, electronic Northerns provide a means of evaluating the quality of predictions based on the prediction data, not just the training data. The e-Northerns of Figures <xref ref-type="fig" rid="F9">9</xref> and <xref ref-type="fig" rid="F10">10</xref>, for instance, are based entirely on prediction data. In these e-Northerns, the experiments exposed the plant to various stress conditions, such as heat, cold, drought, UV-B radiation, etc. Figure <xref ref-type="fig" rid="F9">9</xref> is the e-Northern for the top-50 predictions of our combined classifier, <italic>i.e</italic>., for the 50 genes predicted to most likely to respond to stress. For comparison, Figure <xref ref-type="fig" rid="F10">10</xref> is the e-Northern for 50 genes chosen at random from the prediction set. Note that there is much more colour in Figure <xref ref-type="fig" rid="F9">9</xref> than in Figure <xref ref-type="fig" rid="F10">10</xref>, especially red. This suggests that our combined classifier has indeed extracted meaningful gene expression patterns for genes that respond to stress.</p><fig position="float" id="F9"><label>Figure 9</label><caption><p><bold>Electronic Northern analysis</bold>. E-Northern of the top 50 predictions.</p></caption><graphic xlink:href="1471-2105-8-358-9"/></fig><fig position="float" id="F10"><label>Figure 10</label><caption><p><bold>Electronic Northern analysis</bold>. E-Northern of 50 randomly selected genes.</p></caption><graphic xlink:href="1471-2105-8-358-10"/></fig></sec><sec><title>Gene knockout experiments</title><p>From the predictions of the combined classifier, three genes were chosen for biological analysis using gene knockout experiments. Here, we present the results for one of these genes, At1g16850, which show it to be necessary for the normal response to temperature and NaCl. Our results also confirm that the other two genes, At1g11210 and At4g39675, are involved in a variety of stress responses (data not shown).</p><p>The criteria used to choose candidate genes for subsequent biological analysis were: 1) the gene must be expressed in either root or shoot, 2) gene expression should be strongly increased in response to abiotic stress, such as cold, drought, osmotic and salt stresses, 3) T-DNA knockout lines &#x02013; in which a given gene's expression has been eliminated &#x02013; should available from the Salk Institute [<xref ref-type="bibr" rid="B37">37</xref>], and 4) the gene should not have an annotated function nor be present in any patent database. Further bioinformatics analysis was performed using Athena for promoter motif prediction [<xref ref-type="bibr" rid="B38">38</xref>], Expression Angler for co-expressed gene analysis [<xref ref-type="bibr" rid="B22">22</xref>] and eFP browser for electronic representation of gene expression patterns [<xref ref-type="bibr" rid="B39">39</xref>].</p><sec><title>Stress response</title><p>The increased presence of anthocyanin levels in plants lacking a functional copy of the At1g16850 gene during cold stress of 4C indicates that this gene is involved in cold stress response (Figure <xref ref-type="fig" rid="F11">11</xref>). The same effect is seen at 30C, indicating that this gene is also associated with response to heat stress (Figure <xref ref-type="fig" rid="F11">11</xref>). Interestingly, At1g16850 is normally expressed during the later stages of seed maturation, towards seed dessication, and hence may play a role in seed dormancy. This sort of bifunctionality is seen with other stress response genes, which have documented roles in the cold, heat and salt stress pathways, e.g. RD29A (Response to Desiccation) and LEA (Late Embryogensis Abundant) protein [<xref ref-type="bibr" rid="B40">40</xref>,<xref ref-type="bibr" rid="B41">41</xref>]. These proteins have also been found to accumulate during seed maturation [<xref ref-type="bibr" rid="B40">40</xref>,<xref ref-type="bibr" rid="B41">41</xref>] and are in fact co-expressed with At1g16850 under stress conditions and during seed maturation, as determined using the Expression Angler algorithm [<xref ref-type="bibr" rid="B22">22</xref>].</p><fig position="float" id="F11"><label>Figure 11</label><caption><p><bold>Gene knockout experiments</bold>. 10 day old wild-type and mutant plants after exposure for 7 days at 14. (a) The mutant cotyledons appear darker than wild-type due to increased anthocyanin levels. (b) mutant and wild-type seeds 24 h after sowing on agar plates. Mutant seeds have the appearance of lighter colour compared to wild-type. (c) Quantification of anthocyanin levels measuring A535. Bars indicate standard error of 5 replicate measurements. * indicates significantly different at <italic>p </italic>&#x0003c; 0.05</p></caption><graphic xlink:href="1471-2105-8-358-11"/></fig><p>In addition to modulating a response to temperature, plants lacking a functional At1g16850 exhibit a defective root growth phenotype under increasing salt concentrations (Figure <xref ref-type="fig" rid="F12">12</xref>). This phenotype, combined with previous microarray studies [<xref ref-type="bibr" rid="B42">42</xref>], which found At1g16850 induction at 250 mM NaCl, gives clear indication that At1g16850 is also part of the salt stress response pathway.</p><fig position="float" id="F12"><label>Figure 12</label><caption><p><bold>Gene knockout experiments</bold>. Root growth on 50 mM NaCl, relative to growth on 0 mM NaCl, on 10 day old wild-type and mutant plants transferred to 50 mM NaCl medium. Error bars indicate the standard error of 5 replicates. <italic>n </italic>= 25 measurements per treatment and genotype. * indicates significantly different at <italic>p </italic>&#x0003c; 0.001</p></caption><graphic xlink:href="1471-2105-8-358-12"/></fig></sec></sec></sec><sec><title>Conclusion</title><p>In this study, we evaluated and compared five basic supervised learning methods (LR, LDA, QDA, NB and KNN) for gene function prediction in <italic>A. thaliana </italic>based solely on gene expression data. The major advantage of supervised methods over unsupervised methods is that by including prior knowledge of class information, supervised methods can ignore uninformative features and select informative features that are useful for separating classes. In this study, we focussed on finding genes that respond to stress, as represented by the term <italic>GO:0006950 [response to stress] </italic>in the GOBP hierarchy. Using a training set of genes of known function, we used the basic learning methods to predict the stress response of genes of unknown function. We estimated the accuracy of the predictions using ROC<sub>50 </sub>scores derived through cross validation. We found, for instance, that KNN performs well for various values of K. For the other learning methods, the performance depends greatly on whether the data is preprocessed using PCA, and on how much its dimensionality is reduced. Using various values of K and various amounts of dimensionality reduction, we trained and tested a total of 33 basic classifiers.</p><p>We also investigated combining the basic classifiers using weighted voting. Our method of constructing the combined classifier chooses not only the best combination of supervised learning methods, but also the best amount of dimensionality reduction for each method. Our results show that the combined classifier outperforms all the basic classifiers in predicting whether a gene responds to stress. This can be attributed to the relative robustness of methods for combining classifiers. Intuitively, any single learning method represents a single view of the data, while a combination method represents multiple views strategically combined. The proper choice of combining method is important to the success of a combined classifier. For example, model averaging and stacking are well-known methods for combining classifiers [<xref ref-type="bibr" rid="B21">21</xref>]; however, we found that while they did improve on the overall ROC curves of the basic classifiers, the ROC<sub>50 </sub>curve was often worse (data not shown). In contrast, our weighted voting method using ROC<sub>50 </sub>scores as weights is simple, provides improved accuracy in predicting stress response in <italic>A. thaliana</italic>, and we would expect it to provide improved accuracy in other organisms and for other gene functions.</p><p>Using electronic Northern analysis, we observed significant up-regulation and down-regulation of many of our predictions. The strong up- and down-regulation are also present among the stress-response genes in the training data (data not shown). In contrast, randomly selected genes show much less up- and down-regulation. This visually confirms that the combined classifier is able to distinguish between stress and non-stress genes. Moreover, unlike cross-validation, this confirmation is based on the prediction data, not the training data.</p><p>Using gene knockout experiments &#x02013; in which a given gene's expression is eliminated &#x02013; we tested three of our predictions. We presented the results for one of these genes, At1g16850, which show it to be involved in the stress response pathways to cold (4C), chill (14C) and NaCl. We have also confirmed the biological stress responsive roles of the other two genes, At1g11210 and At4g39675 (data not shown). Further biological studies will determine the pattern of expression in specific cell and tissues types of the plant and the exact physiological role of these genes.</p></sec><sec sec-type="methods"><title>Methods</title><sec><title>Preprocessing of raw gene expression data</title><p>The gene expression data from the Botany Array Resource at the University of Toronto contain <italic>detection calls</italic>: P (present), M (marginal) and A (absent). The detection call determines whether a transcript is reliably detected (present), partially detected (marginal), or not detected (absent). The following is an example for the gene <italic>At3g24440 </italic>under three selected conditions:</p><p>AT3G24440 : 243.10 P : 120.90 A : 109.40 M</p><p>We simply removed these detection calls (P, A, and M) in this study. In addition, gene expression levels were log transformed. The transformed data have approximately normal distributions while the raw data have approximately exponential distributions (data not shown). Many of the learning methods used in this study were designed with normal data in mind.</p></sec><sec><title>Basic supervised learning methods</title><p>Each of the learning methods described below trains a discriminative classifier. We used the methods to train binary classifiers in which the two classes correspond to genes that respond to stress (Class 1) and genes that do not (Class 0). Given a vector, <bold>x</bold>, of gene expression measurements, each classifier returns a discriminant value, <italic>dv</italic>(<bold>x</bold>), reflecting the classifier's confidence that the gene belongs to Class 1. The gene is assigned to Class 1 if and only if <italic>dv</italic>(<bold>x</bold>) &#x0003e; <italic>&#x003c4;</italic>, where <italic>&#x003c4; </italic>is a decision threshold. For the classifiers LR, LDA, QDA and NB, the discriminate value is an estimate of <italic>p</italic>(<italic>k </italic>= 1|<bold>x</bold>), the posterior probability that the gene is in Class 1. For KNN, the discriminant value is simply a number between 0 and 1.</p><sec><title>LR (Logistic Regression)</title><p>Given a set of classes, LR models the log likelihood ratio for any pair of classes as a linear function of the test vector, <bold>x</bold>, and thus defines linear decision boundaries between the classes. In the case of just two classes, the model has the simple form</p><p><disp-formula id="bmcM1"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M10" name="1471-2105-8-358-i7" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#x003b2;</mml:mi><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mi>x</mml:mi></mml:mrow><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaieGacqWFSbaBcqWFVbWBcqWFNbWzdaWcaaqaaiabdchaWjabcIcaOiabdUgaRjabg2da9iabigdaXiabcYha8Hqabiab+Hha4jabcMcaPaqaaiabdchaWjabcIcaOiabdUgaRjabg2da9iabicdaWiabcYha8jab+Hha4jabcMcaPaaacqGH9aqpiiGacqqFYoGydaWgaaWcbaGaeGimaadabeaakiabgUcaRiab9j7aInaaDaaaleaacqaIXaqmaeaacqWGubavaaGccqGF4baEaaa@4DC2@</mml:annotation></mml:semantics></mml:math></disp-formula></p><p>and hence,</p><p><disp-formula id="bmcM2"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M11" name="1471-2105-8-358-i8" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#x003b2;</mml:mi><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#x003b2;</mml:mi><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGWbaCcqGGOaakcqWGRbWAcqGH9aqpcqaIXaqmcqGG8baFieqacqWF4baEcqGGPaqkcqGH9aqpdaWcaaqaaiabdwgaLnaaCaaaleqabaacciGae4NSdi2aaSbaaWqaaiabicdaWaqabaWccqGHRaWkcqGFYoGydaqhaaadbaGaeGymaedabaGaemivaqfaaSGae8hEaGhaaaGcbaGaeGymaeJaey4kaSIaemyzau2aaWbaaSqabeaacqGFYoGydaWgaaadbaGaeGimaadabeaaliabgUcaRiab+j7aInaaDaaameaacqaIXaqmaeaacqWGubavaaWccqWF4baEaaaaaaaa@4E33@</mml:annotation></mml:semantics></mml:math></disp-formula></p><p><disp-formula id="bmcM3"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M12" name="1471-2105-8-358-i9" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>&#x003b2;</mml:mi><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGWbaCcqGGOaakcqWGRbWAcqGH9aqpcqaIWaamcqGG8baFieqacqWF4baEcqGGPaqkcqGH9aqpdaWcaaqaaiabigdaXaqaaiabigdaXiabgUcaRiabdwgaLnaaCaaaleqabaacciGae4NSdi2aaSbaaWqaaiabicdaWaqabaWccqGHRaWkcqGFYoGydaWgaaadbaacbaGae0xmaedabeaaliab=Hha4baaaaaaaa@4356@</mml:annotation></mml:semantics></mml:math></disp-formula></p><p>and <italic>p</italic>(<italic>k </italic>= 1|<bold>x</bold>) + <italic>p</italic>(<italic>k </italic>= 0|<bold>x</bold>) = 1. The parameters <italic>&#x003b2;</italic><sub>0 </sub>and <italic>&#x003b2;</italic><sub>1 </sub>are fitted to the training data using maximum likelihood [<xref ref-type="bibr" rid="B21">21</xref>].</p></sec><sec><title>LDA (Linear Discriminant Analysis)</title><p>LDA models the classes as multivariate Gaussians, where each class is assumed to have the same covariance matrix. The density function for class <italic>k </italic>is therefore given by</p><p><disp-formula id="bmcM4"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M13" name="1471-2105-8-358-i10" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>&#x003a3;</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>&#x003a3;</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGNbWzdaWgaaWcbaGaem4AaSgabeaakiabcIcaOGqabiab=Hha4jabcMcaPiabg2da9maalaaabaGaeGymaedabaGaeiikaGIaeGOmaidcciGae4hWdaNaeiykaKYaaWbaaSqabeaacqWGWbaCcqGGVaWlcqaIYaGmaaGcdaabdaqaaiabfo6atbGaay5bSlaawIa7amaaCaaaleqabaGaeGymaeJaei4la8IaeGOmaidaaaaakiabdwgaLnaaCaaaleqabaGaeyOeI0IaeiikaGIae8hEaGNaeyOeI0Iae4hVd02aaSbaaWqaaiab=TgaRbqabaWccqGGPaqkdaahaaadbeqaaiabdsfaubaaliabfo6atnaaCaaameqabaGaeyOeI0IaeGymaedaaSGaeiikaGIae8hEaGNaeyOeI0Iae4hVd02aaSbaaWqaaiab=TgaRbqabaWccqGGPaqkcqGGVaWlcqaIYaGmaaaaaa@5C4A@</mml:annotation></mml:semantics></mml:math></disp-formula></p><p>where <italic>&#x003bc;</italic><sub><italic>k </italic></sub>is the mean vector for class <italic>k</italic>, &#x003a3; is the common covariance matrix, and <italic>p </italic>is the dimensionality of <bold>x</bold>. It can be shown [<xref ref-type="bibr" rid="B21">21</xref>] that the discriminant function for class <italic>k </italic>is equivalent to the following function:</p><p><disp-formula id="bmcM5"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M14" name="1471-2105-8-358-i11" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mrow><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mi>&#x003a3;</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mi>&#x003bc;</mml:mi><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi>&#x003a3;</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBamXvP5wqSXMqHnxAJn0BKvguHDwzZbqegyvzYrwyUfgarqqtubsr4rNCHbGeaGqiA8vkIkVAFgIELiFeLkFeLk=iY=Hhbbf9v8qqaqFr0xc9pk0xbba9q8WqFfeaY=biLkVcLq=JHqVepeea0=as0db9vqpepesP0xe9Fve9Fve9GapdbaqaaeGacaGaaiaabeqaamqadiabaaGcbaacciGae8hTdq2aaSbaaSqaaiabdUgaRbqabaGccqGGOaakimqacaGF4bGaeiykaKIaeyypa0Jaa4hEamaaCaaaleqabaGaemivaqfaaOGaeu4Odm1aaWbaaSqabeaacqGHsislcqaIXaqmaaGccqWF8oqBdaWgaaWcbaGaem4AaSgabeaakiabgkHiTmaalaaabaGaeGymaedabaGaeGOmaidaaiab=X7aTnaaDaaaleaacqWGRbWAaeaacqWGubavaaGccqqHJoWudaahaaWcbeqaaiabgkHiTiabigdaXaaakiab=X7aTnaaBaaaleaacqWGRbWAaeqaaOGaey4kaSccdiGaa0hBaiaa99gacaqFNbGae8hWda3aaSbaaSqaaiabdUgaRbqabaaaaa@624C@</mml:annotation></mml:semantics></mml:math></disp-formula></p><p>where <italic>&#x003c0;</italic><sub><italic>k </italic></sub>is the prior probability of class <italic>k</italic>. The decision boundaries and therefore linear. The parameters <italic>&#x003c0;</italic><sub><italic>k</italic></sub>, <italic>&#x003bc;</italic><sub><italic>k </italic></sub>and &#x003a3; are estimated by applying maximum likelihood to the training data [<xref ref-type="bibr" rid="B21">21</xref>], giving</p><p><disp-formula id="bmcM6"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M15" name="1471-2105-8-358-i12" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mrow><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:mrow><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWFapaCdaWgaaWcbaGaem4AaSgabeaakiabg2da9maalaaabaGaemOBa42aaSbaaSqaaiabdUgaRbqabaaakeaacqWGUbGBaaaaaa@357A@</mml:annotation></mml:semantics></mml:math></disp-formula></p><p><disp-formula id="bmcM7"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M16" name="1471-2105-8-358-i13" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWF8oqBdaWgaaWcbaGaem4AaSgabeaakiabg2da9maaqafabaWaaSaaaeaaieqacqGF4baEdaWgaaWcbaGaemyAaKgabeaaaOqaaiabd6gaUnaaBaaaleaacqWGRbWAaeqaaaaaaeaacqWGPbqAaeqaniabggHiLdaaaa@3A86@</mml:annotation></mml:semantics></mml:math></disp-formula></p><p><disp-formula id="bmcM8"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M17" name="1471-2105-8-358-i14" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mrow><mml:mi>&#x003a3;</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqqHJoWucqGH9aqpdaaeqbqaamaaqafabaWaaSaaaeaacqGGOaakieqacqWF4baEdaWgaaWcbaGaemyAaKgabeaakiabgkHiTGGaciab+X7aTnaaBaaaleaacqWGRbWAaeqaaOGaeiykaKIaeiikaGIae8hEaG3aaSbaaSqaaiabdMgaPbqabaGccqGHsislcqGF8oqBdaWgaaWcbaGaem4AaSgabeaakiabcMcaPmaaCaaaleqabaGaemivaqfaaaGcbaGaeiikaGIaemOBa4MaeyOeI0Iaem4saSKaeiykaKcaaaWcbaGaem4zaC2aaSbaaWqaaiabdMgaPbqabaWccqGHiiIZcqWGRbWAaeqaniabggHiLdaaleaacqWGRbWAaeqaniabggHiLdaaaa@532D@</mml:annotation></mml:semantics></mml:math></disp-formula></p><p>where <italic>n </italic>is the total number of training samples, <italic>n</italic><sub><italic>k </italic></sub>is the number of training samples in class <italic>k</italic>, and <italic>K </italic>is the number of classes. In this study, <italic>K </italic>= 2.</p></sec><sec><title>QDA (Quadratic Discriminant Analysis)</title><p>QDA is a generalization of LDA in which each class has its own covariance matrix, S<sub><italic>k</italic></sub>. In this case, it can be shown [<xref ref-type="bibr" rid="B21">21</xref>] that the discriminant function for class <italic>k </italic>is equivalent to the following function:</p><p><disp-formula id="bmcM9"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M18" name="1471-2105-8-358-i15" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mrow><mml:msub><mml:mi>&#x003b4;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>&#x003a3;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msubsup><mml:mi>&#x003a3;</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWF0oazdaWgaaWcbaGaem4AaSgabeaakiabcIcaOGqabiab+Hha4jabcMcaPiabg2da9iabgkHiTmaalaaabaGaeGymaedabaGaeGOmaidaaGqaciab9XgaSjab99gaVjab9DgaNnaabmaabaWaaqWaaeaacqqHJoWudaWgaaWcbaGaem4AaSgabeaaaOGaay5bSlaawIa7aaGaayjkaiaawMcaaiabgkHiTmaalaaabaGaeGymaedabaGaeGOmaidaaiabcIcaOiab+Hha4jabgkHiTiab=X7aTnaaBaaaleaacqWGRbWAaeqaaOGaeiykaKYaaWbaaSqabeaacqWGubavaaGccqqHJoWudaqhaaWcbaGaem4AaSgabaGaeyOeI0IaeGymaedaaOGaeiikaGIae4hEaGNaeyOeI0Iae8hVd02aaSbaaSqaaiabdUgaRbqabaGccqGGPaqkcqGHRaWkcqqFSbaBcqqFVbWBcqqFNbWzcqWFapaCdaWgaaWcbaGaem4AaSgabeaaaaa@6300@</mml:annotation></mml:semantics></mml:math></disp-formula></p><p>The decision boundaries are therefore quadratic. Again, the parameters are estimated by applying maximum likelihood to the training data [<xref ref-type="bibr" rid="B21">21</xref>].</p></sec><sec><title>NB (Naive Bayes)</title><p>NB is based on the independent variable assumption: for each class, the variables in the feature vector <bold>x </bold>are assumed to be independent. This assumption allows the class conditional density <italic>p</italic>(<italic>x</italic><sub><italic>i</italic></sub>|<italic>k</italic>) to be estimated separately for each variable, <italic>x</italic><sub><italic>i</italic></sub>. In essence, NB reduces the problem of multi-dimensional density estimation to that of one-dimensional density estimation. Given a class, <italic>k</italic>, each variable in the feature vector <bold>x </bold>= (<italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>, ..., <italic>x</italic><sub><italic>p</italic></sub>)<sup><italic>T </italic></sup>is independent; so</p><p><disp-formula id="bmcM10"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M19" name="1471-2105-8-358-i16" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x0220f;</mml:mo><mml:mi>i</mml:mi><mml:mi>p</mml:mi></mml:munderover><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGWbaCcqGGOaakieqacqWF4baEcqGG8baFcqWGRbWAcqGGPaqkcqGH9aqpdaqeWbqaaiabdchaWjabcIcaOiabdIha4naaBaaaleaacqWGPbqAaeqaaOGaeiiFaWNaem4AaSMaeiykaKcaleaacqWGPbqAaeaacqWGWbaCa0Gaey4dIunaaaa@4324@</mml:annotation></mml:semantics></mml:math></disp-formula></p><p>Using Bayes Rule, we obtain</p><p><disp-formula id="bmcM11"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M20" name="1471-2105-8-358-i17" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x0221d;</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x0220f;</mml:mo><mml:mi>i</mml:mi><mml:mi>p</mml:mi></mml:munderover><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGWbaCcqGGOaakcqWGRbWAcqGG8baFieqacqWF4baEcqGGPaqkcqGHDisTcqWGWbaCcqGGOaakcqWGRbWAcqGGPaqkdaqeWbqaaiabdchaWjabcIcaOiabdIha4naaBaaaleaacqWGPbqAaeqaaOGaeiiFaWNaem4AaSMaeiykaKcaleaacqWGPbqAaeaacqWGWbaCa0Gaey4dIunaaaa@4818@</mml:annotation></mml:semantics></mml:math></disp-formula></p><p>where <italic>p</italic>(<italic>k</italic>) is the prior probability of class <italic>k</italic>, estimated as the ratio of the number of the training samples in class <italic>k </italic>to the total number of training samples. In this paper, we model each variable as a univariate Gaussian, so <italic>p</italic>(<italic>x</italic><sub><italic>i</italic></sub>|<italic>k</italic>) = <italic>N</italic>(<inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M21" name="1471-2105-8-358-i18" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mrow><mml:msubsup><mml:mi>&#x003bc;</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:mrow><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWF8oqBdaqhaaWcbaGaemyAaKgabaGaem4AaSgaaaaa@3150@</mml:annotation></mml:semantics></mml:math></inline-formula>, <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M22" name="1471-2105-8-358-i19" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mrow><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:mrow><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWFdpWCdaqhaaWcbaGaemyAaKgabaGaem4AaSgaaaaa@315D@</mml:annotation></mml:semantics></mml:math></inline-formula>), where the parameters <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M23" name="1471-2105-8-358-i18" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mrow><mml:msubsup><mml:mi>&#x003bc;</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:mrow><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWF8oqBdaqhaaWcbaGaemyAaKgabaGaem4AaSgaaaaa@3150@</mml:annotation></mml:semantics></mml:math></inline-formula> and <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M24" name="1471-2105-8-358-i19" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mrow><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:mrow><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWFdpWCdaqhaaWcbaGaemyAaKgabaGaem4AaSgaaaaa@315D@</mml:annotation></mml:semantics></mml:math></inline-formula> are estimated by applying maximum likelihood to the training data [<xref ref-type="bibr" rid="B21">21</xref>]. Note that NB has far fewer parameters to estimate than either LDA or QDA, and for this reason, it often performs surprisingly well in practise, despite the unrealistic assumption of independent variables [<xref ref-type="bibr" rid="B21">21</xref>].</p></sec><sec><title>KNN (K-Nearest Neighbors)</title><p>KNN is a nonparametric method, since it does not require the estimation of any parameters. Instead, to classify a test vector, KNN finds the vector's <italic>K </italic>nearest neighbors in the training data. If <italic>K</italic><sub>1 </sub>is the number of these neighbors in Class 1, then <italic>K</italic><sub>1</sub><italic>/K </italic>is returned as the discriminant value. The test vector is therefore assigned to Class 1 if and only if <italic>K</italic><sub>1</sub><italic>/K </italic>&#x0003e; <italic>&#x003c4;</italic>, where <italic>&#x003c4; </italic>is the decision threshold.</p><p>A variety of different distance measures can be used with KNN to measure the nearness of one vector to another. In this paper, we use 1 - <italic>&#x003c1;</italic>, where <italic>&#x003c1; </italic>is the Pearson correlation coefficient of the two vectors. That is, if the two vectors are <bold>x </bold>and <bold>y</bold>, then</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M25" name="1471-2105-8-358-i20" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mrow><mml:mi>&#x003c1;</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>&#x000af;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWFbpGCcqGH9aqpdaWcaaqaaiabcIcaOGqabiab+Hha4jabgkHiTiqb+Hha4zaaraGaeiykaKYaaWbaaSqabeaacqWGubavaaGccqGGOaakcqGF5bqEcqGHsislcuGF5bqEgaqeaiabcMcaPaqaamaakaaabaGaeiikaGIae4hEaGNaeyOeI0Iaf4hEaGNbaebacqGGPaqkdaahaaWcbeqaaiabdsfaubaakiabcIcaOiab+Hha4jabgkHiTiqb+Hha4zaaraGaeiykaKIaeiikaGIae4xEaKNaeyOeI0Iaf4xEaKNbaebacqGGPaqkdaahaaWcbeqaaiabdsfaubaakiabcIcaOiab+Lha5jabgkHiTiqb+Lha5zaaraGaeiykaKcaleqaaaaaaaa@55AC@</mml:annotation></mml:semantics></mml:math></disp-formula></p><p>In terms of gene expression measurements, two genes are highly correlated if their expression levels tend to rise and fall together (even though their absolute expression levels may be quite different). For this reason, Pearson correlation is often used to detect coregulation among genes [<xref ref-type="bibr" rid="B2">2</xref>].</p></sec></sec><sec><title>Principal components analysis</title><p>Hidden dependencies and noise among experiments may confound the classification problem. In particular, experiments that are biologically different may actually be similar in terms of gene expression. Principal components analysis (PCA) helps to identify independent information in the data by transforming it to a data set of reduced dimension. The attributes of the reduced data set, called principal components, explain most of the variance in the original data and are mutually uncorrelated and orthogonal [<xref ref-type="bibr" rid="B21">21</xref>]. In addition, by reducing the dimension of the data, PCA reduces the number of parameters that must be estimated during supervised learning, thus permitting more efficient use of the data.</p><p>One can think of PCA as having a learning phase and a prediction phase. During learning, PCA is given a data set, from which it generates (learns) a linear transformation. This transformation maps high-dimensional vectors to low-dimensional vectors, and is applied to the given data set to reduce its dimensionality. During prediction, the transformation is applied to other data.</p><p>We used PCA to reduce the dimensionality of the gene expression data from its original 290 dimensions to <italic>p </italic>dimensions, for <italic>p </italic>= 5, 10, 15, 20, 40, 100. During learning, we gave PCA our entire data set of 22,746 genes, <italic>i.e</italic>., the 11,533 annotated genes and the 11,193 unannotated genes. This is possible because PCA is a form of unsupervised learning, so it uses only the gene expression measurements (which are known), and not the gene annotations (which are to be learned). This increases the effectiveness of PCA by doubling the amount of data that it uses during learning. That is, using a larger data set decreases the variance of the principal components learned by PCA, thus increasing their statistical significance and reducing the number of anomalous components.</p><p>It is worth noting that this use of PCA is different from that of many traditional applications of machine learning. This is because we apply PCA to the entire data set during learning, including the prediction data (<italic>i.e</italic>., the unannotated data). This is not possible in traditional applications simply because the prediction data is not known during learning. In such applications, a learning procedure is first trained and tested on one data set, and then applied to prediction data as it becomes available. This is not the situation for genome-wide expression experiments, since all the genes (and their expression levels) are known in advance, including the genes in the prediction set. PCA can therefore use both the prediction data and the training data during learning. This is a form of transductive inference [<xref ref-type="bibr" rid="B29">29</xref>,<xref ref-type="bibr" rid="B31">31</xref>], in which the prediction data is known and exploited during learning.</p></sec><sec><title>PCA and classifier evaluation</title><p>After PCA is performed on the entire data set, supervised learning is performed on the annotated portion of the dimensionally-reduced data. (As described earlier, this is a form of semi-supervised learning [<xref ref-type="bibr" rid="B29">29</xref>,<xref ref-type="bibr" rid="B30">30</xref>]). The result is a set of classifiers, one for each supervised learning method. The classifiers are then applied to the unannotated portion of the dimensionally-reduced data to predict the missing annotations. Cross validation was used to estimate the accuracy of these predictions.</p><p>Before discussing our use of cross validation, we consider the simpler setting in which the annotated data is divided into two parts, training data and validation data [<xref ref-type="bibr" rid="B21">21</xref>]. This will clarify our handling of PCA during validation. In this setting, classifier evaluation proceeds as follows. First, PCA is applied to the entire data set (training, validation and prediction data) to produce a dimensionally-reduced data set. Then, a supervised learner uses the (dimensionally-reduced) training data to produce a classifier. Finally, the accuracy of the classifier is estimated using the (dimensionally-reduced) validation data. Note that this process treats the validation and prediction data equally. That is, they are <italic>both </italic>used during unsupervised learning, and <italic>neither </italic>is used during supervised learning. In this way, the validation data is representative of the prediction data, as it should be. Also note that PCA is now effectively a preprocessing phase prior to supervised learning.</p><p>Because PCA is applied to the entire data set, this validation process estimates the accuracy of the classifier on prediction data that is known and used during learning. In particular, the estimate does <italic>not </italic>apply to new prediction data that might arrive in the future (<italic>e.g</italic>., if new genes were discovered). In fact, it would likely be an overestimate of classifier accuracy on such data. However, this is not an issue in our application, since most if not all of the genes in <italic>Arabidopsis </italic>are already known. Moreover, even if some new genes were to be discovered in the future, we could simply add them to our prediction data and retrain the classifier on the enlarged data set.</p><p>The above ideas are easily extended to cross validation. First, PCA is applied to the entire data set. Then, a supervised learner uses the annotated portion of the dimensionally-reduced data to produce a classifier. Finally, this classifier is evaluated by cross validation in the normal way, as described below. Note that this approach has the added computational advantage that PCA is applied only once, to the entire data set, and not over-and-over again during the many training phases of cross validation. The discussions below assume that the entire data set has been preprocessed using PCA, so that all references to data refer to the dimensionally-reduced data. Also, all references to generalization performance refer to the accuracy of the classifier on the given set of prediction data.</p></sec><sec><title>Cross validation</title><p>We used 20-fold cross-validation to assess the generalization performance of each classifier as well as to estimate the precision of its predictions. We randomly divided the annotated data into 20 non-overlapping, equal-sized parts, called folds. The classifier was trained on 19 of these folds, and tested on the remaining fold; <italic>i.e</italic>., the trained classifier was used to generate a discriminant value for each gene in the remaining fold. This was done in all 20 possible ways, using a different testing fold each time. In this way, a discriminant value, <italic>dv</italic>, was generated for every gene in the training set. Each gene in the training set was then predicted to be positive (<italic>i.e</italic>., to respond to stress) if and only if <italic>dv </italic>&#x0003e; <italic>&#x003c4;</italic>, where <italic>&#x003c4; </italic>is a decision threshold. From these predictions, true and false positives were computed, from which a point on the ROC<sub>50 </sub>curve was plotted. Using a large number of different decision thresholds, we plotted a large number of points on the ROC<sub>50 </sub>curve, effectively generating the entire curve. The area under this curve is the ROC<sub>50 </sub>score. To get an idea of how stable the estimated performance of the classifier is, we repeated the entire cross-validation and curve-generation procedure 10 times, each time using a different, random, 20-fold split of the training data.</p><p>The above procedure was applied to all the basic classifiers, but assessing the combined classifier involved an additional subtlety. Recall that the combined classifier is a linear combination of the basic classifiers, where the weight given to a basic classifier is proportional to its estimated ROC<sub>50 </sub>score. The subtlety is in computing that score. A naive approach would be to simply use the above procedure to compute a ROC<sub>50 </sub>score for each basic classifier. However, this would mean that during cross validation, 19 of the 20 folds are used to train the basic classifiers, while the 20<sup><italic>th </italic></sup>fold is used to compute the ROC<sub>50 </sub>scores. The result is that <italic>all 20 folds </italic>are involved in computing the weights. Thus, all 20 folds are involved in constructing (<italic>i.e</italic>., training) the combined classifier, so no folds are left for testing it. If cross validation were used anyway to assess the combined classifier, it would amount to using training data as testing data, and the results would tend to overestimate the classifier's performance.</p><p>As described earlier, we surmount this problem by using two sets of validation data. Loosely speaking, 18 of the 20 folds are used to train the basic classifiers, a 19<sup><italic>th </italic></sup>fold is used to compute their ROC<sub>50 </sub>scores, and the 20<sup><italic>th </italic></sup>fold is used to test the combined classifier. This results in what might be called <italic>nested </italic>cross validation. To start, the training data are divided randomly into 20 folds. Picking one of these as a testing fold, the other 19 are used to train the combined classifier. This in turn involves 19-fold cross validation to train and test the basic classifiers (and compute their ROC<sub>50 </sub>scores). Thus, each time the combined classifier is trained once, the basic classifiers are trained 19 times. Since the combined classifier is trained 20 times, each basic classifier is trained a total of 20 &#x000d7; 19 = 380 times. A similar form of nested cross validation is involved in Stacking [<xref ref-type="bibr" rid="B21">21</xref>].</p></sec><sec><title>Predicting gene function and estimating precision</title><p>To predict which genes respond to stress, we first train a combined classifier using the 11,553 annotated genes in the training data. The classifier is then applied to the 11,193 unannotated genes in the prediction data. After this step, each annotated gene has a discriminant value, <italic>dv</italic>. The unannotated genes are then sorted in descending order by discriminant value, as illustrated in Table <xref ref-type="table" rid="T1">1</xref>. To make actual predictions, a gene in the sorted list is chosen as a decision point. This gene and every gene above it in the sorted list are then predicted to respond to stress. In other words, suppose <italic>dv </italic>is the discriminant value of the chosen gene. An unannotated gene is then predicted to respond to stress if and only if its discriminant value is at least <italic>dv</italic>. The fraction of these predictions that are true is the <italic>precision </italic>of the predictions. We estimate this precision using the training data. Recall that each gene in the training set has a discriminant value assigned to it during cross validation. We also know which of these genes respond to stress. To estimate the precision of our predictions, we look at those genes in the training set whose discriminant value is at least <italic>dv</italic>. The fraction of them that respond to stress is an estimate of precision.</p><p>Using this idea we actually get ten precision estimates, not one. This is because we do cross validation ten times, using ten different random splits of the data. The result is that each gene in the training set receives ten discriminant values, and for each one we get a different precision estimate. We could simply use the average of these ten precision estimates; however, to reduce the variance of the estimate, we use a weighted average. Specifically, let us number the cross validation runs from <italic>i </italic>= 1, &#x02026;, 10. Then, given a discriminant value, <italic>dv</italic>, let <italic>PP</italic><sub><italic>i </italic></sub>be the number of genes in the training set whose discriminant values is at least <italic>dv </italic>in the <italic>i</italic><sup><italic>th </italic></sup>run of cross validation. (These are the predicted positives.) Let <italic>TP</italic><sub><italic>i </italic></sub>be the number of these genes that respond to stress (the true positives). Using only this cross validation run, the estimated precision would be <italic>TP</italic><sub><italic>i</italic></sub><italic>/PP</italic><sub><italic>i</italic></sub>. One problem with this estimate is that if <italic>dv </italic>is high, then <italic>PP</italic><sub><italic>i </italic></sub>(and hence <italic>TP</italic><sub><italic>i</italic></sub>) could be 0, so the precision estimate would be undefined, something we observed frequently in practice. More generally, if <italic>PP</italic><sub><italic>i </italic></sub>(and hence <italic>TP</italic><sub><italic>i</italic></sub>) is low, then the precision estimate will have high variance, since it is supported by very little data. To circumvent these problems, we estimate the precision using the formula</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M26" name="1471-2105-8-358-i21" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mtext>TP</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mtext>PP</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mtext>TP</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mtext>PP</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mrow><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGWbaCcqWGYbGCcqWGLbqzcqWGJbWycqWGPbqAcqWGZbWCcqWGPbqAcqWGVbWBcqWGUbGBcqGH9aqpdaWcaaqaamaaqababaGaeeivaqLaeeiuaa1aaSbaaSqaaiabdMgaPbqabaaabaGaemyAaKgabeqdcqGHris5aaGcbaWaaabeaeaacqqGqbaucqqGqbaudaWgaaWcbaGaemyAaKgabeaaaeaacqWGPbqAaeqaniabggHiLdaaaOGaeyypa0ZaaabuaeaacqWG3bWDdaWgaaWcbaGaemyAaKgabeaakiabgEna0oaalaaabaGaeeivaqLaeeiuaa1aaSbaaSqaaiabdMgaPbqabaaakeaacqqGqbaucqqGqbaudaWgaaWcbaGaemyAaKgabeaaaaaabaGaemyAaKgabeqdcqGHris5aaaa@59BB@</mml:annotation></mml:semantics></mml:math></disp-formula></p><p>where <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M27" name="1471-2105-8-358-i22" overflow="scroll"><mml:semantics definitionURL="" encoding=""><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mtext>PP</mml:mtext></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mi>j</mml:mi><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mtext>PP</mml:mtext></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:annotation encoding="MathType-MTEF"> MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWG3bWDdaWgaaWcbaGaemyAaKgabeaakiabg2da9iabbcfaqjabbcfaqnaaBaaaleaacqWGPbqAaeqaaOGaei4la8YaaabmaeaacqqGqbaucqqGqbaudaWgaaWcbaGaemOAaOgabeaaaeaacqWGQbGAaeaacqWGobGta0GaeyyeIuoaaaa@3DCF@</mml:annotation></mml:semantics></mml:math></inline-formula>. The right-hand formula is a weighted average of individual precision estimates, <italic>TP</italic><sub><italic>i</italic></sub><italic>/PP</italic><sub><italic>i</italic></sub>. It gives more weight to precision estimates that are based on more data, <italic>i.e</italic>., for which <italic>PP</italic><sub><italic>i </italic></sub>is higher. In addition, by using the left-hand formula, we rarely end up dividing by zero, since the denominator is a sum of (random) non-negative numbers; <italic>i.e</italic>., &#x003a3;<sub><italic>i</italic></sub><italic>PP</italic><sub><italic>i </italic></sub>is much less likely to be zero than is any individual <italic>PP</italic><sub><italic>i</italic></sub>.</p></sec><sec><title>Biological experiments</title><p>Wild type and homozygous mutant seeds were plated on 0.5X MS media. They were stratified for 3 days and then germinated at 25C for 7 days. The abiotic temperature stresses consisted of 7 days exposure to either 30C, 14C or 4C. Anthocyanin levels were quantified as a measure of plant stress response. Anthocyanin was extracted using methanol-HCl [<xref ref-type="bibr" rid="B43">43</xref>]. In order to measure response to salt stress, plants were germinated for 3 days on 0.5X MS media and then transferred to medium containing 50 mM NaCl or to control plates. New root growth was measured 7 days after the transfer.</p></sec></sec><sec><title>Competing interests</title><p>The author(s) declares that there are no competing interests.</p></sec><sec><title>Authors' contributions</title><p>HL and AJB did the machine learning, with HL doing the actual programming. HL developed the idea of using ROC<sub>50 </sub>scores to combine classifiers. RC performed the gene knockout experiments under the supervision of NJP. HL and AJB wrote the bioinformatics sections of the manuscript, with HL providing the first draft. RC wrote the biological sections. All authors read and approved the final manuscript.</p></sec></body><back><ack><sec><title>Acknowledgements</title><p>AJB is supported by a grant from NSERC. HL is funded by the Department of Computer Science at the University of Toronto. NJP is supported by grants from NSERC. RC is funded in part by a University of Toronto fellowship. The Botany Beowulf Cluster was funded by a Genome Canada grant administered through the Ontario Genomics Institute.</p></sec></ack><ref-list><ref id="B1"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>M</given-names></name><name><surname>Grundy</surname><given-names>W</given-names></name><name><surname>Lin</surname><given-names>D</given-names></name><name><surname>Cristianini</surname><given-names>N</given-names></name><name><surname>Sugnet</surname><given-names>C</given-names></name><name><surname>Furey</surname><given-names>T</given-names></name><name><surname>Ares</surname><given-names>MJ</given-names></name><name><surname>Haussler</surname><given-names>D</given-names></name></person-group><article-title>Knowledge-based analysis of microarray gene expression data by using support vector machines</article-title><source>Proceedings of National Academy of Sciences of the United States of America</source><year>2000</year><volume>97</volume><fpage>262</fpage><lpage>267</lpage></citation></ref><ref id="B2"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Eisen</surname><given-names>M</given-names></name><name><surname>Spellman</surname><given-names>P</given-names></name><name><surname>Brown</surname><given-names>P</given-names></name><name><surname>Botstein</surname><given-names>D</given-names></name></person-group><article-title>Cluster analysis and display of genome-wide expression patterns</article-title><source>Proceedings of National Academy of Sciences of the United States of America</source><year>1998</year><volume>95</volume><fpage>14863</fpage><lpage>14868</lpage></citation></ref><ref id="B3"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hartigan</surname><given-names>T</given-names></name><name><surname>L&#x000e6;greid</surname><given-names>A</given-names></name><name><surname>Komorowski</surname><given-names>J</given-names></name><name><surname>Hoving</surname><given-names>E</given-names></name></person-group><article-title>A literature network of human genes for high-throughput analysis of gene expression</article-title><source>Nature Genetics</source><year>2001</year><volume>28</volume><fpage>21</fpage><lpage>28</lpage><pub-id pub-id-type="pmid">11326270</pub-id></citation></ref><ref id="B4"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hishigaki</surname><given-names>H</given-names></name><name><surname>Nakai</surname><given-names>K</given-names></name><name><surname>Ono</surname><given-names>T</given-names></name><name><surname>Tanigami</surname><given-names>A</given-names></name><name><surname>Takagi</surname><given-names>T</given-names></name></person-group><article-title>Assessment of prediction accuracy of protein function from protein-protein interaction data</article-title><source>Yeast</source><year>2001</year><volume>18</volume><fpage>523</fpage><lpage>531</lpage><pub-id pub-id-type="pmid">11284008</pub-id></citation></ref><ref id="B5"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Niehrs</surname><given-names>C</given-names></name><name><surname>Pollet</surname><given-names>N</given-names></name></person-group><article-title>Synexpression groups in eukaryotes</article-title><source>Nature</source><year>1999</year><volume>402</volume><fpage>483</fpage><lpage>487</lpage><pub-id pub-id-type="pmid">10591207</pub-id></citation></ref><ref id="B6"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Shatkay</surname><given-names>H</given-names></name><name><surname>Edwards</surname><given-names>S</given-names></name><name><surname>Wilbur</surname><given-names>W</given-names></name><name><surname>Boguski</surname><given-names>M</given-names></name></person-group><article-title>Genes, themes and microarrays: Using information retrieval for large-scale gene analysis</article-title><source>Proceedings of the International Conference on Intelligent Systems for Molecular Biology</source><year>2000</year><volume>8</volume><fpage>317</fpage><lpage>328</lpage></citation></ref><ref id="B7"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hughes</surname><given-names>T</given-names></name><name><surname>Marton</surname><given-names>M</given-names></name><name><surname>Jones</surname><given-names>A</given-names></name><name><surname>Roberts</surname><given-names>C</given-names></name><name><surname>Stoughton</surname><given-names>R</given-names></name><name><surname>Armour</surname><given-names>C</given-names></name><name><surname>Bennett</surname><given-names>H</given-names></name><name><surname>Coffey</surname><given-names>E</given-names></name><name><surname>Dai</surname><given-names>H</given-names></name><name><surname>He</surname><given-names>Y</given-names></name><name><surname>Kidd</surname><given-names>M</given-names></name><name><surname>King</surname><given-names>A</given-names></name><name><surname>Meyer</surname><given-names>M</given-names></name><name><surname>Slade</surname><given-names>D</given-names></name><name><surname>Lum</surname><given-names>P</given-names></name><name><surname>Stepaniants</surname><given-names>S</given-names></name><name><surname>Shoemaker</surname><given-names>D</given-names></name><name><surname>Gachotte</surname><given-names>D</given-names></name><name><surname>Chakraburtty</surname><given-names>K</given-names></name><name><surname>Simon</surname><given-names>J</given-names></name><name><surname>Bard</surname><given-names>M</given-names></name><name><surname>Friend</surname><given-names>S</given-names></name></person-group><article-title>Functional discovery via a compendium of expression profiles</article-title><source>Cell</source><year>2000</year><volume>102</volume><fpage>109</fpage><lpage>126</lpage><pub-id pub-id-type="pmid">10929718</pub-id></citation></ref><ref id="B8"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hvidsten</surname><given-names>T</given-names></name><name><surname>Komorowski</surname><given-names>J</given-names></name><name><surname>Sandvik</surname><given-names>A</given-names></name><name><surname>Laegreid</surname><given-names>A</given-names></name></person-group><article-title>Predicting gene function from gene expressions and ontologies</article-title><source>Pacific Symposium on Biocomputing</source><year>2001</year><fpage>299</fpage><lpage>310</lpage><pub-id pub-id-type="pmid">11262949</pub-id></citation></ref><ref id="B9"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>L&#x000e6;greid</surname><given-names>A</given-names></name><name><surname>Hvidsten</surname><given-names>T</given-names></name><name><surname>Midelfart</surname><given-names>H</given-names></name><name><surname>Komorowski</surname><given-names>J</given-names></name><name><surname>Sandvik</surname><given-names>A</given-names></name></person-group><article-title>Predicting gene ontology biological process from temporal gene expression patterns</article-title><source>Genome Research</source><year>2003</year><volume>13</volume><fpage>965</fpage><lpage>979</lpage><pub-id pub-id-type="pmid">12695321</pub-id></citation></ref><ref id="B10"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Kuramochi</surname><given-names>M</given-names></name><name><surname>Karypis</surname><given-names>G</given-names></name></person-group><article-title>Gene classification using expression profiles: A feasibility study</article-title><source>2nd IEEE International Symposium on Bioinformatics and Bioengineering</source><year>2001</year><fpage>191</fpage><lpage>200</lpage></citation></ref><ref id="B11"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Li</surname><given-names>T</given-names></name><name><surname>Zhu</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Ogihara</surname><given-names>M</given-names></name></person-group><article-title>Gene functional classification by semi-supervised learning from heterogeneous data</article-title><source>Proceedings of the 2003 ACM Symposium on Applied Computing</source><year>2003</year><fpage>78</fpage><lpage>82</lpage></citation></ref><ref id="B12"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Mnaimneh</surname><given-names>S</given-names></name><name><surname>Davierwala</surname><given-names>A</given-names></name><name><surname>Haynes</surname><given-names>J</given-names></name><name><surname>Moffat</surname><given-names>J</given-names></name><name><surname>Peng</surname><given-names>W</given-names></name><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Yang</surname><given-names>X</given-names></name><name><surname>Pootoolal</surname><given-names>J</given-names></name><name><surname>Chua</surname><given-names>G</given-names></name><name><surname>Lopez</surname><given-names>A</given-names></name><name><surname>Trochesset</surname><given-names>M</given-names></name><name><surname>Morse</surname><given-names>D</given-names></name><name><surname>Krogan</surname><given-names>N</given-names></name><name><surname>Hiley</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Morris</surname><given-names>Q</given-names></name><name><surname>Grigull</surname><given-names>J</given-names></name><name><surname>Mitsakakis</surname><given-names>N</given-names></name><name><surname>Roberts</surname><given-names>C</given-names></name><name><surname>Greenblatt</surname><given-names>J</given-names></name><name><surname>Boone</surname><given-names>C</given-names></name><name><surname>Kaiser</surname><given-names>C</given-names></name><name><surname>Andrews</surname><given-names>B</given-names></name><name><surname>Hughes</surname><given-names>T</given-names></name></person-group><article-title>Exploration of essential gene functions via titratable promoter alleles</article-title><source>Cell</source><year>2004</year><volume>118</volume><fpage>31</fpage><lpage>44</lpage><pub-id pub-id-type="pmid">15242642</pub-id></citation></ref><ref id="B13"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Pavlidis</surname><given-names>P</given-names></name><name><surname>Weston</surname><given-names>J</given-names></name><name><surname>Cai</surname><given-names>J</given-names></name><name><surname>Grundy</surname><given-names>W</given-names></name></person-group><article-title>Gene functional classification from heterogeneous data</article-title><source>Proceedings of the 5th International Conference on Computational Molecular Biology</source><year>2001</year><fpage>242</fpage><lpage>248</lpage></citation></ref><ref id="B14"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Mateos</surname><given-names>A</given-names></name><name><surname>Dopazo</surname><given-names>J</given-names></name><name><surname>Jansen</surname><given-names>R</given-names></name><name><surname>Tu</surname><given-names>Y</given-names></name><name><surname>Gerstein</surname><given-names>M</given-names></name><name><surname>Stolovitzky</surname><given-names>G</given-names></name></person-group><article-title>Systematic learning of gene functional classes from DNA array expression data by using multilayer perceptrons</article-title><source>Genome Research</source><year>2002</year><volume>12</volume><fpage>1703</fpage><lpage>1715</lpage><pub-id pub-id-type="pmid">12421757</pub-id></citation></ref><ref id="B15"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Trochesset</surname><given-names>M</given-names></name><name><surname>Bonner</surname><given-names>A</given-names></name></person-group><article-title>Clustering Labeled Data and Cross-Validation for Classification with Few Positives in Yeast</article-title><source>Proceedings of the 4th ACM SIGKDD Workshop on Data Mining in Bioinformatics (BioKDD)</source><year>2004</year></citation></ref><ref id="B16"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Morris</surname><given-names>Q</given-names></name><name><surname>Chang</surname><given-names>R</given-names></name><name><surname>Shai</surname><given-names>O</given-names></name><name><surname>Bakowski</surname><given-names>M</given-names></name><name><surname>Mitsakakis</surname><given-names>N</given-names></name><name><surname>Mohammad</surname><given-names>N</given-names></name><name><surname>Robinson</surname><given-names>M</given-names></name><name><surname>Zirngibl</surname><given-names>R</given-names></name><name><surname>Somogyi</surname><given-names>E</given-names></name><name><surname>Laurin</surname><given-names>N</given-names></name><name><surname>Eftekharpour</surname><given-names>E</given-names></name><name><surname>Sat</surname><given-names>E</given-names></name><name><surname>Grigull</surname><given-names>J</given-names></name><name><surname>Pan</surname><given-names>Q</given-names></name><name><surname>Peng</surname><given-names>W</given-names></name><name><surname>Krogan</surname><given-names>N</given-names></name><name><surname>Greenblatt</surname><given-names>J</given-names></name><name><surname>Fehlings</surname><given-names>M</given-names></name><name><surname>Kooy</surname><given-names>vdD</given-names></name><name><surname>Aubin</surname><given-names>J</given-names></name><name><surname>Bruneau</surname><given-names>B</given-names></name><name><surname>Rossant</surname><given-names>J</given-names></name><name><surname>Blencowe</surname><given-names>B</given-names></name><name><surname>Frey</surname><given-names>B</given-names></name><name><surname>Hughes</surname><given-names>T</given-names></name></person-group><article-title>The functional landscape of mouse gene expression</article-title><source>Jounral of Biology</source><year>2004</year><volume>3</volume><fpage>21</fpage></citation></ref><ref id="B17"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Walbot</surname><given-names>V</given-names></name></person-group><article-title>A green chapter in the book of life</article-title><source>Nature</source><year>2000</year><volume>408</volume><fpage>794</fpage><lpage>795</lpage><pub-id pub-id-type="pmid">11130710</pub-id></citation></ref><ref id="B18"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Rhee</surname><given-names>S</given-names></name></person-group><article-title>Bioinformatic resources, challenges, and opportunities using Arabidopsis as a model organism in a post-genomic era</article-title><source>Plant Physiology</source><year>2000</year><volume>124</volume><fpage>1460</fpage><lpage>1464</lpage><pub-id pub-id-type="pmid">11115859</pub-id></citation></ref><ref id="B19"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Joshi</surname><given-names>T</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Alexandrov</surname><given-names>N</given-names></name><name><surname>Xu</surname><given-names>D</given-names></name></person-group><article-title>Cellular function prediction and biological pathway discovery in Arabidopsis thaliana using microarray data</article-title><source>Proceedings of the 26th Annual International Conference of the IEEE EMBS</source><year>2004</year><publisher-name>San Francisco, CA</publisher-name><fpage>2881</fpage><lpage>2884</lpage></citation></ref><ref id="B20"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Clare</surname><given-names>A</given-names></name><name><surname>Karwath</surname><given-names>A</given-names></name><name><surname>Ougham</surname><given-names>H</given-names></name><name><surname>King</surname><given-names>R</given-names></name></person-group><article-title>Functional Bioinformatics for Arabidopsis thaliana</article-title><source>Bioinformatics</source><year>2006</year><volume>22</volume><fpage>1130</fpage><lpage>1136</lpage><pub-id pub-id-type="pmid">16481336</pub-id></citation></ref><ref id="B21"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name><name><surname>Friedman</surname><given-names>J</given-names></name></person-group><source>The Elements of Statistical Learning: Data Mining, Inference and Prediction</source><year>2001</year><publisher-name>Springer-Verlag, New York</publisher-name></citation></ref><ref id="B22"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Toufighi</surname><given-names>K</given-names></name><name><surname>Brady</surname><given-names>SM</given-names></name><name><surname>Austin</surname><given-names>R</given-names></name><name><surname>Ly</surname><given-names>E</given-names></name><name><surname>Provart</surname><given-names>NJ</given-names></name></person-group><article-title>The Botany Array Resource: e-Northerns, Expression Angling, and promoter analyses</article-title><source>The Plant Journal</source><year>2005</year><volume>43</volume><fpage>153</fpage><lpage>163</lpage><pub-id pub-id-type="pmid">15960624</pub-id></citation></ref><ref id="B23"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kilian</surname><given-names>J</given-names></name><name><surname>Whitehead</surname><given-names>D</given-names></name><name><surname>Horak</surname><given-names>J</given-names></name><name><surname>Wanke</surname><given-names>D</given-names></name><name><surname>Weinl</surname><given-names>S</given-names></name><name><surname>Batistic</surname><given-names>O</given-names></name><name><surname>D'Angelo</surname><given-names>C</given-names></name><name><surname>Bornberg-Bauer</surname><given-names>E</given-names></name><name><surname>Kudla</surname><given-names>J</given-names></name><name><surname>Harter</surname><given-names>K</given-names></name></person-group><article-title>The AtGenExpress global stress expression data set: protocols, evaluation and model data analysis of UV-B light, drought and cold stress responses</article-title><source>The Plant Journal</source><year>2007</year><volume>50</volume><fpage>347</fpage><lpage>363</lpage><pub-id pub-id-type="pmid">17376166</pub-id></citation></ref><ref id="B24"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Craigon</surname><given-names>D</given-names></name><name><surname>James</surname><given-names>N</given-names></name><name><surname>Okyere</surname><given-names>J</given-names></name><name><surname>Higgins</surname><given-names>J</given-names></name><name><surname>Jotham</surname><given-names>J</given-names></name><name><surname>May</surname><given-names>S</given-names></name></person-group><article-title>NASCArrays: a repository for microarray data generated by NASC's transcriptomics service</article-title><source>Nucleic Acids Research</source><year>2004</year><fpage>575</fpage><lpage>577</lpage></citation></ref><ref id="B25"><citation citation-type="other"><article-title>Nottingham Arabidopsis Stock Centre (NASC)</article-title><ext-link ext-link-type="uri" xlink:href="http://arabidopsis.info"/></citation></ref><ref id="B26"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Consortium</surname><given-names>TGO</given-names></name></person-group><article-title>Gene Ontology: Tool for the unification of biology</article-title><source>Nature Genetics</source><year>2000</year><volume>25</volume><fpage>25</fpage><lpage>29</lpage><pub-id pub-id-type="pmid">10802651</pub-id></citation></ref><ref id="B27"><citation citation-type="other"><article-title>The Arabidopsis Information Resource (TAIR)</article-title><ext-link ext-link-type="uri" xlink:href="http://www.arabidopsis.org"/></citation></ref><ref id="B28"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Berardini</surname><given-names>T</given-names></name><name><surname>Mundodi</surname><given-names>S</given-names></name><name><surname>Reiser</surname><given-names>R</given-names></name><name><surname>Huala</surname><given-names>E</given-names></name><name><surname>Garcia-Hernandez</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>P</given-names></name><name><surname>Mueller</surname><given-names>L</given-names></name><name><surname>Yoon</surname><given-names>J</given-names></name><name><surname>Doyle</surname><given-names>A</given-names></name><name><surname>Lander</surname><given-names>G</given-names></name><name><surname>Moseyko</surname><given-names>N</given-names></name><name><surname>Yoo</surname><given-names>D</given-names></name><name><surname>Xu</surname><given-names>I</given-names></name><name><surname>Zoeckler</surname><given-names>B</given-names></name><name><surname>Montoya</surname><given-names>M</given-names></name><name><surname>Miller</surname><given-names>N</given-names></name><name><surname>Weems</surname><given-names>D</given-names></name><name><surname>Rhee</surname><given-names>S</given-names></name></person-group><article-title>Functional annotation of the Arabidopsis genome using controlled vocabularies</article-title><source>Plant Physiology</source><year>2004</year><volume>135</volume><fpage>1</fpage><lpage>11</lpage></citation></ref><ref id="B29"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Belkin</surname><given-names>M</given-names></name><name><surname>Niyogi</surname><given-names>P</given-names></name></person-group><article-title>Semi-supervised Learning on Riemannian Manifolds</article-title><source>Machine Learning</source><year>2004</year><volume>56</volume><fpage>209</fpage><lpage>239</lpage></citation></ref><ref id="B30"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Oliveira</surname><given-names>CS</given-names></name><name><surname>Cozman</surname><given-names>FG</given-names></name></person-group><article-title>Splitting the Unsupervised and Supervised Components of Semi-Supervised Learning</article-title><source>Proceedings of the 22nd ICML Workshop on Learning with Partially Classified Training Data, Bonn, Germany</source><year>2005</year><fpage>67</fpage><lpage>74</lpage></citation></ref><ref id="B31"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Vapnik</surname><given-names>V</given-names></name></person-group><source>Statistical Learning Theory</source><year>1998</year><publisher-name>Wiley-Interscience</publisher-name></citation></ref><ref id="B32"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Fawcett</surname><given-names>T</given-names></name></person-group><article-title>ROC Graphs: Notes and practical considerations for researchers</article-title><source>Tech Rep HPL-2003-4, HP Laboratories, Palo Alto, CA</source><year>2003</year></citation></ref><ref id="B33"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Ng</surname><given-names>AY</given-names></name><name><surname>Jordan</surname><given-names>MI</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Dietterich TG, Becker S, Ghahramani Z</surname></name></person-group><article-title>On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes</article-title><source>Advances in Neural Information Processing Systems 14</source><year>2002</year><publisher-name>Cambridge, MA: MIT Press</publisher-name></citation></ref><ref id="B34"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Gribskov</surname><given-names>M</given-names></name><name><surname>Robinson</surname><given-names>N</given-names></name></person-group><article-title>Use of Receiver Operating Characteristic (ROC) analysis to evaluate sequence matching</article-title><source>Computers and Chemistry</source><year>1996</year><fpage>25</fpage><lpage>33</lpage><pub-id pub-id-type="pmid">16718863</pub-id></citation></ref><ref id="B35"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y</given-names></name><name><surname>Hochberg</surname><given-names>Y</given-names></name></person-group><article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title><source>Journal of the Royal Statistical Society: Series B</source><year>1995</year><volume>57</volume><fpage>289</fpage><lpage>300</lpage></citation></ref><ref id="B36"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Schmid</surname><given-names>M</given-names></name><name><surname>Davison</surname><given-names>T</given-names></name><name><surname>Henz</surname><given-names>S</given-names></name><name><surname>Pape</surname><given-names>U</given-names></name><name><surname>Demar</surname><given-names>M</given-names></name><name><surname>Vingron</surname><given-names>M</given-names></name><name><surname>Sholkpf</surname><given-names>B</given-names></name><name><surname>Weigel</surname><given-names>D</given-names></name><name><surname>Lohmann</surname><given-names>J</given-names></name></person-group><article-title>A gene expression map of Arabidopsis thaliana development</article-title><source>Nature Genetics</source><year>2005</year><volume>37</volume><fpage>501</fpage><lpage>506</lpage><pub-id pub-id-type="pmid">15806101</pub-id></citation></ref><ref id="B37"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Alonso</surname><given-names>J</given-names></name><name><surname>Stepanova</surname><given-names>A</given-names></name><name><surname>Leisse</surname><given-names>T</given-names></name><name><surname>Kim</surname><given-names>C</given-names></name><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Shinn</surname><given-names>P</given-names></name><name><surname>Stevenson</surname><given-names>D</given-names></name><name><surname>Zimmerman</surname><given-names>J</given-names></name><name><surname>Barajas</surname><given-names>P</given-names></name><name><surname>Cheuk</surname><given-names>R</given-names></name><name><surname>Gadrinab</surname><given-names>C</given-names></name><name><surname>Heller</surname><given-names>C</given-names></name><name><surname>Jeske</surname><given-names>A</given-names></name><name><surname>Koesema</surname><given-names>E</given-names></name><name><surname>Meyers</surname><given-names>C</given-names></name><name><surname>Parker</surname><given-names>H</given-names></name><name><surname>Prednis</surname><given-names>L</given-names></name><name><surname>Ansari</surname><given-names>Y</given-names></name><name><surname>Choy</surname><given-names>N</given-names></name><name><surname>Deen</surname><given-names>H</given-names></name><name><surname>Geralt</surname><given-names>M</given-names></name><name><surname>Hazari</surname><given-names>N</given-names></name><name><surname>Hom</surname><given-names>E</given-names></name><name><surname>Karnes</surname><given-names>M</given-names></name><name><surname>Mulholland</surname><given-names>C</given-names></name><name><surname>Ndubaku</surname><given-names>R</given-names></name><name><surname>Schmidt</surname><given-names>I</given-names></name><name><surname>Guzman</surname><given-names>P</given-names></name><name><surname>Aguilar-Henonin</surname><given-names>L</given-names></name><name><surname>Schmid</surname><given-names>M</given-names></name><name><surname>Weigel</surname><given-names>D</given-names></name><name><surname>Carter</surname><given-names>D</given-names></name><name><surname>Marchand</surname><given-names>T</given-names></name><name><surname>Risseeuw</surname><given-names>E</given-names></name><name><surname>Brogden</surname><given-names>D</given-names></name><name><surname>Zeko</surname><given-names>A</given-names></name><name><surname>Crosby</surname><given-names>W</given-names></name><name><surname>Berry</surname><given-names>C</given-names></name><name><surname>Ecker</surname><given-names>J</given-names></name></person-group><article-title>Genome-wide insertional mutagenesis of Arabidopsis thaliana</article-title><source>Science</source><year>2003</year><volume>2003</volume><fpage>653</fpage><lpage>657</lpage><pub-id pub-id-type="pmid">12893945</pub-id></citation></ref><ref id="B38"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>O'Connor</surname><given-names>T</given-names></name><name><surname>Dyreson</surname><given-names>C</given-names></name><name><surname>Wyrick</surname><given-names>J</given-names></name></person-group><article-title>Athena: a resource for rapid visualization and systematic analysis of Arabidopsis promoter sequences</article-title><source>Bioinformatics</source><year>2006</year><volume>21</volume><fpage>4411</fpage><lpage>4413</lpage><pub-id pub-id-type="pmid">16223790</pub-id></citation></ref><ref id="B39"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Winter</surname><given-names>D</given-names></name><name><surname>Vinegar</surname><given-names>B</given-names></name><name><surname>Wilson</surname><given-names>G</given-names></name><name><surname>Provart</surname><given-names>N</given-names></name></person-group><article-title>An 'electronic fluorescent protein' browser for exploring Arabidopsis Microarray Data</article-title><source>in prep</source><year>2006</year></citation></ref><ref id="B40"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Fowler</surname><given-names>S</given-names></name><name><surname>Thomashow</surname><given-names>M</given-names></name></person-group><article-title>Arabidopsis transcriptome profiling indicates that multiple regulatory pathways are activated during cold acclimation in addition to the CBF cold response pathway</article-title><source>Plant Cell</source><year>2002</year><volume>14</volume><fpage>1675</fpage><lpage>1690</lpage><pub-id pub-id-type="pmid">12172015</pub-id></citation></ref><ref id="B41"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Smirnoff</surname><given-names>N</given-names></name><name><surname>Bryant</surname><given-names>J</given-names></name></person-group><article-title>DREB takes the stress out of growing up</article-title><source>Nature Biotechnology</source><year>1999</year><volume>17</volume><fpage>229</fpage><lpage>230</lpage><pub-id pub-id-type="pmid">10096286</pub-id></citation></ref><ref id="B42"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Taji</surname><given-names>T</given-names></name><name><surname>Seki</surname><given-names>M</given-names></name><name><surname>Satou</surname><given-names>M</given-names></name><name><surname>Sakurai</surname><given-names>T</given-names></name><name><surname>Kobayashi</surname><given-names>M</given-names></name><name><surname>Ishiyama</surname><given-names>K</given-names></name><name><surname>Naruasak</surname><given-names>Y</given-names></name><name><surname>Narusaka</surname><given-names>M</given-names></name><name><surname>Zhu</surname><given-names>J</given-names></name><name><surname>Shinozaki</surname><given-names>K</given-names></name></person-group><article-title>Comparative genomics in salt tolerance between Arabidopsis and Arabidopsis-related halophyte salt cress using Arabidopsis microarray</article-title><source>Plant Physiology</source><year>2004</year><volume>135</volume><fpage>1697</fpage><lpage>1709</lpage><pub-id pub-id-type="pmid">15247402</pub-id></citation></ref><ref id="B43"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Solfanelli</surname><given-names>C</given-names></name><name><surname>Poggi</surname><given-names>A</given-names></name><name><surname>Loreti</surname><given-names>E</given-names></name><name><surname>Alpi</surname><given-names>A</given-names></name><name><surname>Perata</surname><given-names>P</given-names></name></person-group><article-title>Sucrose-specific induction of the anthocyanin biosynthetic pathway in Arabidopsis</article-title><source>Plant Physiology</source><year>2006</year><volume>140</volume><fpage>637</fpage><lpage>646</lpage><pub-id pub-id-type="pmid">16384906</pub-id></citation></ref></ref-list></back></article> 